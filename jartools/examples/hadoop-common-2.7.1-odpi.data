{
  "org/apache/hadoop/fs/Options.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/ha/HealthMonitor$MonitorDaemon.class": "Compiled from \"HealthMonitor.java\"\npublic class org.apache.hadoop.ha.HealthMonitor {\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.HealthMonitor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  public void addCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public void removeCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public synchronized void addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public synchronized void removeServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public void shutdown();\n  public synchronized org.apache.hadoop.ha.HAServiceProtocol getProxy();\n  protected org.apache.hadoop.ha.HAServiceProtocol createProxy() throws java.io.IOException;\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getHealthState();\n  synchronized org.apache.hadoop.ha.HAServiceStatus getLastServiceStatus();\n  boolean isAlive();\n  void join() throws java.lang.InterruptedException;\n  void start();\n  static org.apache.hadoop.ha.HAServiceTarget access$100(org.apache.hadoop.ha.HealthMonitor);\n  static org.apache.commons.logging.Log access$200();\n  static void access$300(org.apache.hadoop.ha.HealthMonitor, org.apache.hadoop.ha.HealthMonitor$State);\n  static boolean access$400(org.apache.hadoop.ha.HealthMonitor);\n  static void access$500(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static void access$600(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Truncate.class": "Compiled from \"Truncate.java\"\npublic class org.apache.hadoop.fs.shell.Truncate extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  protected long newLength;\n  protected java.util.List<org.apache.hadoop.fs.shell.PathData> waitList;\n  protected boolean waitOpt;\n  public org.apache.hadoop.fs.shell.Truncate();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/record/meta/RecordTypeInfo.class": "Compiled from \"RecordTypeInfo.java\"\npublic class org.apache.hadoop.record.meta.RecordTypeInfo extends org.apache.hadoop.record.Record {\n  org.apache.hadoop.record.meta.StructTypeID sTid;\n  public org.apache.hadoop.record.meta.RecordTypeInfo();\n  public org.apache.hadoop.record.meta.RecordTypeInfo(java.lang.String);\n  public java.lang.String getName();\n  public void setName(java.lang.String);\n  public void addField(java.lang.String, org.apache.hadoop.record.meta.TypeID);\n  public java.util.Collection<org.apache.hadoop.record.meta.FieldTypeInfo> getFieldTypeInfos();\n  public org.apache.hadoop.record.meta.RecordTypeInfo getNestedStructTypeInfo(java.lang.String);\n  public void serialize(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  public void deserialize(org.apache.hadoop.record.RecordInput, java.lang.String) throws java.io.IOException;\n  public int compareTo(java.lang.Object) throws java.lang.ClassCastException;\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/jvm/JvmMetrics.class": "Compiled from \"JvmMetrics.java\"\npublic class org.apache.hadoop.metrics.jvm.JvmMetrics implements org.apache.hadoop.metrics.Updater {\n  public static synchronized org.apache.hadoop.metrics.jvm.JvmMetrics init(java.lang.String, java.lang.String);\n  public static synchronized org.apache.hadoop.metrics.jvm.JvmMetrics init(java.lang.String, java.lang.String, java.lang.String);\n  public void doUpdates(org.apache.hadoop.metrics.MetricsContext);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer2$QuotingInputFilter$RequestQuoter$1.class": "Compiled from \"HttpServer2.java\"\npublic final class org.apache.hadoop.http.HttpServer2 implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  public static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean);\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public java.net.InetSocketAddress getConnectorAddress(int);\n  public void setThreads(int, int);\n  public void start() throws java.io.IOException;\n  void openListeners() throws java.lang.Exception;\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  org.apache.hadoop.http.HttpServer2(org.apache.hadoop.http.HttpServer2$Builder, org.apache.hadoop.http.HttpServer2$1) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.http.HttpServer2, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.http.HttpServer2, org.mortbay.jetty.Connector);\n  static void access$300(org.apache.hadoop.http.HttpServer2);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/AbstractMetric.class": "Compiled from \"AbstractMetric.java\"\npublic abstract class org.apache.hadoop.metrics2.AbstractMetric implements org.apache.hadoop.metrics2.MetricsInfo {\n  protected org.apache.hadoop.metrics2.AbstractMetric(org.apache.hadoop.metrics2.MetricsInfo);\n  public java.lang.String name();\n  public java.lang.String description();\n  protected org.apache.hadoop.metrics2.MetricsInfo info();\n  public abstract java.lang.Number value();\n  public abstract org.apache.hadoop.metrics2.MetricType type();\n  public abstract void visit(org.apache.hadoop.metrics2.MetricsVisitor);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/net/SocketIOWithTimeout$SelectorPool$ProviderInfo.class": "Compiled from \"SocketIOWithTimeout.java\"\nabstract class org.apache.hadoop.net.SocketIOWithTimeout {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.net.SocketIOWithTimeout(java.nio.channels.SelectableChannel, long) throws java.io.IOException;\n  void close();\n  boolean isOpen();\n  java.nio.channels.SelectableChannel getChannel();\n  static void checkChannelValidity(java.lang.Object) throws java.io.IOException;\n  abstract int performIO(java.nio.ByteBuffer) throws java.io.IOException;\n  int doIO(java.nio.ByteBuffer, int) throws java.io.IOException;\n  static void connect(java.nio.channels.SocketChannel, java.net.SocketAddress, int) throws java.io.IOException;\n  void waitForIO(int) throws java.io.IOException;\n  public void setTimeout(long);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyShell.class": "Compiled from \"KeyShell.java\"\npublic class org.apache.hadoop.crypto.key.KeyShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.crypto.key.KeyShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.crypto.key.KeyShell);\n  static boolean access$300(org.apache.hadoop.crypto.key.KeyShell);\n}\n", 
  "org/apache/hadoop/metrics/spi/AbstractMetricsContext$1.class": "Compiled from \"AbstractMetricsContext.java\"\npublic abstract class org.apache.hadoop.metrics.spi.AbstractMetricsContext implements org.apache.hadoop.metrics.MetricsContext {\n  protected org.apache.hadoop.metrics.spi.AbstractMetricsContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  protected java.lang.String getAttribute(java.lang.String);\n  protected java.util.Map<java.lang.String, java.lang.String> getAttributeTable(java.lang.String);\n  public java.lang.String getContextName();\n  public org.apache.hadoop.metrics.ContextFactory getContextFactory();\n  public synchronized void startMonitoring() throws java.io.IOException;\n  public synchronized void stopMonitoring();\n  public boolean isMonitoring();\n  public synchronized void close();\n  public final synchronized org.apache.hadoop.metrics.MetricsRecord createRecord(java.lang.String);\n  protected org.apache.hadoop.metrics.MetricsRecord newRecord(java.lang.String);\n  public synchronized void registerUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized void unregisterUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized java.util.Map<java.lang.String, java.util.Collection<org.apache.hadoop.metrics.spi.OutputRecord>> getAllRecords();\n  protected abstract void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord) throws java.io.IOException;\n  protected void flush() throws java.io.IOException;\n  protected void update(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  protected void remove(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  public int getPeriod();\n  protected void setPeriod(int);\n  protected void parseAndSetPeriod(java.lang.String);\n  static void access$000(org.apache.hadoop.metrics.spi.AbstractMetricsContext) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/PureJavaCrc32.class": "Compiled from \"PureJavaCrc32.java\"\npublic class org.apache.hadoop.util.PureJavaCrc32 implements java.util.zip.Checksum {\n  public org.apache.hadoop.util.PureJavaCrc32();\n  public long getValue();\n  public void reset();\n  public void update(byte[], int, int);\n  public final void update(int);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/source/JvmMetrics$Singleton.class": "Compiled from \"JvmMetrics.java\"\npublic class org.apache.hadoop.metrics2.source.JvmMetrics implements org.apache.hadoop.metrics2.MetricsSource {\n  static final float M;\n  final java.lang.management.MemoryMXBean memoryMXBean;\n  final java.util.List<java.lang.management.GarbageCollectorMXBean> gcBeans;\n  final java.lang.management.ThreadMXBean threadMXBean;\n  final java.lang.String processName;\n  final java.lang.String sessionId;\n  final java.util.concurrent.ConcurrentHashMap<java.lang.String, org.apache.hadoop.metrics2.MetricsInfo[]> gcInfoCache;\n  org.apache.hadoop.metrics2.source.JvmMetrics(java.lang.String, java.lang.String);\n  public void setPauseMonitor(org.apache.hadoop.util.JvmPauseMonitor);\n  public static org.apache.hadoop.metrics2.source.JvmMetrics create(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSystem);\n  public static org.apache.hadoop.metrics2.source.JvmMetrics initSingleton(java.lang.String, java.lang.String);\n  public void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n}\n", 
  "org/apache/hadoop/tools/GetUserMappingsProtocol.class": "Compiled from \"GetUserMappingsProtocol.java\"\npublic interface org.apache.hadoop.tools.GetUserMappingsProtocol {\n  public static final long versionID;\n  public abstract java.lang.String[] getGroupsForUser(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/serializer/avro/AvroSerialization$AvroDeserializer.class": "Compiled from \"AvroSerialization.java\"\npublic abstract class org.apache.hadoop.io.serializer.avro.AvroSerialization<T> extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.io.serializer.Serialization<T> {\n  public static final java.lang.String AVRO_SCHEMA_KEY;\n  public org.apache.hadoop.io.serializer.avro.AvroSerialization();\n  public org.apache.hadoop.io.serializer.Deserializer<T> getDeserializer(java.lang.Class<T>);\n  public org.apache.hadoop.io.serializer.Serializer<T> getSerializer(java.lang.Class<T>);\n  public abstract org.apache.avro.Schema getSchema(T);\n  public abstract org.apache.avro.io.DatumWriter<T> getWriter(java.lang.Class<T>);\n  public abstract org.apache.avro.io.DatumReader<T> getReader(java.lang.Class<T>);\n}\n", 
  "org/apache/hadoop/security/ssl/SSLHostnameVerifier$1.class": "Compiled from \"SSLHostnameVerifier.java\"\npublic interface org.apache.hadoop.security.ssl.SSLHostnameVerifier extends javax.net.ssl.HostnameVerifier {\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT_AND_LOCALHOST;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT_IE6;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier ALLOW_ALL;\n  public abstract boolean verify(java.lang.String, javax.net.ssl.SSLSession);\n  public abstract void check(java.lang.String, javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String, java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String, java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String[], java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$8.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$GracefulFailoverRequestProto.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/Text.class": "Compiled from \"Text.java\"\npublic class org.apache.hadoop.io.Text extends org.apache.hadoop.io.BinaryComparable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.BinaryComparable> {\n  public static final int DEFAULT_MAX_LEN;\n  static final int[] bytesFromUTF8;\n  static final int[] offsetsFromUTF8;\n  public org.apache.hadoop.io.Text();\n  public org.apache.hadoop.io.Text(java.lang.String);\n  public org.apache.hadoop.io.Text(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text(byte[]);\n  public byte[] copyBytes();\n  public byte[] getBytes();\n  public int getLength();\n  public int charAt(int);\n  public int find(java.lang.String);\n  public int find(java.lang.String, int);\n  public void set(java.lang.String);\n  public void set(byte[]);\n  public void set(org.apache.hadoop.io.Text);\n  public void set(byte[], int, int);\n  public void append(byte[], int, int);\n  public void clear();\n  public java.lang.String toString();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void readFields(java.io.DataInput, int) throws java.io.IOException;\n  public static void skip(java.io.DataInput) throws java.io.IOException;\n  public void readWithKnownLength(java.io.DataInput, int) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void write(java.io.DataOutput, int) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static java.lang.String decode(byte[]) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String decode(byte[], int, int) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String decode(byte[], int, int, boolean) throws java.nio.charset.CharacterCodingException;\n  public static java.nio.ByteBuffer encode(java.lang.String) throws java.nio.charset.CharacterCodingException;\n  public static java.nio.ByteBuffer encode(java.lang.String, boolean) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput, int) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String, int) throws java.io.IOException;\n  public static void validateUTF8(byte[]) throws java.nio.charset.MalformedInputException;\n  public static void validateUTF8(byte[], int, int) throws java.nio.charset.MalformedInputException;\n  public static int bytesToCodePoint(java.nio.ByteBuffer);\n  public static int utf8Length(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSRESTConstants.class": "Compiled from \"KMSRESTConstants.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSRESTConstants {\n  public static final java.lang.String SERVICE_VERSION;\n  public static final java.lang.String KEY_RESOURCE;\n  public static final java.lang.String KEYS_RESOURCE;\n  public static final java.lang.String KEYS_METADATA_RESOURCE;\n  public static final java.lang.String KEYS_NAMES_RESOURCE;\n  public static final java.lang.String KEY_VERSION_RESOURCE;\n  public static final java.lang.String METADATA_SUB_RESOURCE;\n  public static final java.lang.String VERSIONS_SUB_RESOURCE;\n  public static final java.lang.String EEK_SUB_RESOURCE;\n  public static final java.lang.String CURRENT_VERSION_SUB_RESOURCE;\n  public static final java.lang.String KEY;\n  public static final java.lang.String EEK_OP;\n  public static final java.lang.String EEK_GENERATE;\n  public static final java.lang.String EEK_DECRYPT;\n  public static final java.lang.String EEK_NUM_KEYS;\n  public static final java.lang.String IV_FIELD;\n  public static final java.lang.String NAME_FIELD;\n  public static final java.lang.String CIPHER_FIELD;\n  public static final java.lang.String LENGTH_FIELD;\n  public static final java.lang.String DESCRIPTION_FIELD;\n  public static final java.lang.String ATTRIBUTES_FIELD;\n  public static final java.lang.String CREATED_FIELD;\n  public static final java.lang.String VERSIONS_FIELD;\n  public static final java.lang.String MATERIAL_FIELD;\n  public static final java.lang.String VERSION_NAME_FIELD;\n  public static final java.lang.String ENCRYPTED_KEY_VERSION_FIELD;\n  public static final java.lang.String ERROR_EXCEPTION_JSON;\n  public static final java.lang.String ERROR_MESSAGE_JSON;\n  public org.apache.hadoop.crypto.key.kms.KMSRESTConstants();\n}\n", 
  "org/apache/hadoop/security/authorize/AuthorizationException.class": "Compiled from \"AuthorizationException.java\"\npublic class org.apache.hadoop.security.authorize.AuthorizationException extends org.apache.hadoop.security.AccessControlException {\n  public org.apache.hadoop.security.authorize.AuthorizationException();\n  public org.apache.hadoop.security.authorize.AuthorizationException(java.lang.String);\n  public org.apache.hadoop.security.authorize.AuthorizationException(java.lang.Throwable);\n  public java.lang.StackTraceElement[] getStackTrace();\n  public void printStackTrace();\n  public void printStackTrace(java.io.PrintStream);\n  public void printStackTrace(java.io.PrintWriter);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB$1.class": "Compiled from \"HAServiceProtocolClientSideTranslatorPB.java\"\npublic class org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB implements org.apache.hadoop.ha.HAServiceProtocol,java.io.Closeable,org.apache.hadoop.ipc.ProtocolTranslator {\n  public org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public void monitorHealth() throws java.io.IOException;\n  public void transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws java.io.IOException;\n  public void transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws java.io.IOException;\n  public org.apache.hadoop.ha.HAServiceStatus getServiceStatus() throws java.io.IOException;\n  public void close();\n  public java.lang.Object getUnderlyingProxyObject();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RpcNoSuchMethodException.class": "Compiled from \"RpcNoSuchMethodException.java\"\npublic class org.apache.hadoop.ipc.RpcNoSuchMethodException extends org.apache.hadoop.ipc.RpcServerException {\n  public org.apache.hadoop.ipc.RpcNoSuchMethodException(java.lang.String);\n  public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto getRpcStatusProto();\n  public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto getRpcErrorCodeProto();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSystemImpl$3.class": "Compiled from \"MetricsSystemImpl.java\"\npublic class org.apache.hadoop.metrics2.impl.MetricsSystemImpl extends org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSource {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String MS_NAME;\n  static final java.lang.String MS_STATS_NAME;\n  static final java.lang.String MS_STATS_DESC;\n  static final java.lang.String MS_CONTROL_NAME;\n  static final java.lang.String MS_INIT_MODE_KEY;\n  org.apache.hadoop.metrics2.lib.MutableStat snapshotStat;\n  org.apache.hadoop.metrics2.lib.MutableStat publishStat;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong droppedPubAll;\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl(java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl();\n  public synchronized org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public synchronized void unregisterSource(java.lang.String);\n  synchronized void registerSource(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource);\n  public synchronized <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  synchronized void registerSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink);\n  public synchronized void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public synchronized void startMetricsMBeans();\n  public synchronized void stopMetricsMBeans();\n  public synchronized java.lang.String currentConfig();\n  synchronized void onTimerEvent();\n  public synchronized void publishMetricsNow();\n  synchronized org.apache.hadoop.metrics2.impl.MetricsBuffer sampleMetrics();\n  synchronized void publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, boolean);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static java.lang.String getHostname();\n  public synchronized void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n  public synchronized boolean shutdown();\n  public org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter getSourceAdapter(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$RemoveSpanReceiverResponseProto$Builder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$MultipleLinearRandomRetry.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$ParsedTimeDuration$4.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor$GzipStateLabel.class": "Compiled from \"BuiltInGzipDecompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor implements org.apache.hadoop.io.compress.Decompressor {\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor();\n  public synchronized boolean needsInput();\n  public synchronized void setInput(byte[], int, int);\n  public synchronized int decompress(byte[], int, int) throws java.io.IOException;\n  public synchronized long getBytesRead();\n  public synchronized int getRemaining();\n  public synchronized boolean needsDictionary();\n  public synchronized void setDictionary(byte[], int, int);\n  public synchronized boolean finished();\n  public synchronized void reset();\n  public synchronized void end();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Delete$Rmdir.class": "Compiled from \"Delete.java\"\nclass org.apache.hadoop.fs.shell.Delete {\n  org.apache.hadoop.fs.shell.Delete();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/util/Timer.class": "Compiled from \"Timer.java\"\npublic class org.apache.hadoop.util.Timer {\n  public org.apache.hadoop.util.Timer();\n  public long now();\n  public long monotonicNow();\n}\n", 
  "org/apache/hadoop/io/retry/FailoverProxyProvider.class": "Compiled from \"FailoverProxyProvider.java\"\npublic interface org.apache.hadoop.io.retry.FailoverProxyProvider<T> extends java.io.Closeable {\n  public abstract org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo<T> getProxy();\n  public abstract void performFailover(T);\n  public abstract java.lang.Class<T> getInterface();\n}\n", 
  "org/apache/hadoop/io/ReadaheadPool$ReadaheadRequestImpl.class": "Compiled from \"ReadaheadPool.java\"\npublic class org.apache.hadoop.io.ReadaheadPool {\n  static final org.apache.commons.logging.Log LOG;\n  public static org.apache.hadoop.io.ReadaheadPool getInstance();\n  public org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest readaheadStream(java.lang.String, java.io.FileDescriptor, long, long, long, org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest);\n  public org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest submitReadahead(java.lang.String, java.io.FileDescriptor, long, long);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JRecord$CppRecord.class": "Compiled from \"JRecord.java\"\npublic class org.apache.hadoop.record.compiler.JRecord extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JRecord(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType>>);\n  java.lang.String getSignature();\n  void genCppCode(java.io.FileWriter, java.io.FileWriter, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n  void genJavaCode(java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderExtension$Extension.class": "Compiled from \"KeyProviderExtension.java\"\npublic abstract class org.apache.hadoop.crypto.key.KeyProviderExtension<E extends org.apache.hadoop.crypto.key.KeyProviderExtension$Extension> extends org.apache.hadoop.crypto.key.KeyProvider {\n  public org.apache.hadoop.crypto.key.KeyProviderExtension(org.apache.hadoop.crypto.key.KeyProvider, E);\n  protected E getExtension();\n  protected org.apache.hadoop.crypto.key.KeyProvider getKeyProvider();\n  public boolean isTransient();\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserMappingsProtocolService$1.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager$SASLOwnerACLProvider.class": "Compiled from \"ZKDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent> {\n  public static final java.lang.String ZK_DTSM_ZK_NUM_RETRIES;\n  public static final java.lang.String ZK_DTSM_ZK_SESSION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_SHUTDOWN_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH;\n  public static final java.lang.String ZK_DTSM_ZK_AUTH_TYPE;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_STRING;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_KEYTAB;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_PRINCIPAL;\n  public static final int ZK_DTSM_ZK_NUM_RETRIES_DEFAULT;\n  public static final int ZK_DTSM_ZK_SESSION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_CONNECTION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_SHUTDOWN_TIMEOUT_DEFAULT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH_DEAFULT;\n  public static void setCurator(org.apache.curator.framework.CuratorFramework);\n  public org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager(org.apache.hadoop.conf.Configuration);\n  public void startThreads() throws java.io.IOException;\n  public void stopThreads();\n  protected int getDelegationTokenSeqNum();\n  protected int incrementDelegationTokenSeqNum();\n  protected void setDelegationTokenSeqNum(int);\n  protected int getCurrentKeyId();\n  protected int incrementCurrentKeyId();\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  static java.lang.String getNodePath(java.lang.String, java.lang.String);\n  public java.util.concurrent.ExecutorService getListenerThreadPool();\n  static void access$100(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, byte[]) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, java.lang.String);\n  static void access$300(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static void access$400(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/util/Servers.class": "Compiled from \"Servers.java\"\npublic class org.apache.hadoop.metrics2.util.Servers {\n  public static java.util.List<java.net.InetSocketAddress> parse(java.lang.String, int);\n}\n", 
  "org/apache/hadoop/io/ArrayPrimitiveWritable$Internal.class": "Compiled from \"ArrayPrimitiveWritable.java\"\npublic class org.apache.hadoop.io.ArrayPrimitiveWritable implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.io.ArrayPrimitiveWritable();\n  public org.apache.hadoop.io.ArrayPrimitiveWritable(java.lang.Class<?>);\n  public org.apache.hadoop.io.ArrayPrimitiveWritable(java.lang.Object);\n  public java.lang.Object get();\n  public java.lang.Class<?> getComponentType();\n  public java.lang.Class<?> getDeclaredComponentType();\n  public boolean isDeclaredComponentType(java.lang.Class<?>);\n  public void set(java.lang.Object);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/hash/Hash.class": "Compiled from \"Hash.java\"\npublic abstract class org.apache.hadoop.util.hash.Hash {\n  public static final int INVALID_HASH;\n  public static final int JENKINS_HASH;\n  public static final int MURMUR_HASH;\n  public org.apache.hadoop.util.hash.Hash();\n  public static int parseHashType(java.lang.String);\n  public static int getHashType(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.util.hash.Hash getInstance(int);\n  public static org.apache.hadoop.util.hash.Hash getInstance(org.apache.hadoop.conf.Configuration);\n  public int hash(byte[]);\n  public int hash(byte[], int);\n  public abstract int hash(byte[], int, int);\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProtoOrBuilder.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/StopWatch.class": "Compiled from \"StopWatch.java\"\npublic class org.apache.hadoop.util.StopWatch implements java.io.Closeable {\n  public org.apache.hadoop.util.StopWatch();\n  public boolean isRunning();\n  public org.apache.hadoop.util.StopWatch start();\n  public org.apache.hadoop.util.StopWatch stop();\n  public org.apache.hadoop.util.StopWatch reset();\n  public long now(java.util.concurrent.TimeUnit);\n  public long now();\n  public java.lang.String toString();\n  public void close();\n}\n", 
  "org/apache/hadoop/conf/ReconfigurationUtil.class": "Compiled from \"ReconfigurationUtil.java\"\npublic class org.apache.hadoop.conf.ReconfigurationUtil {\n  public org.apache.hadoop.conf.ReconfigurationUtil();\n  public static java.util.Collection<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange> getChangedProperties(org.apache.hadoop.conf.Configuration, org.apache.hadoop.conf.Configuration);\n  public java.util.Collection<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange> parseChangedProperties(org.apache.hadoop.conf.Configuration, org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/ipc/GenericRefreshProtocol.class": "Compiled from \"GenericRefreshProtocol.java\"\npublic interface org.apache.hadoop.ipc.GenericRefreshProtocol {\n  public static final long versionID;\n  public abstract java.util.Collection<org.apache.hadoop.ipc.RefreshResponse> refresh(java.lang.String, java.lang.String[]) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/FsShellPermissions$Chmod.class": "Compiled from \"FsShellPermissions.java\"\npublic class org.apache.hadoop.fs.FsShellPermissions extends org.apache.hadoop.fs.shell.FsCommand {\n  static org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.fs.FsShellPermissions();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  static java.lang.String access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcRequestHeaderProtoOrBuilder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$ParsedTimeDuration$1.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/AclStatus$1.class": "Compiled from \"AclStatus.java\"\npublic class org.apache.hadoop.fs.permission.AclStatus {\n  public java.lang.String getOwner();\n  public java.lang.String getGroup();\n  public boolean isStickyBit();\n  public java.util.List<org.apache.hadoop.fs.permission.AclEntry> getEntries();\n  public org.apache.hadoop.fs.permission.FsPermission getPermission();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.permission.FsAction getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry);\n  public org.apache.hadoop.fs.permission.FsAction getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry, org.apache.hadoop.fs.permission.FsPermission) throws java.lang.IllegalArgumentException;\n  org.apache.hadoop.fs.permission.AclStatus(java.lang.String, java.lang.String, boolean, java.lang.Iterable, org.apache.hadoop.fs.permission.FsPermission, org.apache.hadoop.fs.permission.AclStatus$1);\n}\n", 
  "org/apache/hadoop/io/compress/snappy/SnappyCompressor.class": "Compiled from \"SnappyCompressor.java\"\npublic class org.apache.hadoop.io.compress.snappy.SnappyCompressor implements org.apache.hadoop.io.compress.Compressor {\n  public static boolean isNativeCodeLoaded();\n  public org.apache.hadoop.io.compress.snappy.SnappyCompressor(int);\n  public org.apache.hadoop.io.compress.snappy.SnappyCompressor();\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public void finish();\n  public boolean finished();\n  public int compress(byte[], int, int) throws java.io.IOException;\n  public void reset();\n  public void reinit(org.apache.hadoop.conf.Configuration);\n  public long getBytesRead();\n  public long getBytesWritten();\n  public void end();\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/LossyRetryInvocationHandler.class": "Compiled from \"LossyRetryInvocationHandler.java\"\npublic class org.apache.hadoop.io.retry.LossyRetryInvocationHandler<T> extends org.apache.hadoop.io.retry.RetryInvocationHandler<T> {\n  public org.apache.hadoop.io.retry.LossyRetryInvocationHandler(int, org.apache.hadoop.io.retry.FailoverProxyProvider<T>, org.apache.hadoop.io.retry.RetryPolicy);\n  public java.lang.Object invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[]) throws java.lang.Throwable;\n  protected java.lang.Object invokeMethod(java.lang.reflect.Method, java.lang.Object[]) throws java.lang.Throwable;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtocolSignature$ProtocolSigFingerprint.class": "Compiled from \"ProtocolSignature.java\"\npublic class org.apache.hadoop.ipc.ProtocolSignature implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.ipc.ProtocolSignature();\n  public org.apache.hadoop.ipc.ProtocolSignature(long, int[]);\n  public long getVersion();\n  public int[] getMethods();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  static int getFingerprint(java.lang.reflect.Method);\n  static int getFingerprint(java.lang.reflect.Method[]);\n  static int getFingerprint(int[]);\n  public static void resetCache();\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(int, long, java.lang.Class<? extends org.apache.hadoop.ipc.VersionedProtocol>);\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(java.lang.String, long) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol, java.lang.String, long, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$GetDelegationTokenResponseProto.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Time.class": "Compiled from \"Time.java\"\npublic final class org.apache.hadoop.util.Time {\n  public org.apache.hadoop.util.Time();\n  public static long now();\n  public static long monotonicNow();\n}\n", 
  "org/apache/hadoop/fs/FileContext$27.class": "", 
  "org/apache/hadoop/fs/FileContext$37.class": "", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToActiveResponseProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SecurityUtil.class": "Compiled from \"SecurityUtil.java\"\npublic class org.apache.hadoop.security.SecurityUtil {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String HOSTNAME_PATTERN;\n  public static final java.lang.String FAILED_TO_GET_UGI_MSG_HEADER;\n  static boolean useIpForTokenService;\n  static org.apache.hadoop.security.SecurityUtil$HostResolver hostResolver;\n  public org.apache.hadoop.security.SecurityUtil();\n  public static void setTokenServiceUseIp(boolean);\n  static boolean isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal);\n  protected static boolean isOriginalTGT(javax.security.auth.kerberos.KerberosTicket);\n  public static java.lang.String getServerPrincipal(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String getServerPrincipal(java.lang.String, java.net.InetAddress) throws java.io.IOException;\n  static java.lang.String getLocalHostName() throws java.net.UnknownHostException;\n  public static void login(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void login(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String buildDTServiceName(java.net.URI, int);\n  public static java.lang.String getHostFromPrincipal(java.lang.String);\n  public static void setSecurityInfoProviders(org.apache.hadoop.security.SecurityInfo...);\n  public static org.apache.hadoop.security.KerberosInfo getKerberosInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.security.token.TokenInfo getTokenInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static java.net.InetSocketAddress getTokenServiceAddr(org.apache.hadoop.security.token.Token<?>);\n  public static void setTokenService(org.apache.hadoop.security.token.Token<?>, java.net.InetSocketAddress);\n  public static org.apache.hadoop.io.Text buildTokenService(java.net.InetSocketAddress);\n  public static org.apache.hadoop.io.Text buildTokenService(java.net.URI);\n  public static <T extends java/lang/Object> T doAsLoginUserOrFatal(java.security.PrivilegedAction<T>);\n  public static <T extends java/lang/Object> T doAsLoginUser(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException;\n  public static <T extends java/lang/Object> T doAsCurrentUser(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException;\n  public static java.net.InetAddress getByName(java.lang.String) throws java.net.UnknownHostException;\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod(org.apache.hadoop.conf.Configuration);\n  public static void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod, org.apache.hadoop.conf.Configuration);\n  public static boolean isPrivilegedPort(int);\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.class": "Compiled from \"AvroReflectSerialization.java\"\npublic class org.apache.hadoop.io.serializer.avro.AvroReflectSerialization extends org.apache.hadoop.io.serializer.avro.AvroSerialization<java.lang.Object> {\n  public static final java.lang.String AVRO_REFLECT_PACKAGES;\n  public org.apache.hadoop.io.serializer.avro.AvroReflectSerialization();\n  public synchronized boolean accept(java.lang.Class<?>);\n  public org.apache.avro.io.DatumReader getReader(java.lang.Class<java.lang.Object>);\n  public org.apache.avro.Schema getSchema(java.lang.Object);\n  public org.apache.avro.io.DatumWriter getWriter(java.lang.Class<java.lang.Object>);\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler$1.class": "Compiled from \"DelegationTokenAuthenticationHandler.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler implements org.apache.hadoop.security.authentication.server.AuthenticationHandler {\n  protected static final java.lang.String TYPE_POSTFIX;\n  public static final java.lang.String PREFIX;\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String DELEGATION_TOKEN_UGI_ATTRIBUTE;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler(org.apache.hadoop.security.authentication.server.AuthenticationHandler);\n  org.apache.hadoop.security.token.delegation.web.DelegationTokenManager getTokenManager();\n  public void init(java.util.Properties) throws javax.servlet.ServletException;\n  public void setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  public void initTokenManager(java.util.Properties);\n  public void destroy();\n  public java.lang.String getType();\n  public boolean managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.authentication.server.AuthenticationToken authenticate(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/TokenSelector.class": "Compiled from \"TokenSelector.java\"\npublic interface org.apache.hadoop.security.token.TokenSelector<T extends org.apache.hadoop.security.token.TokenIdentifier> {\n  public abstract org.apache.hadoop.security.token.Token<T> selectToken(org.apache.hadoop.io.Text, java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>>);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/MetricsException.class": "Compiled from \"MetricsException.java\"\npublic class org.apache.hadoop.metrics.MetricsException extends java.lang.RuntimeException {\n  public org.apache.hadoop.metrics.MetricsException();\n  public org.apache.hadoop.metrics.MetricsException(java.lang.String);\n}\n", 
  "org/apache/hadoop/service/ServiceStateChangeListener.class": "Compiled from \"ServiceStateChangeListener.java\"\npublic interface org.apache.hadoop.service.ServiceStateChangeListener {\n  public abstract void stateChanged(org.apache.hadoop.service.Service);\n}\n", 
  "org/apache/hadoop/io/compress/zlib/ZlibCompressor$CompressionStrategy.class": "Compiled from \"ZlibCompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.ZlibCompressor implements org.apache.hadoop.io.compress.Compressor {\n  static boolean isNativeZlibLoaded();\n  protected final void construct(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader, int);\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor();\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader, int);\n  public void reinit(org.apache.hadoop.conf.Configuration);\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public void finish();\n  public boolean finished();\n  public int compress(byte[], int, int) throws java.io.IOException;\n  public long getBytesWritten();\n  public long getBytesRead();\n  public void reset();\n  public void end();\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/util/XMLUtils.class": "Compiled from \"XMLUtils.java\"\npublic class org.apache.hadoop.util.XMLUtils {\n  public org.apache.hadoop.util.XMLUtils();\n  public static void transform(java.io.InputStream, java.io.InputStream, java.io.Writer) throws javax.xml.transform.TransformerConfigurationException, javax.xml.transform.TransformerException;\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$CancelDelegationTokenResponseProto$1.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Options$StringOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/security/ShellBasedIdMapping.class": "Compiled from \"ShellBasedIdMapping.java\"\npublic class org.apache.hadoop.security.ShellBasedIdMapping implements org.apache.hadoop.security.IdMappingServiceProvider {\n  static final java.lang.String GET_ALL_USERS_CMD;\n  static final java.lang.String GET_ALL_GROUPS_CMD;\n  static final java.lang.String MAC_GET_ALL_USERS_CMD;\n  static final java.lang.String MAC_GET_ALL_GROUPS_CMD;\n  public org.apache.hadoop.security.ShellBasedIdMapping(org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.security.ShellBasedIdMapping(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public long getTimeout();\n  public com.google.common.collect.BiMap<java.lang.Integer, java.lang.String> getUidNameMap();\n  public com.google.common.collect.BiMap<java.lang.Integer, java.lang.String> getGidNameMap();\n  public synchronized void clearNameMaps();\n  public static boolean updateMapInternal(com.google.common.collect.BiMap<java.lang.Integer, java.lang.String>, java.lang.String, java.lang.String, java.lang.String, java.util.Map<java.lang.Integer, java.lang.Integer>) throws java.io.IOException;\n  public synchronized void updateMaps() throws java.io.IOException;\n  static org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping parseStaticMap(java.io.File) throws java.io.IOException;\n  public synchronized int getUid(java.lang.String) throws java.io.IOException;\n  public synchronized int getGid(java.lang.String) throws java.io.IOException;\n  public synchronized java.lang.String getUserName(int, java.lang.String);\n  public synchronized java.lang.String getGroupName(int, java.lang.String);\n  public int getUidAllowingUnknown(java.lang.String);\n  public int getGidAllowingUnknown(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$SpanReceiverListInfo$Builder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/SignalLogger.class": "Compiled from \"SignalLogger.java\"\npublic final class org.apache.hadoop.util.SignalLogger extends java.lang.Enum<org.apache.hadoop.util.SignalLogger> {\n  public static final org.apache.hadoop.util.SignalLogger INSTANCE;\n  public static org.apache.hadoop.util.SignalLogger[] values();\n  public static org.apache.hadoop.util.SignalLogger valueOf(java.lang.String);\n  public void register(org.apache.commons.logging.Log);\n  void register(org.apache.hadoop.util.LogAdapter);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/AclCommands$SetfaclCommand.class": "Compiled from \"AclCommands.java\"\nclass org.apache.hadoop.fs.shell.AclCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.AclCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  static java.lang.String access$000();\n  static java.lang.String access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolInfoService$Stub.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSystemImpl$4.class": "Compiled from \"MetricsSystemImpl.java\"\npublic class org.apache.hadoop.metrics2.impl.MetricsSystemImpl extends org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSource {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String MS_NAME;\n  static final java.lang.String MS_STATS_NAME;\n  static final java.lang.String MS_STATS_DESC;\n  static final java.lang.String MS_CONTROL_NAME;\n  static final java.lang.String MS_INIT_MODE_KEY;\n  org.apache.hadoop.metrics2.lib.MutableStat snapshotStat;\n  org.apache.hadoop.metrics2.lib.MutableStat publishStat;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong droppedPubAll;\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl(java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl();\n  public synchronized org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public synchronized void unregisterSource(java.lang.String);\n  synchronized void registerSource(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource);\n  public synchronized <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  synchronized void registerSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink);\n  public synchronized void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public synchronized void startMetricsMBeans();\n  public synchronized void stopMetricsMBeans();\n  public synchronized java.lang.String currentConfig();\n  synchronized void onTimerEvent();\n  public synchronized void publishMetricsNow();\n  synchronized org.apache.hadoop.metrics2.impl.MetricsBuffer sampleMetrics();\n  synchronized void publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, boolean);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static java.lang.String getHostname();\n  public synchronized void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n  public synchronized boolean shutdown();\n  public org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter getSourceAdapter(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Compression.class": "Compiled from \"Compression.java\"\nfinal class org.apache.hadoop.io.file.tfile.Compression {\n  static final org.apache.commons.logging.Log LOG;\n  static org.apache.hadoop.io.file.tfile.Compression$Algorithm getCompressionAlgorithmByName(java.lang.String);\n  static java.lang.String[] getSupportedAlgorithms();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Ls.class": "Compiled from \"Ls.java\"\nclass org.apache.hadoop.fs.shell.Ls extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  protected final java.text.SimpleDateFormat dateFormat;\n  protected int maxRepl;\n  protected int maxLen;\n  protected int maxOwner;\n  protected int maxGroup;\n  protected java.lang.String lineFormat;\n  protected boolean dirRecurse;\n  protected boolean humanReadable;\n  org.apache.hadoop.fs.shell.Ls();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected java.lang.String formatSize(long);\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processPathArgument(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPaths(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData...) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/metrics/MetricsUtil.class": "Compiled from \"MetricsUtil.java\"\npublic class org.apache.hadoop.metrics.MetricsUtil {\n  public static final org.apache.commons.logging.Log LOG;\n  public static org.apache.hadoop.metrics.MetricsContext getContext(java.lang.String);\n  public static org.apache.hadoop.metrics.MetricsContext getContext(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics.MetricsRecord createRecord(org.apache.hadoop.metrics.MetricsContext, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/service/CompositeService.class": "Compiled from \"CompositeService.java\"\npublic class org.apache.hadoop.service.CompositeService extends org.apache.hadoop.service.AbstractService {\n  protected static final boolean STOP_ONLY_STARTED_SERVICES;\n  public org.apache.hadoop.service.CompositeService(java.lang.String);\n  public java.util.List<org.apache.hadoop.service.Service> getServices();\n  protected void addService(org.apache.hadoop.service.Service);\n  protected boolean addIfService(java.lang.Object);\n  protected synchronized boolean removeService(org.apache.hadoop.service.Service);\n  protected void serviceInit(org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  protected void serviceStart() throws java.lang.Exception;\n  protected void serviceStop() throws java.lang.Exception;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/UniqueNames$Count.class": "Compiled from \"UniqueNames.java\"\npublic class org.apache.hadoop.metrics2.lib.UniqueNames {\n  static final com.google.common.base.Joiner joiner;\n  final java.util.Map<java.lang.String, org.apache.hadoop.metrics2.lib.UniqueNames$Count> map;\n  public org.apache.hadoop.metrics2.lib.UniqueNames();\n  public synchronized java.lang.String uniqueName(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$MonitorHealthResponseProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$AddSpanReceiverResponseProtoOrBuilder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Display$AvroFileInputStream.class": "Compiled from \"Display.java\"\nclass org.apache.hadoop.fs.shell.Display extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.Display();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/metrics2/source/JvmMetrics$1.class": "Compiled from \"JvmMetrics.java\"\npublic class org.apache.hadoop.metrics2.source.JvmMetrics implements org.apache.hadoop.metrics2.MetricsSource {\n  static final float M;\n  final java.lang.management.MemoryMXBean memoryMXBean;\n  final java.util.List<java.lang.management.GarbageCollectorMXBean> gcBeans;\n  final java.lang.management.ThreadMXBean threadMXBean;\n  final java.lang.String processName;\n  final java.lang.String sessionId;\n  final java.util.concurrent.ConcurrentHashMap<java.lang.String, org.apache.hadoop.metrics2.MetricsInfo[]> gcInfoCache;\n  org.apache.hadoop.metrics2.source.JvmMetrics(java.lang.String, java.lang.String);\n  public void setPauseMonitor(org.apache.hadoop.util.JvmPauseMonitor);\n  public static org.apache.hadoop.metrics2.source.JvmMetrics create(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSystem);\n  public static org.apache.hadoop.metrics2.source.JvmMetrics initSingleton(java.lang.String, java.lang.String);\n  public void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$RenewDelegationTokenResponseProtoOrBuilder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/JceAesCtrCryptoCodec$JceAesCtrCipher.class": "Compiled from \"JceAesCtrCryptoCodec.java\"\npublic class org.apache.hadoop.crypto.JceAesCtrCryptoCodec extends org.apache.hadoop.crypto.AesCtrCryptoCodec {\n  public org.apache.hadoop.crypto.JceAesCtrCryptoCodec();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.crypto.Encryptor createEncryptor() throws java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.Decryptor createDecryptor() throws java.security.GeneralSecurityException;\n  public void generateSecureRandom(byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$ActiveNotFoundException.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$CedeActiveResponseProto$1.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ContentSummary$1.class": "Compiled from \"ContentSummary.java\"\npublic class org.apache.hadoop.fs.ContentSummary implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.fs.ContentSummary();\n  public org.apache.hadoop.fs.ContentSummary(long, long, long);\n  public org.apache.hadoop.fs.ContentSummary(long, long, long, long, long, long);\n  public long getLength();\n  public long getDirectoryCount();\n  public long getFileCount();\n  public long getQuota();\n  public long getSpaceConsumed();\n  public long getSpaceQuota();\n  public long getTypeQuota(org.apache.hadoop.fs.StorageType);\n  public long getTypeConsumed(org.apache.hadoop.fs.StorageType);\n  public boolean isTypeQuotaSet();\n  public boolean isTypeConsumedAvailable();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static java.lang.String getHeader(boolean);\n  public java.lang.String toString();\n  public java.lang.String toString(boolean);\n  public java.lang.String toString(boolean, boolean);\n  org.apache.hadoop.fs.ContentSummary(long, long, long, long, long, long, long[], long[], org.apache.hadoop.fs.ContentSummary$1);\n  static {};\n}\n", 
  "org/apache/hadoop/util/VersionUtil.class": "Compiled from \"VersionUtil.java\"\npublic abstract class org.apache.hadoop.util.VersionUtil {\n  public org.apache.hadoop.util.VersionUtil();\n  public static int compareVersions(java.lang.String, java.lang.String);\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$RemoveSpanReceiverResponseProto.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HardLink$HardLinkCGUnix.class": "Compiled from \"HardLink.java\"\npublic class org.apache.hadoop.fs.HardLink {\n  public final org.apache.hadoop.fs.HardLink$LinkStats linkStats;\n  public org.apache.hadoop.fs.HardLink();\n  public static void createHardLink(java.io.File, java.io.File) throws java.io.IOException;\n  public static void createHardLinkMult(java.io.File, java.lang.String[], java.io.File) throws java.io.IOException;\n  public static int getLinkCount(java.io.File) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolServerSideTranslatorPB.class": "Compiled from \"GetUserMappingsProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB implements org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB {\n  public org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB(org.apache.hadoop.tools.GetUserMappingsProtocol);\n  public org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto getGroupsForUser(com.google.protobuf.RpcController, org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto) throws com.google.protobuf.ServiceException;\n}\n", 
  "org/apache/hadoop/fs/CommonConfigurationKeys.class": "Compiled from \"CommonConfigurationKeys.java\"\npublic class org.apache.hadoop.fs.CommonConfigurationKeys extends org.apache.hadoop.fs.CommonConfigurationKeysPublic {\n  public static final java.lang.String FS_HOME_DIR_KEY;\n  public static final java.lang.String FS_HOME_DIR_DEFAULT;\n  public static final java.lang.String FS_PERMISSIONS_UMASK_KEY;\n  public static final int FS_PERMISSIONS_UMASK_DEFAULT;\n  public static final java.lang.String IPC_PING_INTERVAL_KEY;\n  public static final int IPC_PING_INTERVAL_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_PING_KEY;\n  public static final boolean IPC_CLIENT_PING_DEFAULT;\n  public static final java.lang.String IPC_SERVER_RPC_MAX_RESPONSE_SIZE_KEY;\n  public static final int IPC_SERVER_RPC_MAX_RESPONSE_SIZE_DEFAULT;\n  public static final java.lang.String IPC_SERVER_RPC_READ_THREADS_KEY;\n  public static final int IPC_SERVER_RPC_READ_THREADS_DEFAULT;\n  public static final java.lang.String IPC_SERVER_RPC_READ_CONNECTION_QUEUE_SIZE_KEY;\n  public static final int IPC_SERVER_RPC_READ_CONNECTION_QUEUE_SIZE_DEFAULT;\n  public static final java.lang.String IPC_MAXIMUM_DATA_LENGTH;\n  public static final int IPC_MAXIMUM_DATA_LENGTH_DEFAULT;\n  public static final java.lang.String IPC_SERVER_HANDLER_QUEUE_SIZE_KEY;\n  public static final int IPC_SERVER_HANDLER_QUEUE_SIZE_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_NAMESPACE;\n  public static final java.lang.String IPC_CALLQUEUE_IMPL_KEY;\n  public static final java.lang.String IPC_CALLQUEUE_IDENTITY_PROVIDER_KEY;\n  public static final java.lang.String NET_TOPOLOGY_CONFIGURED_NODE_MAPPING_KEY;\n  public static final java.lang.String IO_COMPRESSION_CODECS_KEY;\n  public static final java.lang.String IO_COMPRESSION_CODEC_LZO_BUFFERSIZE_KEY;\n  public static final int IO_COMPRESSION_CODEC_LZO_BUFFERSIZE_DEFAULT;\n  public static final java.lang.String IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_KEY;\n  public static final int IO_COMPRESSION_CODEC_SNAPPY_BUFFERSIZE_DEFAULT;\n  public static final java.lang.String IO_COMPRESSION_CODEC_LZ4_BUFFERSIZE_KEY;\n  public static final int IO_COMPRESSION_CODEC_LZ4_BUFFERSIZE_DEFAULT;\n  public static final java.lang.String IO_COMPRESSION_CODEC_LZ4_USELZ4HC_KEY;\n  public static final boolean IO_COMPRESSION_CODEC_LZ4_USELZ4HC_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_AUTHORIZATION_DEFAULT_ACL;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_AUTHORIZATION_DEFAULT_BLOCKED_ACL;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_AUTHORIZATION_REFRESH_POLICY;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_AUTHORIZATION_GET_USER_MAPPINGS;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_AUTHORIZATION_REFRESH_USER_MAPPINGS;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_AUTHORIZATION_REFRESH_CALLQUEUE;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_AUTHORIZATION_GENERIC_REFRESH;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_AUTHORIZATION_TRACING;\n  public static final java.lang.String SECURITY_HA_SERVICE_PROTOCOL_ACL;\n  public static final java.lang.String SECURITY_ZKFC_PROTOCOL_ACL;\n  public static final java.lang.String SECURITY_CLIENT_PROTOCOL_ACL;\n  public static final java.lang.String SECURITY_CLIENT_DATANODE_PROTOCOL_ACL;\n  public static final java.lang.String SECURITY_DATANODE_PROTOCOL_ACL;\n  public static final java.lang.String SECURITY_INTER_DATANODE_PROTOCOL_ACL;\n  public static final java.lang.String SECURITY_NAMENODE_PROTOCOL_ACL;\n  public static final java.lang.String SECURITY_QJOURNAL_SERVICE_PROTOCOL_ACL;\n  public static final java.lang.String HADOOP_SECURITY_TOKEN_SERVICE_USE_IP;\n  public static final boolean HADOOP_SECURITY_TOKEN_SERVICE_USE_IP_DEFAULT;\n  public static final java.lang.String HA_HM_CONNECT_RETRY_INTERVAL_KEY;\n  public static final long HA_HM_CONNECT_RETRY_INTERVAL_DEFAULT;\n  public static final java.lang.String HA_HM_CHECK_INTERVAL_KEY;\n  public static final long HA_HM_CHECK_INTERVAL_DEFAULT;\n  public static final java.lang.String HA_HM_SLEEP_AFTER_DISCONNECT_KEY;\n  public static final long HA_HM_SLEEP_AFTER_DISCONNECT_DEFAULT;\n  public static final java.lang.String HA_HM_RPC_TIMEOUT_KEY;\n  public static final int HA_HM_RPC_TIMEOUT_DEFAULT;\n  public static final java.lang.String HA_FC_NEW_ACTIVE_TIMEOUT_KEY;\n  public static final int HA_FC_NEW_ACTIVE_TIMEOUT_DEFAULT;\n  public static final java.lang.String HA_FC_GRACEFUL_FENCE_TIMEOUT_KEY;\n  public static final int HA_FC_GRACEFUL_FENCE_TIMEOUT_DEFAULT;\n  public static final java.lang.String HA_FC_GRACEFUL_FENCE_CONNECTION_RETRIES;\n  public static final int HA_FC_GRACEFUL_FENCE_CONNECTION_RETRIES_DEFAULT;\n  public static final java.lang.String HA_FC_ELECTOR_ZK_OP_RETRIES_KEY;\n  public static final int HA_FC_ELECTOR_ZK_OP_RETRIES_DEFAULT;\n  public static final java.lang.String HA_FC_CLI_CHECK_TIMEOUT_KEY;\n  public static final int HA_FC_CLI_CHECK_TIMEOUT_DEFAULT;\n  public static final java.lang.String HADOOP_HTTP_STATIC_USER;\n  public static final java.lang.String DEFAULT_HADOOP_HTTP_STATIC_USER;\n  public static final java.lang.String HADOOP_USER_GROUP_STATIC_OVERRIDES;\n  public static final java.lang.String HADOOP_USER_GROUP_STATIC_OVERRIDES_DEFAULT;\n  public static final java.lang.String HADOOP_JETTY_LOGS_SERVE_ALIASES;\n  public static final boolean DEFAULT_HADOOP_JETTY_LOGS_SERVE_ALIASES;\n  public static final java.lang.String KERBEROS_TICKET_CACHE_PATH;\n  public static final java.lang.String HADOOP_SECURITY_UID_NAME_CACHE_TIMEOUT_KEY;\n  public static final long HADOOP_SECURITY_UID_NAME_CACHE_TIMEOUT_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY;\n  public static final boolean IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SASL_KEY;\n  public static final int IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SASL_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_CONNECTION_IDLESCANINTERVAL_KEY;\n  public static final int IPC_CLIENT_CONNECTION_IDLESCANINTERVAL_DEFAULT;\n  public static final java.lang.String HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS;\n  public static final java.lang.String RPC_METRICS_QUANTILE_ENABLE;\n  public static final boolean RPC_METRICS_QUANTILE_ENABLE_DEFAULT;\n  public static final java.lang.String RPC_METRICS_PERCENTILES_INTERVALS_KEY;\n  public static final java.lang.String NFS_EXPORTS_ALLOWED_HOSTS_SEPARATOR;\n  public static final java.lang.String NFS_EXPORTS_ALLOWED_HOSTS_KEY;\n  public static final java.lang.String NFS_EXPORTS_ALLOWED_HOSTS_KEY_DEFAULT;\n  public org.apache.hadoop.fs.CommonConfigurationKeys();\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableMetricsFactory.class": "Compiled from \"MutableMetricsFactory.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableMetricsFactory {\n  public org.apache.hadoop.metrics2.lib.MutableMetricsFactory();\n  org.apache.hadoop.metrics2.lib.MutableMetric newForField(java.lang.reflect.Field, org.apache.hadoop.metrics2.annotation.Metric, org.apache.hadoop.metrics2.lib.MetricsRegistry);\n  org.apache.hadoop.metrics2.lib.MutableMetric newForMethod(java.lang.Object, java.lang.reflect.Method, org.apache.hadoop.metrics2.annotation.Metric, org.apache.hadoop.metrics2.lib.MetricsRegistry);\n  protected org.apache.hadoop.metrics2.lib.MutableMetric newForField(java.lang.reflect.Field, org.apache.hadoop.metrics2.annotation.Metric);\n  protected org.apache.hadoop.metrics2.lib.MutableMetric newForMethod(java.lang.Object, java.lang.reflect.Method, org.apache.hadoop.metrics2.annotation.Metric);\n  protected org.apache.hadoop.metrics2.MetricsInfo getInfo(org.apache.hadoop.metrics2.annotation.Metric, java.lang.reflect.Field);\n  protected java.lang.String getName(java.lang.reflect.Field);\n  protected org.apache.hadoop.metrics2.MetricsInfo getInfo(org.apache.hadoop.metrics2.annotation.Metric, java.lang.reflect.Method);\n  protected org.apache.hadoop.metrics2.MetricsInfo getInfo(java.lang.Class<?>, org.apache.hadoop.metrics2.annotation.Metrics);\n  protected java.lang.String getName(java.lang.reflect.Method);\n  protected org.apache.hadoop.metrics2.MetricsInfo getInfo(org.apache.hadoop.metrics2.annotation.Metric, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/OpensslCipher$Transform.class": "Compiled from \"OpensslCipher.java\"\npublic final class org.apache.hadoop.crypto.OpensslCipher {\n  public static final int ENCRYPT_MODE;\n  public static final int DECRYPT_MODE;\n  public static java.lang.String getLoadingFailureReason();\n  public static final org.apache.hadoop.crypto.OpensslCipher getInstance(java.lang.String) throws java.security.NoSuchAlgorithmException, javax.crypto.NoSuchPaddingException;\n  public void init(int, byte[], byte[]);\n  public int update(java.nio.ByteBuffer, java.nio.ByteBuffer) throws javax.crypto.ShortBufferException;\n  public int doFinal(java.nio.ByteBuffer) throws javax.crypto.ShortBufferException, javax.crypto.IllegalBlockSizeException, javax.crypto.BadPaddingException;\n  public void clean();\n  protected void finalize() throws java.lang.Throwable;\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RpcNoSuchProtocolException.class": "Compiled from \"RpcNoSuchProtocolException.java\"\npublic class org.apache.hadoop.ipc.RpcNoSuchProtocolException extends org.apache.hadoop.ipc.RpcServerException {\n  public org.apache.hadoop.ipc.RpcNoSuchProtocolException(java.lang.String);\n  public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto getRpcStatusProto();\n  public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto getRpcErrorCodeProto();\n}\n", 
  "org/apache/hadoop/io/compress/CodecPool$1.class": "Compiled from \"CodecPool.java\"\npublic class org.apache.hadoop.io.compress.CodecPool {\n  public org.apache.hadoop.io.compress.CodecPool();\n  public static org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec);\n  public static org.apache.hadoop.io.compress.Decompressor getDecompressor(org.apache.hadoop.io.compress.CompressionCodec);\n  public static void returnCompressor(org.apache.hadoop.io.compress.Compressor);\n  public static void returnDecompressor(org.apache.hadoop.io.compress.Decompressor);\n  public static int getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec);\n  public static int getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec);\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$POSIX$CachedName.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns$Info.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$ConnectionManager.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolSignatureResponseProtoOrBuilder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$4.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/BufferedFSInputStream.class": "Compiled from \"BufferedFSInputStream.java\"\npublic class org.apache.hadoop.fs.BufferedFSInputStream extends java.io.BufferedInputStream implements org.apache.hadoop.fs.Seekable,org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.HasFileDescriptor {\n  public org.apache.hadoop.fs.BufferedFSInputStream(org.apache.hadoop.fs.FSInputStream, int);\n  public long getPos() throws java.io.IOException;\n  public long skip(long) throws java.io.IOException;\n  public void seek(long) throws java.io.IOException;\n  public boolean seekToNewSource(long) throws java.io.IOException;\n  public int read(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[]) throws java.io.IOException;\n  public java.io.FileDescriptor getFileDescriptor() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProtoOrBuilder.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSystemImpl.class": "Compiled from \"MetricsSystemImpl.java\"\npublic class org.apache.hadoop.metrics2.impl.MetricsSystemImpl extends org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSource {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String MS_NAME;\n  static final java.lang.String MS_STATS_NAME;\n  static final java.lang.String MS_STATS_DESC;\n  static final java.lang.String MS_CONTROL_NAME;\n  static final java.lang.String MS_INIT_MODE_KEY;\n  org.apache.hadoop.metrics2.lib.MutableStat snapshotStat;\n  org.apache.hadoop.metrics2.lib.MutableStat publishStat;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong droppedPubAll;\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl(java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl();\n  public synchronized org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public synchronized void unregisterSource(java.lang.String);\n  synchronized void registerSource(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource);\n  public synchronized <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  synchronized void registerSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink);\n  public synchronized void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public synchronized void startMetricsMBeans();\n  public synchronized void stopMetricsMBeans();\n  public synchronized java.lang.String currentConfig();\n  synchronized void onTimerEvent();\n  public synchronized void publishMetricsNow();\n  synchronized org.apache.hadoop.metrics2.impl.MetricsBuffer sampleMetrics();\n  synchronized void publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, boolean);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static java.lang.String getHostname();\n  public synchronized void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n  public synchronized boolean shutdown();\n  public org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter getSourceAdapter(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableCounterInt.class": "Compiled from \"MutableCounterInt.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableCounterInt extends org.apache.hadoop.metrics2.lib.MutableCounter {\n  org.apache.hadoop.metrics2.lib.MutableCounterInt(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public void incr();\n  public synchronized void incr(int);\n  public int value();\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$6.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/DataOutputByteBuffer$Buffer.class": "Compiled from \"DataOutputByteBuffer.java\"\npublic class org.apache.hadoop.io.DataOutputByteBuffer extends java.io.DataOutputStream {\n  public org.apache.hadoop.io.DataOutputByteBuffer();\n  public org.apache.hadoop.io.DataOutputByteBuffer(int);\n  public org.apache.hadoop.io.DataOutputByteBuffer(int, boolean);\n  public java.nio.ByteBuffer[] getData();\n  public int getLength();\n  public void reset();\n}\n", 
  "org/apache/hadoop/io/BytesWritable.class": "Compiled from \"BytesWritable.java\"\npublic class org.apache.hadoop.io.BytesWritable extends org.apache.hadoop.io.BinaryComparable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.BinaryComparable> {\n  public org.apache.hadoop.io.BytesWritable();\n  public org.apache.hadoop.io.BytesWritable(byte[]);\n  public org.apache.hadoop.io.BytesWritable(byte[], int);\n  public byte[] copyBytes();\n  public byte[] getBytes();\n  public byte[] get();\n  public int getLength();\n  public int getSize();\n  public void setSize(int);\n  public int getCapacity();\n  public void setCapacity(int);\n  public void set(org.apache.hadoop.io.BytesWritable);\n  public void set(byte[], int, int);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetUserMappingsProtocolService.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.class": "Compiled from \"JniBasedUnixGroupsNetgroupMapping.java\"\npublic class org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping extends org.apache.hadoop.security.JniBasedUnixGroupsMapping {\n  public org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping();\n  native java.lang.String[] getUsersForNetgroupJNI(java.lang.String);\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void cacheGroupsRefresh() throws java.io.IOException;\n  public void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n  protected synchronized java.util.List<java.lang.String> getUsersForNetgroup(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$1.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Reader$OnlyHeaderOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderCryptoExtension$DefaultCryptoExtension$1.class": "Compiled from \"KeyProviderCryptoExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderCryptoExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension> {\n  public static final java.lang.String EEK;\n  public static final java.lang.String EK;\n  protected org.apache.hadoop.crypto.key.KeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider, org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension);\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public static org.apache.hadoop.crypto.key.KeyProviderCryptoExtension createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/serializer/Serialization.class": "Compiled from \"Serialization.java\"\npublic interface org.apache.hadoop.io.serializer.Serialization<T> {\n  public abstract boolean accept(java.lang.Class<?>);\n  public abstract org.apache.hadoop.io.serializer.Serializer<T> getSerializer(java.lang.Class<T>);\n  public abstract org.apache.hadoop.io.serializer.Deserializer<T> getDeserializer(java.lang.Class<T>);\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$2.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileStatus.class": "Compiled from \"FileStatus.java\"\npublic class org.apache.hadoop.fs.FileStatus implements org.apache.hadoop.io.Writable,java.lang.Comparable {\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.fs.FileStatus();\n  public org.apache.hadoop.fs.FileStatus(long, boolean, int, long, long, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus(long, boolean, int, long, long, long, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus(long, boolean, int, long, long, long, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus(org.apache.hadoop.fs.FileStatus) throws java.io.IOException;\n  public long getLen();\n  public boolean isFile();\n  public boolean isDirectory();\n  public boolean isDir();\n  public boolean isSymlink();\n  public long getBlockSize();\n  public short getReplication();\n  public long getModificationTime();\n  public long getAccessTime();\n  public org.apache.hadoop.fs.permission.FsPermission getPermission();\n  public boolean isEncrypted();\n  public java.lang.String getOwner();\n  public java.lang.String getGroup();\n  public org.apache.hadoop.fs.Path getPath();\n  public void setPath(org.apache.hadoop.fs.Path);\n  protected void setPermission(org.apache.hadoop.fs.permission.FsPermission);\n  protected void setOwner(java.lang.String);\n  protected void setGroup(java.lang.String);\n  public org.apache.hadoop.fs.Path getSymlink() throws java.io.IOException;\n  public void setSymlink(org.apache.hadoop.fs.Path);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public int compareTo(java.lang.Object);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$AddSpanReceiverRequestProto.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/PermissionStatus$2.class": "Compiled from \"PermissionStatus.java\"\npublic class org.apache.hadoop.fs.permission.PermissionStatus implements org.apache.hadoop.io.Writable {\n  static final org.apache.hadoop.io.WritableFactory FACTORY;\n  public static org.apache.hadoop.fs.permission.PermissionStatus createImmutable(java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.permission.PermissionStatus(java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission);\n  public java.lang.String getUserName();\n  public java.lang.String getGroupName();\n  public org.apache.hadoop.fs.permission.FsPermission getPermission();\n  public org.apache.hadoop.fs.permission.PermissionStatus applyUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public static org.apache.hadoop.fs.permission.PermissionStatus read(java.io.DataInput) throws java.io.IOException;\n  public static void write(java.io.DataOutput, java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public java.lang.String toString();\n  org.apache.hadoop.fs.permission.PermissionStatus(org.apache.hadoop.fs.permission.PermissionStatus$1);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$3.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsShellPermissions$Chgrp.class": "Compiled from \"FsShellPermissions.java\"\npublic class org.apache.hadoop.fs.FsShellPermissions extends org.apache.hadoop.fs.shell.FsCommand {\n  static org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.fs.FsShellPermissions();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  static java.lang.String access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ListSpanReceiversRequestProtoOrBuilder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtobufHelper.class": "Compiled from \"ProtobufHelper.java\"\npublic class org.apache.hadoop.ipc.ProtobufHelper {\n  public static java.io.IOException getRemoteException(com.google.protobuf.ServiceException);\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminProtocol.class": "Compiled from \"TraceAdminProtocol.java\"\npublic interface org.apache.hadoop.tracing.TraceAdminProtocol {\n  public static final long versionID;\n  public abstract org.apache.hadoop.tracing.SpanReceiverInfo[] listSpanReceivers() throws java.io.IOException;\n  public abstract long addSpanReceiver(org.apache.hadoop.tracing.SpanReceiverInfo) throws java.io.IOException;\n  public abstract void removeSpanReceiver(long) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/IndexedSorter.class": "Compiled from \"IndexedSorter.java\"\npublic interface org.apache.hadoop.util.IndexedSorter {\n  public abstract void sort(org.apache.hadoop.util.IndexedSortable, int, int);\n  public abstract void sort(org.apache.hadoop.util.IndexedSortable, int, int, org.apache.hadoop.util.Progressable);\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceProtocolService$BlockingStub.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Compression$Algorithm$3.class": "Compiled from \"Compression.java\"\nfinal class org.apache.hadoop.io.file.tfile.Compression {\n  static final org.apache.commons.logging.Log LOG;\n  static org.apache.hadoop.io.file.tfile.Compression$Algorithm getCompressionAlgorithmByName(java.lang.String);\n  static java.lang.String[] getSupportedAlgorithms();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns$CacheWith2Keys$2.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/StandbyException.class": "Compiled from \"StandbyException.java\"\npublic class org.apache.hadoop.ipc.StandbyException extends java.io.IOException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.ipc.StandbyException(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/shell/CopyCommands$AppendToFile.class": "Compiled from \"CopyCommands.java\"\nclass org.apache.hadoop.fs.shell.CopyCommands {\n  org.apache.hadoop.fs.shell.CopyCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/http/HttpServer2.class": "Compiled from \"HttpServer2.java\"\npublic final class org.apache.hadoop.http.HttpServer2 implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  public static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean);\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public java.net.InetSocketAddress getConnectorAddress(int);\n  public void setThreads(int, int);\n  public void start() throws java.io.IOException;\n  void openListeners() throws java.lang.Exception;\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  org.apache.hadoop.http.HttpServer2(org.apache.hadoop.http.HttpServer2$Builder, org.apache.hadoop.http.HttpServer2$1) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.http.HttpServer2, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.http.HttpServer2, org.mortbay.jetty.Connector);\n  static void access$300(org.apache.hadoop.http.HttpServer2);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$2.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto$SaslState$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/GSetByHashMap.class": "Compiled from \"GSetByHashMap.java\"\npublic class org.apache.hadoop.util.GSetByHashMap<K, E extends K> implements org.apache.hadoop.util.GSet<K, E> {\n  public org.apache.hadoop.util.GSetByHashMap(int, float);\n  public int size();\n  public boolean contains(K);\n  public E get(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  public void clear();\n}\n", 
  "org/apache/hadoop/util/Shell.class": "Compiled from \"Shell.java\"\npublic abstract class org.apache.hadoop.util.Shell {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int WINDOWS_MAX_SHELL_LENGHT;\n  public static final java.lang.String USER_NAME_COMMAND;\n  public static final java.lang.Object WindowsProcessLaunchLock;\n  public static final org.apache.hadoop.util.Shell$OSType osType;\n  public static final boolean WINDOWS;\n  public static final boolean SOLARIS;\n  public static final boolean MAC;\n  public static final boolean FREEBSD;\n  public static final boolean LINUX;\n  public static final boolean OTHER;\n  public static final boolean PPC_64;\n  public static final java.lang.String SET_PERMISSION_COMMAND;\n  public static final java.lang.String SET_OWNER_COMMAND;\n  public static final java.lang.String SET_GROUP_COMMAND;\n  public static final java.lang.String LINK_COMMAND;\n  public static final java.lang.String READ_LINK_COMMAND;\n  protected long timeOutInterval;\n  public static final java.lang.String WINUTILS;\n  public static final boolean isSetsidAvailable;\n  public static final java.lang.String TOKEN_SEPARATOR_REGEX;\n  public static boolean isJava7OrAbove();\n  public static void checkWindowsCommandLineLength(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String[] getGroupsCommand();\n  public static java.lang.String[] getGroupsForUserCommand(java.lang.String);\n  public static java.lang.String[] getUsersForNetgroupCommand(java.lang.String);\n  public static java.lang.String[] getGetPermissionCommand();\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean);\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean, java.lang.String);\n  public static java.lang.String[] getSetOwnerCommand(java.lang.String);\n  public static java.lang.String[] getSymlinkCommand(java.lang.String, java.lang.String);\n  public static java.lang.String[] getReadlinkCommand(java.lang.String);\n  public static java.lang.String[] getCheckProcessIsAliveCommand(java.lang.String);\n  public static java.lang.String[] getSignalKillCommand(int, java.lang.String);\n  public static java.lang.String getEnvironmentVariableRegex();\n  public static java.io.File appendScriptExtension(java.io.File, java.lang.String);\n  public static java.lang.String appendScriptExtension(java.lang.String);\n  public static java.lang.String[] getRunScriptCommand(java.io.File);\n  public static final java.lang.String getHadoopHome() throws java.io.IOException;\n  public static final java.lang.String getQualifiedBinPath(java.lang.String) throws java.io.IOException;\n  public static final java.lang.String getWinUtilsPath();\n  public org.apache.hadoop.util.Shell();\n  public org.apache.hadoop.util.Shell(long);\n  public org.apache.hadoop.util.Shell(long, boolean);\n  protected void setEnvironment(java.util.Map<java.lang.String, java.lang.String>);\n  protected void setWorkingDirectory(java.io.File);\n  protected void run() throws java.io.IOException;\n  protected abstract java.lang.String[] getExecString();\n  protected abstract void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public java.lang.String getEnvironment(java.lang.String);\n  public java.lang.Process getProcess();\n  public int getExitCode();\n  public boolean isTimedOut();\n  public static java.lang.String execCommand(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String[], long) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String...) throws java.io.IOException;\n  static java.util.concurrent.atomic.AtomicBoolean access$000(org.apache.hadoop.util.Shell);\n  static void access$100(org.apache.hadoop.util.Shell);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$CompressionOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetUserMappingsProtocolService$BlockingStub.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HtmlQuoting$1.class": "Compiled from \"HtmlQuoting.java\"\npublic class org.apache.hadoop.http.HtmlQuoting {\n  public org.apache.hadoop.http.HtmlQuoting();\n  public static boolean needsQuoting(byte[], int, int);\n  public static boolean needsQuoting(java.lang.String);\n  public static void quoteHtmlChars(java.io.OutputStream, byte[], int, int) throws java.io.IOException;\n  public static java.lang.String quoteHtmlChars(java.lang.String);\n  public static java.io.OutputStream quoteOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public static java.lang.String unquoteHtmlChars(java.lang.String);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HAServiceProtocol$RequestSource.class": "Compiled from \"HAServiceProtocol.java\"\npublic interface org.apache.hadoop.ha.HAServiceProtocol {\n  public static final long versionID;\n  public abstract void monitorHealth() throws org.apache.hadoop.ha.HealthCheckFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract void transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws org.apache.hadoop.ha.ServiceFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract void transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws org.apache.hadoop.ha.ServiceFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract org.apache.hadoop.ha.HAServiceStatus getServiceStatus() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$MetadataOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto$Builder.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ListSpanReceiversRequestProto$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto$SaslAuthOrBuilder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/DataInputBuffer$Buffer.class": "Compiled from \"DataInputBuffer.java\"\npublic class org.apache.hadoop.io.DataInputBuffer extends java.io.DataInputStream {\n  public org.apache.hadoop.io.DataInputBuffer();\n  public void reset(byte[], int);\n  public void reset(byte[], int, int);\n  public byte[] getData();\n  public int getPosition();\n  public int getLength();\n}\n", 
  "org/apache/hadoop/util/IdGenerator.class": "Compiled from \"IdGenerator.java\"\npublic interface org.apache.hadoop.util.IdGenerator {\n  public abstract long nextValue();\n}\n", 
  "org/apache/hadoop/fs/ftp/FTPFileSystem$1.class": "Compiled from \"FTPFileSystem.java\"\npublic class org.apache.hadoop.fs.ftp.FTPFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int DEFAULT_BUFFER_SIZE;\n  public static final int DEFAULT_BLOCK_SIZE;\n  public static final java.lang.String FS_FTP_USER_PREFIX;\n  public static final java.lang.String FS_FTP_HOST;\n  public static final java.lang.String FS_FTP_HOST_PORT;\n  public static final java.lang.String FS_FTP_PASSWORD_PREFIX;\n  public static final java.lang.String E_SAME_DIRECTORY_ONLY;\n  public org.apache.hadoop.fs.ftp.FTPFileSystem();\n  public java.lang.String getScheme();\n  protected int getDefaultPort();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public java.net.URI getUri();\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  static void access$000(org.apache.hadoop.fs.ftp.FTPFileSystem, org.apache.commons.net.ftp.FTPClient) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsException.class": "Compiled from \"MetricsException.java\"\npublic class org.apache.hadoop.metrics2.MetricsException extends java.lang.RuntimeException {\n  public org.apache.hadoop.metrics2.MetricsException(java.lang.String);\n  public org.apache.hadoop.metrics2.MetricsException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.metrics2.MetricsException(java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$GetServiceStatusRequestProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/IntWritable.class": "Compiled from \"IntWritable.java\"\npublic class org.apache.hadoop.io.IntWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.IntWritable> {\n  public org.apache.hadoop.io.IntWritable();\n  public org.apache.hadoop.io.IntWritable(int);\n  public void set(int);\n  public int get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.IntWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/metrics/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.ipc.metrics.package-info {\n}\n", 
  "org/apache/hadoop/util/HostsFileReader.class": "Compiled from \"HostsFileReader.java\"\npublic class org.apache.hadoop.util.HostsFileReader {\n  public org.apache.hadoop.util.HostsFileReader(java.lang.String, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.util.HostsFileReader(java.lang.String, java.io.InputStream, java.lang.String, java.io.InputStream) throws java.io.IOException;\n  public static void readFileToSet(java.lang.String, java.lang.String, java.util.Set<java.lang.String>) throws java.io.IOException;\n  public static void readFileToSetWithFileInputStream(java.lang.String, java.lang.String, java.io.InputStream, java.util.Set<java.lang.String>) throws java.io.IOException;\n  public synchronized void refresh() throws java.io.IOException;\n  public synchronized void refresh(java.io.InputStream, java.io.InputStream) throws java.io.IOException;\n  public synchronized java.util.Set<java.lang.String> getHosts();\n  public synchronized java.util.Set<java.lang.String> getExcludedHosts();\n  public synchronized void setIncludesFile(java.lang.String);\n  public synchronized void setExcludesFile(java.lang.String);\n  public synchronized void updateFileNames(java.lang.String, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/util/curator/ChildReaper$State.class": "Compiled from \"ChildReaper.java\"\npublic class org.apache.hadoop.util.curator.ChildReaper implements java.io.Closeable {\n  static final int DEFAULT_REAPING_THRESHOLD_MS;\n  public static <E extends java/lang/Object> java.util.Set<E> newConcurrentHashSet();\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, int);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, java.util.concurrent.ScheduledExecutorService, int);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, java.util.concurrent.ScheduledExecutorService, int, java.lang.String);\n  public void start() throws java.lang.Exception;\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.util.curator.ChildReaper addPath(java.lang.String);\n  public boolean removePath(java.lang.String);\n  static void access$000(org.apache.hadoop.util.curator.ChildReaper);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer$QuotingInputFilter$RequestQuoter.class": "Compiled from \"HttpServer.java\"\npublic class org.apache.hadoop.http.HttpServer implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.Connector listener;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[]) throws java.io.IOException;\n  public org.mortbay.jetty.Connector createBaseListener(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean) throws java.io.IOException;\n  protected void addContext(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public void setThreads(int, int);\n  public void addSslListener(java.net.InetSocketAddress, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void addSslListener(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  protected void initSpnego(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void start() throws java.io.IOException;\n  void openListener() throws java.lang.Exception;\n  public java.net.InetSocketAddress getListenerAddress();\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  static org.apache.hadoop.security.ssl.SSLFactory access$000(org.apache.hadoop.http.HttpServer);\n  static {};\n}\n", 
  "org/apache/hadoop/net/ScriptBasedMappingWithDependency.class": "Compiled from \"ScriptBasedMappingWithDependency.java\"\npublic class org.apache.hadoop.net.ScriptBasedMappingWithDependency extends org.apache.hadoop.net.ScriptBasedMapping implements org.apache.hadoop.net.DNSToSwitchMappingWithDependency {\n  static final java.lang.String DEPENDENCY_SCRIPT_FILENAME_KEY;\n  public org.apache.hadoop.net.ScriptBasedMappingWithDependency();\n  public java.lang.String toString();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public java.util.List<java.lang.String> getDependency(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/FsShell$Usage.class": "Compiled from \"FsShell.java\"\npublic class org.apache.hadoop.fs.FsShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.fs.shell.CommandFactory commandFactory;\n  public org.apache.hadoop.fs.FsShell();\n  public org.apache.hadoop.fs.FsShell(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.fs.FileSystem getFS() throws java.io.IOException;\n  protected org.apache.hadoop.fs.Trash getTrash() throws java.io.IOException;\n  protected void init() throws java.io.IOException;\n  protected void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.Path getCurrentTrashDir() throws java.io.IOException;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public void close() throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  protected static org.apache.hadoop.fs.FsShell newShellInstance();\n  static void access$000(org.apache.hadoop.fs.FsShell, java.io.PrintStream);\n  static void access$100(org.apache.hadoop.fs.FsShell, java.io.PrintStream, java.lang.String);\n  static void access$200(org.apache.hadoop.fs.FsShell, java.io.PrintStream);\n  static void access$300(org.apache.hadoop.fs.FsShell, java.io.PrintStream, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricGaugeInt.class": "Compiled from \"MetricGaugeInt.java\"\nclass org.apache.hadoop.metrics2.impl.MetricGaugeInt extends org.apache.hadoop.metrics2.AbstractMetric {\n  final int value;\n  org.apache.hadoop.metrics2.impl.MetricGaugeInt(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public java.lang.Integer value();\n  public org.apache.hadoop.metrics2.MetricType type();\n  public void visit(org.apache.hadoop.metrics2.MetricsVisitor);\n  public java.lang.Number value();\n}\n", 
  "org/apache/hadoop/io/VIntWritable.class": "Compiled from \"VIntWritable.java\"\npublic class org.apache.hadoop.io.VIntWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.VIntWritable> {\n  public org.apache.hadoop.io.VIntWritable();\n  public org.apache.hadoop.io.VIntWritable(int);\n  public void set(int);\n  public int get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.VIntWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/http/HttpServer$StackServlet.class": "Compiled from \"HttpServer.java\"\npublic class org.apache.hadoop.http.HttpServer implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.Connector listener;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[]) throws java.io.IOException;\n  public org.mortbay.jetty.Connector createBaseListener(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean) throws java.io.IOException;\n  protected void addContext(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public void setThreads(int, int);\n  public void addSslListener(java.net.InetSocketAddress, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void addSslListener(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  protected void initSpnego(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void start() throws java.io.IOException;\n  void openListener() throws java.lang.Exception;\n  public java.net.InetSocketAddress getListenerAddress();\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  static org.apache.hadoop.security.ssl.SSLFactory access$000(org.apache.hadoop.http.HttpServer);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/BaseExpression.class": "Compiled from \"BaseExpression.java\"\npublic abstract class org.apache.hadoop.fs.shell.find.BaseExpression implements org.apache.hadoop.fs.shell.find.Expression,org.apache.hadoop.conf.Configurable {\n  public org.apache.hadoop.fs.shell.find.BaseExpression();\n  protected void setUsage(java.lang.String[]);\n  protected void setHelp(java.lang.String[]);\n  public java.lang.String[] getUsage();\n  public java.lang.String[] getHelp();\n  public void setOptions(org.apache.hadoop.fs.shell.find.FindOptions) throws java.io.IOException;\n  public void prepare() throws java.io.IOException;\n  public void finish() throws java.io.IOException;\n  protected org.apache.hadoop.fs.shell.find.FindOptions getOptions();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.lang.String toString();\n  public boolean isAction();\n  public boolean isOperator();\n  protected java.util.List<java.lang.String> getArguments();\n  protected java.lang.String getArgument(int) throws java.io.IOException;\n  protected java.util.List<org.apache.hadoop.fs.shell.find.Expression> getChildren();\n  public int getPrecedence();\n  public void addChildren(java.util.Deque<org.apache.hadoop.fs.shell.find.Expression>);\n  protected void addChildren(java.util.Deque<org.apache.hadoop.fs.shell.find.Expression>, int);\n  public void addArguments(java.util.Deque<java.lang.String>);\n  protected void addArguments(java.util.Deque<java.lang.String>, int);\n  protected void addArgument(java.lang.String);\n  protected org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path getPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/conf/Configuration$IntegerRanges.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/MetricValue.class": "Compiled from \"MetricValue.java\"\npublic class org.apache.hadoop.metrics.spi.MetricValue {\n  public static final boolean ABSOLUTE;\n  public static final boolean INCREMENT;\n  public org.apache.hadoop.metrics.spi.MetricValue(java.lang.Number, boolean);\n  public boolean isIncrement();\n  public boolean isAbsolute();\n  public java.lang.Number getNumber();\n}\n", 
  "org/apache/hadoop/crypto/key/kms/ValueQueue$UniqueKeyBlockingQueue.class": "Compiled from \"ValueQueue.java\"\npublic class org.apache.hadoop.crypto.key.kms.ValueQueue<E> {\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public void initializeQueuesForKeys(java.lang.String...) throws java.util.concurrent.ExecutionException;\n  public E getNext(java.lang.String) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void drain(java.lang.String);\n  public int getSize(java.lang.String) throws java.util.concurrent.ExecutionException;\n  public java.util.List<E> getAtMost(java.lang.String, int) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void shutdown();\n  static int access$200(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static float access$300(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller access$400(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Utils.class": "Compiled from \"Utils.java\"\npublic final class org.apache.hadoop.io.file.tfile.Utils {\n  public static void writeVInt(java.io.DataOutput, int) throws java.io.IOException;\n  public static void writeVLong(java.io.DataOutput, long) throws java.io.IOException;\n  public static int readVInt(java.io.DataInput) throws java.io.IOException;\n  public static long readVLong(java.io.DataInput) throws java.io.IOException;\n  public static void writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static <T extends java/lang/Object> int lowerBound(java.util.List<? extends T>, T, java.util.Comparator<? super T>);\n  public static <T extends java/lang/Object> int upperBound(java.util.List<? extends T>, T, java.util.Comparator<? super T>);\n  public static <T extends java/lang/Object> int lowerBound(java.util.List<? extends java.lang.Comparable<? super T>>, T);\n  public static <T extends java/lang/Object> int upperBound(java.util.List<? extends java.lang.Comparable<? super T>>, T);\n}\n", 
  "org/apache/hadoop/fs/MD5MD5CRC32FileChecksum$1.class": "Compiled from \"MD5MD5CRC32FileChecksum.java\"\npublic class org.apache.hadoop.fs.MD5MD5CRC32FileChecksum extends org.apache.hadoop.fs.FileChecksum {\n  public static final int LENGTH;\n  public org.apache.hadoop.fs.MD5MD5CRC32FileChecksum();\n  public org.apache.hadoop.fs.MD5MD5CRC32FileChecksum(int, long, org.apache.hadoop.io.MD5Hash);\n  public java.lang.String getAlgorithmName();\n  public static org.apache.hadoop.util.DataChecksum$Type getCrcTypeFromAlgorithmName(java.lang.String) throws java.io.IOException;\n  public int getLength();\n  public byte[] getBytes();\n  public org.apache.hadoop.util.DataChecksum$Type getCrcType();\n  public org.apache.hadoop.fs.Options$ChecksumOpt getChecksumOpt();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public static void write(org.znerd.xmlenc.XMLOutputter, org.apache.hadoop.fs.MD5MD5CRC32FileChecksum) throws java.io.IOException;\n  public static org.apache.hadoop.fs.MD5MD5CRC32FileChecksum valueOf(org.xml.sax.Attributes) throws org.xml.sax.SAXException;\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/BZip2Constants.class": "Compiled from \"BZip2Constants.java\"\npublic interface org.apache.hadoop.io.compress.bzip2.BZip2Constants {\n  public static final int baseBlockSize;\n  public static final int MAX_ALPHA_SIZE;\n  public static final int MAX_CODE_LEN;\n  public static final int RUNA;\n  public static final int RUNB;\n  public static final int N_GROUPS;\n  public static final int G_SIZE;\n  public static final int N_ITERS;\n  public static final int MAX_SELECTORS;\n  public static final int NUM_OVERSHOOT_BYTES;\n  public static final int END_OF_BLOCK;\n  public static final int END_OF_STREAM;\n  public static final int[] rNums;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSystemImpl$1.class": "Compiled from \"MetricsSystemImpl.java\"\npublic class org.apache.hadoop.metrics2.impl.MetricsSystemImpl extends org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSource {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String MS_NAME;\n  static final java.lang.String MS_STATS_NAME;\n  static final java.lang.String MS_STATS_DESC;\n  static final java.lang.String MS_CONTROL_NAME;\n  static final java.lang.String MS_INIT_MODE_KEY;\n  org.apache.hadoop.metrics2.lib.MutableStat snapshotStat;\n  org.apache.hadoop.metrics2.lib.MutableStat publishStat;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong droppedPubAll;\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl(java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl();\n  public synchronized org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public synchronized void unregisterSource(java.lang.String);\n  synchronized void registerSource(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource);\n  public synchronized <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  synchronized void registerSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink);\n  public synchronized void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public synchronized void startMetricsMBeans();\n  public synchronized void stopMetricsMBeans();\n  public synchronized java.lang.String currentConfig();\n  synchronized void onTimerEvent();\n  public synchronized void publishMetricsNow();\n  synchronized org.apache.hadoop.metrics2.impl.MetricsBuffer sampleMetrics();\n  synchronized void publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, boolean);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static java.lang.String getHostname();\n  public synchronized void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n  public synchronized boolean shutdown();\n  public org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter getSourceAdapter(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$CedeActiveResponseProtoOrBuilder.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$TraceAdminService$BlockingStub.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/JvmPauseMonitor$1.class": "Compiled from \"JvmPauseMonitor.java\"\npublic class org.apache.hadoop.util.JvmPauseMonitor {\n  public org.apache.hadoop.util.JvmPauseMonitor(org.apache.hadoop.conf.Configuration);\n  public void start();\n  public void stop();\n  public boolean isStarted();\n  public long getNumGcWarnThreadholdExceeded();\n  public long getNumGcInfoThresholdExceeded();\n  public long getTotalGcExtraSleepTime();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static java.util.Map access$400(org.apache.hadoop.util.JvmPauseMonitor);\n  static boolean access$500(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$600(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$704(org.apache.hadoop.util.JvmPauseMonitor);\n  static java.lang.String access$800(org.apache.hadoop.util.JvmPauseMonitor, long, java.util.Map, java.util.Map);\n  static org.apache.commons.logging.Log access$900();\n  static long access$1000(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$1104(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$1214(org.apache.hadoop.util.JvmPauseMonitor, long);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.class": "Compiled from \"BoundedRangeFileInputStream.java\"\nclass org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream extends java.io.InputStream {\n  public org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream(org.apache.hadoop.fs.FSDataInputStream, long, long);\n  public int available() throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[]) throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public long skip(long) throws java.io.IOException;\n  public synchronized void mark(int);\n  public synchronized void reset() throws java.io.IOException;\n  public boolean markSupported();\n  public void close();\n}\n", 
  "org/apache/hadoop/net/TableMapping$RawTableMapping.class": "Compiled from \"TableMapping.java\"\npublic class org.apache.hadoop.net.TableMapping extends org.apache.hadoop.net.CachedDNSToSwitchMapping {\n  public org.apache.hadoop.net.TableMapping();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void reloadCachedMappings();\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$AuthProtocol.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFileDumper$Align.class": "Compiled from \"TFileDumper.java\"\nclass org.apache.hadoop.io.file.tfile.TFileDumper {\n  static final org.apache.commons.logging.Log LOG;\n  public static void dumpInfo(java.lang.String, java.io.PrintStream, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/CompositeContext$MetricsRecordDelegator.class": "Compiled from \"CompositeContext.java\"\npublic class org.apache.hadoop.metrics.spi.CompositeContext extends org.apache.hadoop.metrics.spi.AbstractMetricsContext {\n  public org.apache.hadoop.metrics.spi.CompositeContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  public org.apache.hadoop.metrics.MetricsRecord newRecord(java.lang.String);\n  protected void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord) throws java.io.IOException;\n  protected void flush() throws java.io.IOException;\n  public void startMonitoring() throws java.io.IOException;\n  public void stopMonitoring();\n  public boolean isMonitoring();\n  public void close();\n  public void registerUpdater(org.apache.hadoop.metrics.Updater);\n  public void unregisterUpdater(org.apache.hadoop.metrics.Updater);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Display$TextRecordInputStream.class": "Compiled from \"Display.java\"\nclass org.apache.hadoop.fs.shell.Display extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.Display();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/security/Groups$GroupCacheLoader.class": "Compiled from \"Groups.java\"\npublic class org.apache.hadoop.security.Groups {\n  public org.apache.hadoop.security.Groups(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.security.Groups(org.apache.hadoop.conf.Configuration, org.apache.hadoop.util.Timer);\n  java.util.Set<java.lang.String> getNegativeCache();\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void refresh();\n  public void cacheGroupsAdd(java.util.List<java.lang.String>);\n  public static org.apache.hadoop.security.Groups getUserToGroupsMappingService();\n  public static synchronized org.apache.hadoop.security.Groups getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration);\n  public static synchronized org.apache.hadoop.security.Groups getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration);\n  static boolean access$100(org.apache.hadoop.security.Groups);\n  static java.util.Set access$200(org.apache.hadoop.security.Groups);\n  static java.io.IOException access$300(org.apache.hadoop.security.Groups, java.lang.String);\n  static org.apache.hadoop.util.Timer access$400(org.apache.hadoop.security.Groups);\n  static org.apache.hadoop.security.GroupMappingServiceProvider access$500(org.apache.hadoop.security.Groups);\n  static long access$600(org.apache.hadoop.security.Groups);\n  static org.apache.commons.logging.Log access$700();\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/Token$TrivialRenewer.class": "Compiled from \"Token.java\"\npublic class org.apache.hadoop.security.token.Token<T extends org.apache.hadoop.security.token.TokenIdentifier> implements org.apache.hadoop.io.Writable {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.token.Token(T, org.apache.hadoop.security.token.SecretManager<T>);\n  public org.apache.hadoop.security.token.Token(byte[], byte[], org.apache.hadoop.io.Text, org.apache.hadoop.io.Text);\n  public org.apache.hadoop.security.token.Token();\n  public org.apache.hadoop.security.token.Token(org.apache.hadoop.security.token.Token<T>);\n  public byte[] getIdentifier();\n  public T decodeIdentifier() throws java.io.IOException;\n  public byte[] getPassword();\n  public synchronized org.apache.hadoop.io.Text getKind();\n  public synchronized void setKind(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text getService();\n  public void setService(org.apache.hadoop.io.Text);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.lang.String encodeToUrlString() throws java.io.IOException;\n  public void decodeFromUrlString(java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public boolean isManaged() throws java.io.IOException;\n  public long renew(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException;\n  public void cancel(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.io.Text access$000(org.apache.hadoop.security.token.Token);\n  static {};\n}\n", 
  "org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.class": "Compiled from \"RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java\"\npublic class org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB implements org.apache.hadoop.ipc.ProtocolMetaInterface,org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol,java.io.Closeable {\n  public org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB(org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB);\n  public void close() throws java.io.IOException;\n  public void refreshServiceAcl() throws java.io.IOException;\n  public boolean isMethodSupported(java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Command.class": "Compiled from \"Command.java\"\npublic abstract class org.apache.hadoop.fs.shell.Command extends org.apache.hadoop.conf.Configured {\n  public static java.lang.String NAME;\n  public static java.lang.String USAGE;\n  public static java.lang.String DESCRIPTION;\n  protected java.lang.String[] args;\n  protected java.lang.String name;\n  protected int exitCode;\n  protected int numErrors;\n  protected boolean recursive;\n  protected java.util.ArrayList<java.lang.Exception> exceptions;\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  protected org.apache.hadoop.fs.shell.Command();\n  protected org.apache.hadoop.fs.shell.Command(org.apache.hadoop.conf.Configuration);\n  public abstract java.lang.String getCommandName();\n  protected void setRecursive(boolean);\n  protected boolean isRecursive();\n  protected int getDepth();\n  protected abstract void run(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public int runAll();\n  public void setCommandFactory(org.apache.hadoop.fs.shell.CommandFactory);\n  protected org.apache.hadoop.fs.shell.CommandFactory getCommandFactory();\n  public int run(java.lang.String...);\n  protected int exitCodeForError();\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processRawArguments(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected java.util.LinkedList<org.apache.hadoop.fs.shell.PathData> expandArguments(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected java.util.List<org.apache.hadoop.fs.shell.PathData> expandArgument(java.lang.String) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  protected void processArgument(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPathArgument(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processNonexistentPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPaths(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData...) throws java.io.IOException;\n  protected boolean isPathRecursable(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void postProcessPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void recursePath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  public void displayError(java.lang.Exception);\n  public void displayError(java.lang.String);\n  public void displayWarning(java.lang.String);\n  public java.lang.String getName();\n  public void setName(java.lang.String);\n  public java.lang.String getUsage();\n  public java.lang.String getDescription();\n  public final boolean isDeprecated();\n  public java.lang.String getReplacementCommand();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$IpcConnectionContextProtoOrBuilder.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/Node.class": "Compiled from \"Node.java\"\npublic interface org.apache.hadoop.net.Node {\n  public abstract java.lang.String getNetworkLocation();\n  public abstract void setNetworkLocation(java.lang.String);\n  public abstract java.lang.String getName();\n  public abstract org.apache.hadoop.net.Node getParent();\n  public abstract void setParent(org.apache.hadoop.net.Node);\n  public abstract int getLevel();\n  public abstract void setLevel(int);\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$5.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshAuthorizationPolicyProtocolService$Interface.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$23.class": "", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$UserInformationProtoOrBuilder.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/WritableRpcEngine$Invocation.class": "Compiled from \"WritableRpcEngine.java\"\npublic class org.apache.hadoop.ipc.WritableRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final long writableRpcVersion;\n  public org.apache.hadoop.ipc.WritableRpcEngine();\n  public static synchronized void ensureInitialized();\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$000();\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$ExponentialBackoffRetry.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/AclStatus.class": "Compiled from \"AclStatus.java\"\npublic class org.apache.hadoop.fs.permission.AclStatus {\n  public java.lang.String getOwner();\n  public java.lang.String getGroup();\n  public boolean isStickyBit();\n  public java.util.List<org.apache.hadoop.fs.permission.AclEntry> getEntries();\n  public org.apache.hadoop.fs.permission.FsPermission getPermission();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.permission.FsAction getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry);\n  public org.apache.hadoop.fs.permission.FsAction getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry, org.apache.hadoop.fs.permission.FsPermission) throws java.lang.IllegalArgumentException;\n  org.apache.hadoop.fs.permission.AclStatus(java.lang.String, java.lang.String, boolean, java.lang.Iterable, org.apache.hadoop.fs.permission.FsPermission, org.apache.hadoop.fs.permission.AclStatus$1);\n}\n", 
  "org/apache/hadoop/record/compiler/JLong$JavaLong.class": "Compiled from \"JLong.java\"\npublic class org.apache.hadoop.record.compiler.JLong extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JLong();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.class": "Compiled from \"DelegationTokenAuthenticationFilter.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter extends org.apache.hadoop.security.authentication.server.AuthenticationFilter {\n  public static final java.lang.String DELEGATION_TOKEN_SECRET_MANAGER_ATTR;\n  public static final java.lang.String PROXYUSER_PREFIX;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter();\n  protected java.util.Properties getConfiguration(java.lang.String, javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void setAuthHandlerClass(java.util.Properties) throws javax.servlet.ServletException;\n  protected org.apache.hadoop.conf.Configuration getProxyuserConfiguration(javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  public void init(javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void initializeAuthHandler(java.lang.String, javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void setHandlerAuthMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  static java.lang.String getDoAs(javax.servlet.http.HttpServletRequest);\n  static org.apache.hadoop.security.UserGroupInformation getHttpUserGroupInformationInContext();\n  protected void doFilter(javax.servlet.FilterChain, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n  static org.apache.hadoop.security.SaslRpcServer$AuthMethod access$000(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtocolMetaInfoPB.class": "Compiled from \"ProtocolMetaInfoPB.java\"\npublic interface org.apache.hadoop.ipc.ProtocolMetaInfoPB extends org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolInfoService$BlockingInterface {\n}\n", 
  "org/apache/hadoop/fs/local/LocalConfigKeys.class": "Compiled from \"LocalConfigKeys.java\"\npublic class org.apache.hadoop.fs.local.LocalConfigKeys extends org.apache.hadoop.fs.CommonConfigurationKeys {\n  public static final java.lang.String BLOCK_SIZE_KEY;\n  public static final long BLOCK_SIZE_DEFAULT;\n  public static final java.lang.String REPLICATION_KEY;\n  public static final short REPLICATION_DEFAULT;\n  public static final java.lang.String STREAM_BUFFER_SIZE_KEY;\n  public static final int STREAM_BUFFER_SIZE_DEFAULT;\n  public static final java.lang.String BYTES_PER_CHECKSUM_KEY;\n  public static final int BYTES_PER_CHECKSUM_DEFAULT;\n  public static final java.lang.String CLIENT_WRITE_PACKET_SIZE_KEY;\n  public static final int CLIENT_WRITE_PACKET_SIZE_DEFAULT;\n  public static final boolean ENCRYPT_DATA_TRANSFER_DEFAULT;\n  public static final long FS_TRASH_INTERVAL_DEFAULT;\n  public static final org.apache.hadoop.util.DataChecksum$Type CHECKSUM_TYPE_DEFAULT;\n  public org.apache.hadoop.fs.local.LocalConfigKeys();\n  public static org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileEncryptionInfo.class": "Compiled from \"FileEncryptionInfo.java\"\npublic class org.apache.hadoop.fs.FileEncryptionInfo {\n  public org.apache.hadoop.fs.FileEncryptionInfo(org.apache.hadoop.crypto.CipherSuite, org.apache.hadoop.crypto.CryptoProtocolVersion, byte[], byte[], java.lang.String, java.lang.String);\n  public org.apache.hadoop.crypto.CipherSuite getCipherSuite();\n  public org.apache.hadoop.crypto.CryptoProtocolVersion getCryptoProtocolVersion();\n  public byte[] getEncryptedDataEncryptionKey();\n  public byte[] getIV();\n  public java.lang.String getKeyName();\n  public java.lang.String getEzKeyVersionName();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/security/alias/CredentialShell$PasswordReader.class": "Compiled from \"CredentialShell.java\"\npublic class org.apache.hadoop.security.alias.CredentialShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.security.alias.CredentialShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int init(java.lang.String[]) throws java.io.IOException;\n  protected char[] promptForCredential() throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialShell$PasswordReader getPasswordReader();\n  public void setPasswordReader(org.apache.hadoop.security.alias.CredentialShell$PasswordReader);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.security.alias.CredentialShell);\n  static boolean access$300(org.apache.hadoop.security.alias.CredentialShell);\n  static java.lang.String access$400(org.apache.hadoop.security.alias.CredentialShell);\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation$AuthenticationMethod.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider$2.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/util/LightWeightCache$1.class": "Compiled from \"LightWeightCache.java\"\npublic class org.apache.hadoop.util.LightWeightCache<K, E extends K> extends org.apache.hadoop.util.LightWeightGSet<K, E> {\n  public org.apache.hadoop.util.LightWeightCache(int, int, long, long);\n  org.apache.hadoop.util.LightWeightCache(int, int, long, long, org.apache.hadoop.util.LightWeightCache$Clock);\n  void setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  boolean isExpired(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  public E get(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer2$QuotingInputFilter.class": "Compiled from \"HttpServer2.java\"\npublic final class org.apache.hadoop.http.HttpServer2 implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  public static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean);\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public java.net.InetSocketAddress getConnectorAddress(int);\n  public void setThreads(int, int);\n  public void start() throws java.io.IOException;\n  void openListeners() throws java.lang.Exception;\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  org.apache.hadoop.http.HttpServer2(org.apache.hadoop.http.HttpServer2$Builder, org.apache.hadoop.http.HttpServer2$1) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.http.HttpServer2, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.http.HttpServer2, org.mortbay.jetty.Connector);\n  static void access$300(org.apache.hadoop.http.HttpServer2);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$21.class": "", 
  "org/apache/hadoop/security/User.class": "Compiled from \"User.java\"\nclass org.apache.hadoop.security.User implements java.security.Principal {\n  public org.apache.hadoop.security.User(java.lang.String);\n  public org.apache.hadoop.security.User(java.lang.String, org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod, javax.security.auth.login.LoginContext);\n  public java.lang.String getName();\n  public java.lang.String getShortName();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public javax.security.auth.login.LoginContext getLogin();\n  public void setLogin(javax.security.auth.login.LoginContext);\n  public void setLastLogin(long);\n  public long getLastLogin();\n}\n", 
  "org/apache/hadoop/security/AccessControlException.class": "Compiled from \"AccessControlException.java\"\npublic class org.apache.hadoop.security.AccessControlException extends org.apache.hadoop.fs.permission.AccessControlException {\n  public org.apache.hadoop.security.AccessControlException();\n  public org.apache.hadoop.security.AccessControlException(java.lang.String);\n  public org.apache.hadoop.security.AccessControlException(java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/fs/FileContext$13.class": "", 
  "org/apache/hadoop/security/proto/SecurityProtos$RenewDelegationTokenResponseProto$1.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolPB.class": "Compiled from \"RefreshUserMappingsProtocolPB.java\"\npublic interface org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB extends org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserMappingsProtocolService$BlockingInterface {\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.class": "Compiled from \"DelegationTokenAuthenticator.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator implements org.apache.hadoop.security.authentication.client.Authenticator {\n  public static final java.lang.String OP_PARAM;\n  public static final java.lang.String DELEGATION_TOKEN_HEADER;\n  public static final java.lang.String DELEGATION_PARAM;\n  public static final java.lang.String TOKEN_PARAM;\n  public static final java.lang.String RENEWER_PARAM;\n  public static final java.lang.String DELEGATION_TOKEN_JSON;\n  public static final java.lang.String DELEGATION_TOKEN_URL_STRING_JSON;\n  public static final java.lang.String RENEW_DELEGATION_TOKEN_JSON;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator(org.apache.hadoop.security.authentication.client.Authenticator);\n  public void setConnectionConfigurator(org.apache.hadoop.security.authentication.client.ConnectionConfigurator);\n  public void authenticate(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> getDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> getDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, java.lang.String, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public long renewDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public long renewDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public void cancelDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>) throws java.io.IOException;\n  public void cancelDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/generated/SimpleCharStream.class": "Compiled from \"SimpleCharStream.java\"\npublic class org.apache.hadoop.record.compiler.generated.SimpleCharStream {\n  public static final boolean staticFlag;\n  int bufsize;\n  int available;\n  int tokenBegin;\n  public int bufpos;\n  protected int[] bufline;\n  protected int[] bufcolumn;\n  protected int column;\n  protected int line;\n  protected boolean prevCharIsCR;\n  protected boolean prevCharIsLF;\n  protected java.io.Reader inputStream;\n  protected char[] buffer;\n  protected int maxNextCharInd;\n  protected int inBuf;\n  protected int tabSize;\n  protected void setTabSize(int);\n  protected int getTabSize(int);\n  protected void ExpandBuff(boolean);\n  protected void FillBuff() throws java.io.IOException;\n  public char BeginToken() throws java.io.IOException;\n  protected void UpdateLineColumn(char);\n  public char readChar() throws java.io.IOException;\n  public int getEndColumn();\n  public int getEndLine();\n  public int getBeginColumn();\n  public int getBeginLine();\n  public void backup(int);\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.Reader, int, int, int);\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.Reader, int, int);\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.Reader);\n  public void ReInit(java.io.Reader, int, int, int);\n  public void ReInit(java.io.Reader, int, int);\n  public void ReInit(java.io.Reader);\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.InputStream, java.lang.String, int, int, int) throws java.io.UnsupportedEncodingException;\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.InputStream, int, int, int);\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.InputStream, java.lang.String, int, int) throws java.io.UnsupportedEncodingException;\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.InputStream, int, int);\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.InputStream, java.lang.String) throws java.io.UnsupportedEncodingException;\n  public org.apache.hadoop.record.compiler.generated.SimpleCharStream(java.io.InputStream);\n  public void ReInit(java.io.InputStream, java.lang.String, int, int, int) throws java.io.UnsupportedEncodingException;\n  public void ReInit(java.io.InputStream, int, int, int);\n  public void ReInit(java.io.InputStream, java.lang.String) throws java.io.UnsupportedEncodingException;\n  public void ReInit(java.io.InputStream);\n  public void ReInit(java.io.InputStream, java.lang.String, int, int) throws java.io.UnsupportedEncodingException;\n  public void ReInit(java.io.InputStream, int, int);\n  public java.lang.String GetImage();\n  public char[] GetSuffix(int);\n  public void Done();\n  public void adjustBeginLineColumn(int, int);\n}\n", 
  "org/apache/hadoop/io/Text$1.class": "Compiled from \"Text.java\"\npublic class org.apache.hadoop.io.Text extends org.apache.hadoop.io.BinaryComparable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.BinaryComparable> {\n  public static final int DEFAULT_MAX_LEN;\n  static final int[] bytesFromUTF8;\n  static final int[] offsetsFromUTF8;\n  public org.apache.hadoop.io.Text();\n  public org.apache.hadoop.io.Text(java.lang.String);\n  public org.apache.hadoop.io.Text(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text(byte[]);\n  public byte[] copyBytes();\n  public byte[] getBytes();\n  public int getLength();\n  public int charAt(int);\n  public int find(java.lang.String);\n  public int find(java.lang.String, int);\n  public void set(java.lang.String);\n  public void set(byte[]);\n  public void set(org.apache.hadoop.io.Text);\n  public void set(byte[], int, int);\n  public void append(byte[], int, int);\n  public void clear();\n  public java.lang.String toString();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void readFields(java.io.DataInput, int) throws java.io.IOException;\n  public static void skip(java.io.DataInput) throws java.io.IOException;\n  public void readWithKnownLength(java.io.DataInput, int) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void write(java.io.DataOutput, int) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static java.lang.String decode(byte[]) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String decode(byte[], int, int) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String decode(byte[], int, int, boolean) throws java.nio.charset.CharacterCodingException;\n  public static java.nio.ByteBuffer encode(java.lang.String) throws java.nio.charset.CharacterCodingException;\n  public static java.nio.ByteBuffer encode(java.lang.String, boolean) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput, int) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String, int) throws java.io.IOException;\n  public static void validateUTF8(byte[]) throws java.nio.charset.MalformedInputException;\n  public static void validateUTF8(byte[], int, int) throws java.nio.charset.MalformedInputException;\n  public static int bytesToCodePoint(java.nio.ByteBuffer);\n  public static int utf8Length(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/PathFilter.class": "Compiled from \"PathFilter.java\"\npublic interface org.apache.hadoop.fs.PathFilter {\n  public abstract boolean accept(org.apache.hadoop.fs.Path);\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$RenewDelegationTokenRequestProto.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/LocatedFileStatus.class": "Compiled from \"LocatedFileStatus.java\"\npublic class org.apache.hadoop.fs.LocatedFileStatus extends org.apache.hadoop.fs.FileStatus {\n  public org.apache.hadoop.fs.LocatedFileStatus(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.BlockLocation[]) throws java.io.IOException;\n  public org.apache.hadoop.fs.LocatedFileStatus(long, boolean, int, long, long, long, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.BlockLocation[]);\n  public org.apache.hadoop.fs.BlockLocation[] getBlockLocations();\n  public int compareTo(java.lang.Object);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation$HadoopConfiguration.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/PathAccessDeniedException.class": "Compiled from \"PathAccessDeniedException.java\"\npublic class org.apache.hadoop.fs.PathAccessDeniedException extends org.apache.hadoop.fs.PathIOException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.PathAccessDeniedException(java.lang.String);\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableGauge.class": "Compiled from \"MutableGauge.java\"\npublic abstract class org.apache.hadoop.metrics2.lib.MutableGauge extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  protected org.apache.hadoop.metrics2.lib.MutableGauge(org.apache.hadoop.metrics2.MetricsInfo);\n  protected org.apache.hadoop.metrics2.MetricsInfo info();\n  public abstract void incr();\n  public abstract void decr();\n}\n", 
  "org/apache/hadoop/io/file/tfile/ByteArray.class": "Compiled from \"ByteArray.java\"\npublic final class org.apache.hadoop.io.file.tfile.ByteArray implements org.apache.hadoop.io.file.tfile.RawComparable {\n  public org.apache.hadoop.io.file.tfile.ByteArray(org.apache.hadoop.io.BytesWritable);\n  public org.apache.hadoop.io.file.tfile.ByteArray(byte[]);\n  public org.apache.hadoop.io.file.tfile.ByteArray(byte[], int, int);\n  public byte[] buffer();\n  public int offset();\n  public int size();\n}\n", 
  "org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.class": "Compiled from \"WeightedRoundRobinMultiplexer.java\"\npublic class org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer implements org.apache.hadoop.ipc.RpcMultiplexer {\n  public static final java.lang.String IPC_CALLQUEUE_WRRMUX_WEIGHTS_KEY;\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer(int, java.lang.String, org.apache.hadoop.conf.Configuration);\n  public int getAndAdvanceCurrentIndex();\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$RecordCompressWriter.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/filter/RegexFilter.class": "Compiled from \"RegexFilter.java\"\npublic class org.apache.hadoop.metrics2.filter.RegexFilter extends org.apache.hadoop.metrics2.filter.AbstractPatternFilter {\n  public org.apache.hadoop.metrics2.filter.RegexFilter();\n  protected java.util.regex.Pattern compile(java.lang.String);\n}\n", 
  "org/apache/hadoop/security/alias/CredentialShell$Command.class": "Compiled from \"CredentialShell.java\"\npublic class org.apache.hadoop.security.alias.CredentialShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.security.alias.CredentialShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int init(java.lang.String[]) throws java.io.IOException;\n  protected char[] promptForCredential() throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialShell$PasswordReader getPasswordReader();\n  public void setPasswordReader(org.apache.hadoop.security.alias.CredentialShell$PasswordReader);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.security.alias.CredentialShell);\n  static boolean access$300(org.apache.hadoop.security.alias.CredentialShell);\n  static java.lang.String access$400(org.apache.hadoop.security.alias.CredentialShell);\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Cache.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtocolProxy.class": "Compiled from \"ProtocolProxy.java\"\npublic class org.apache.hadoop.ipc.ProtocolProxy<T> {\n  public org.apache.hadoop.ipc.ProtocolProxy(java.lang.Class<T>, T, boolean);\n  public T getProxy();\n  public synchronized boolean isMethodSupported(java.lang.String, java.lang.Class<?>...) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/WritableFactories.class": "Compiled from \"WritableFactories.java\"\npublic class org.apache.hadoop.io.WritableFactories {\n  public static void setFactory(java.lang.Class, org.apache.hadoop.io.WritableFactory);\n  public static org.apache.hadoop.io.WritableFactory getFactory(java.lang.Class);\n  public static org.apache.hadoop.io.Writable newInstance(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.io.Writable newInstance(java.lang.Class<? extends org.apache.hadoop.io.Writable>);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/AbstractMetricsContext$TagMap.class": "Compiled from \"AbstractMetricsContext.java\"\npublic abstract class org.apache.hadoop.metrics.spi.AbstractMetricsContext implements org.apache.hadoop.metrics.MetricsContext {\n  protected org.apache.hadoop.metrics.spi.AbstractMetricsContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  protected java.lang.String getAttribute(java.lang.String);\n  protected java.util.Map<java.lang.String, java.lang.String> getAttributeTable(java.lang.String);\n  public java.lang.String getContextName();\n  public org.apache.hadoop.metrics.ContextFactory getContextFactory();\n  public synchronized void startMonitoring() throws java.io.IOException;\n  public synchronized void stopMonitoring();\n  public boolean isMonitoring();\n  public synchronized void close();\n  public final synchronized org.apache.hadoop.metrics.MetricsRecord createRecord(java.lang.String);\n  protected org.apache.hadoop.metrics.MetricsRecord newRecord(java.lang.String);\n  public synchronized void registerUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized void unregisterUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized java.util.Map<java.lang.String, java.util.Collection<org.apache.hadoop.metrics.spi.OutputRecord>> getAllRecords();\n  protected abstract void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord) throws java.io.IOException;\n  protected void flush() throws java.io.IOException;\n  protected void update(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  protected void remove(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  public int getPeriod();\n  protected void setPeriod(int);\n  protected void parseAndSetPeriod(java.lang.String);\n  static void access$000(org.apache.hadoop.metrics.spi.AbstractMetricsContext) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/ftp/FtpConfigKeys.class": "Compiled from \"FtpConfigKeys.java\"\npublic class org.apache.hadoop.fs.ftp.FtpConfigKeys extends org.apache.hadoop.fs.CommonConfigurationKeys {\n  public static final java.lang.String BLOCK_SIZE_KEY;\n  public static final long BLOCK_SIZE_DEFAULT;\n  public static final java.lang.String REPLICATION_KEY;\n  public static final short REPLICATION_DEFAULT;\n  public static final java.lang.String STREAM_BUFFER_SIZE_KEY;\n  public static final int STREAM_BUFFER_SIZE_DEFAULT;\n  public static final java.lang.String BYTES_PER_CHECKSUM_KEY;\n  public static final int BYTES_PER_CHECKSUM_DEFAULT;\n  public static final java.lang.String CLIENT_WRITE_PACKET_SIZE_KEY;\n  public static final int CLIENT_WRITE_PACKET_SIZE_DEFAULT;\n  public static final boolean ENCRYPT_DATA_TRANSFER_DEFAULT;\n  public static final long FS_TRASH_INTERVAL_DEFAULT;\n  public static final org.apache.hadoop.util.DataChecksum$Type CHECKSUM_TYPE_DEFAULT;\n  public org.apache.hadoop.fs.ftp.FtpConfigKeys();\n  protected static org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/authorize/RefreshAuthorizationPolicyProtocol.class": "Compiled from \"RefreshAuthorizationPolicyProtocol.java\"\npublic interface org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol {\n  public static final long versionID;\n  public abstract void refreshServiceAcl() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueProtocolService$Stub.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/CanSetDropBehind.class": "Compiled from \"CanSetDropBehind.java\"\npublic interface org.apache.hadoop.fs.CanSetDropBehind {\n  public abstract void setDropBehind(java.lang.Boolean) throws java.io.IOException, java.lang.UnsupportedOperationException;\n}\n", 
  "org/apache/hadoop/io/retry/Idempotent.class": "Compiled from \"Idempotent.java\"\npublic interface org.apache.hadoop.io.retry.Idempotent extends java.lang.annotation.Annotation {\n}\n", 
  "org/apache/hadoop/security/alias/CredentialProviderFactory.class": "Compiled from \"CredentialProviderFactory.java\"\npublic abstract class org.apache.hadoop.security.alias.CredentialProviderFactory {\n  public static final java.lang.String CREDENTIAL_PROVIDER_PATH;\n  public org.apache.hadoop.security.alias.CredentialProviderFactory();\n  public abstract org.apache.hadoop.security.alias.CredentialProvider createProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.security.alias.CredentialProvider> getProviders(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$GetServiceStatusRequestProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FilterFs.class": "Compiled from \"FilterFs.java\"\npublic abstract class org.apache.hadoop.fs.FilterFs extends org.apache.hadoop.fs.AbstractFileSystem {\n  protected org.apache.hadoop.fs.AbstractFileSystem getMyFs();\n  protected org.apache.hadoop.fs.FilterFs(org.apache.hadoop.fs.AbstractFileSystem) throws java.net.URISyntaxException;\n  public org.apache.hadoop.fs.FileSystem$Statistics getStatistics();\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public int getUriDefaultPort();\n  public java.net.URI getUri();\n  public void checkPath(org.apache.hadoop.fs.Path);\n  public java.lang.String getUriPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void setVerifyChecksum(boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public boolean isValidName(java.lang.String);\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/shell/Stat.class": "Compiled from \"Stat.java\"\nclass org.apache.hadoop.fs.shell.Stat extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  protected final java.text.SimpleDateFormat timeFmt;\n  protected java.lang.String format;\n  org.apache.hadoop.fs.shell.Stat();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/metrics/RpcMetrics.class": "Compiled from \"RpcMetrics.java\"\npublic class org.apache.hadoop.ipc.metrics.RpcMetrics {\n  static final org.apache.commons.logging.Log LOG;\n  final org.apache.hadoop.ipc.Server server;\n  final org.apache.hadoop.metrics2.lib.MetricsRegistry registry;\n  final java.lang.String name;\n  final boolean rpcQuantileEnable;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong receivedBytes;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong sentBytes;\n  org.apache.hadoop.metrics2.lib.MutableRate rpcQueueTime;\n  org.apache.hadoop.metrics2.lib.MutableQuantiles[] rpcQueueTimeMillisQuantiles;\n  org.apache.hadoop.metrics2.lib.MutableRate rpcProcessingTime;\n  org.apache.hadoop.metrics2.lib.MutableQuantiles[] rpcProcessingTimeMillisQuantiles;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong rpcAuthenticationFailures;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong rpcAuthenticationSuccesses;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong rpcAuthorizationFailures;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong rpcAuthorizationSuccesses;\n  org.apache.hadoop.ipc.metrics.RpcMetrics(org.apache.hadoop.ipc.Server, org.apache.hadoop.conf.Configuration);\n  public java.lang.String name();\n  public static org.apache.hadoop.ipc.metrics.RpcMetrics create(org.apache.hadoop.ipc.Server, org.apache.hadoop.conf.Configuration);\n  public int numOpenConnections();\n  public int callQueueLength();\n  public void incrAuthenticationFailures();\n  public void incrAuthenticationSuccesses();\n  public void incrAuthorizationSuccesses();\n  public void incrAuthorizationFailures();\n  public void shutdown();\n  public void incrSentBytes(int);\n  public void incrReceivedBytes(int);\n  public void addRpcQueueTime(int);\n  public void addRpcProcessingTime(int);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslRpcServer$QualityOfProtection.class": "Compiled from \"SaslRpcServer.java\"\npublic class org.apache.hadoop.security.SaslRpcServer {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String SASL_DEFAULT_REALM;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod authMethod;\n  public java.lang.String mechanism;\n  public java.lang.String protocol;\n  public java.lang.String serverId;\n  public org.apache.hadoop.security.SaslRpcServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod) throws java.io.IOException;\n  public javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection, java.util.Map<java.lang.String, ?>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void init(org.apache.hadoop.conf.Configuration);\n  static java.lang.String encodeIdentifier(byte[]);\n  static byte[] decodeIdentifier(java.lang.String);\n  public static <T extends org/apache/hadoop/security/token/TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  static char[] encodePassword(byte[]);\n  public static java.lang.String[] splitKerberosName(java.lang.String);\n  static javax.security.sasl.SaslServerFactory access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/local/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.fs.local.package-info {\n}\n", 
  "org/apache/hadoop/fs/PathNotFoundException.class": "Compiled from \"PathNotFoundException.java\"\npublic class org.apache.hadoop.fs.PathNotFoundException extends org.apache.hadoop.fs.PathIOException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.PathNotFoundException(java.lang.String);\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetUserMappingsProtocolService$1.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$TokenProto$1.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation$DynamicConfiguration.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/http/AdminAuthorizedServlet.class": "Compiled from \"AdminAuthorizedServlet.java\"\npublic class org.apache.hadoop.http.AdminAuthorizedServlet extends org.mortbay.jetty.servlet.DefaultServlet {\n  public org.apache.hadoop.http.AdminAuthorizedServlet();\n  protected void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator$DelegationTokenOperation.class": "Compiled from \"DelegationTokenAuthenticator.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator implements org.apache.hadoop.security.authentication.client.Authenticator {\n  public static final java.lang.String OP_PARAM;\n  public static final java.lang.String DELEGATION_TOKEN_HEADER;\n  public static final java.lang.String DELEGATION_PARAM;\n  public static final java.lang.String TOKEN_PARAM;\n  public static final java.lang.String RENEWER_PARAM;\n  public static final java.lang.String DELEGATION_TOKEN_JSON;\n  public static final java.lang.String DELEGATION_TOKEN_URL_STRING_JSON;\n  public static final java.lang.String RENEW_DELEGATION_TOKEN_JSON;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator(org.apache.hadoop.security.authentication.client.Authenticator);\n  public void setConnectionConfigurator(org.apache.hadoop.security.authentication.client.ConnectionConfigurator);\n  public void authenticate(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> getDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> getDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, java.lang.String, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public long renewDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public long renewDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public void cancelDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>) throws java.io.IOException;\n  public void cancelDelegationToken(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token, org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/authorize/ImpersonationProvider.class": "Compiled from \"ImpersonationProvider.java\"\npublic interface org.apache.hadoop.security.authorize.ImpersonationProvider extends org.apache.hadoop.conf.Configurable {\n  public abstract void init(java.lang.String);\n  public abstract void authorize(org.apache.hadoop.security.UserGroupInformation, java.lang.String) throws org.apache.hadoop.security.authorize.AuthorizationException;\n}\n", 
  "org/apache/hadoop/record/BinaryRecordInput$BinaryIndex.class": "Compiled from \"BinaryRecordInput.java\"\npublic class org.apache.hadoop.record.BinaryRecordInput implements org.apache.hadoop.record.RecordInput {\n  public static org.apache.hadoop.record.BinaryRecordInput get(java.io.DataInput);\n  public org.apache.hadoop.record.BinaryRecordInput(java.io.InputStream);\n  public org.apache.hadoop.record.BinaryRecordInput(java.io.DataInput);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.record.BinaryRecordInput(org.apache.hadoop.record.BinaryRecordInput$1);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$ActiveStandbyElectorCallback.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MethodMetric$3.class": "Compiled from \"MethodMetric.java\"\nclass org.apache.hadoop.metrics2.lib.MethodMetric extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  org.apache.hadoop.metrics2.lib.MethodMetric(java.lang.Object, java.lang.reflect.Method, org.apache.hadoop.metrics2.MetricsInfo, org.apache.hadoop.metrics2.annotation.Metric$Type);\n  org.apache.hadoop.metrics2.lib.MutableMetric newCounter(java.lang.Class<?>);\n  static boolean isInt(java.lang.Class<?>);\n  static boolean isLong(java.lang.Class<?>);\n  static boolean isFloat(java.lang.Class<?>);\n  static boolean isDouble(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newGauge(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newTag(java.lang.Class<?>);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  static org.apache.hadoop.metrics2.MetricsInfo metricInfo(java.lang.reflect.Method);\n  static java.lang.String nameFrom(java.lang.reflect.Method);\n  static java.lang.Object access$000(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static java.lang.reflect.Method access$100(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.hadoop.metrics2.MetricsInfo access$200(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.commons.logging.Log access$300();\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpConfig.class": "Compiled from \"HttpConfig.java\"\npublic class org.apache.hadoop.http.HttpConfig {\n  public org.apache.hadoop.http.HttpConfig();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.class": "Compiled from \"MetricsSourceAdapter.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsSourceAdapter implements javax.management.DynamicMBean {\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter(java.lang.String, java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource, java.lang.Iterable<org.apache.hadoop.metrics2.MetricsTag>, org.apache.hadoop.metrics2.MetricsFilter, org.apache.hadoop.metrics2.MetricsFilter, int, boolean);\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter(java.lang.String, java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource, java.lang.Iterable<org.apache.hadoop.metrics2.MetricsTag>, int, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  void start();\n  public java.lang.Object getAttribute(java.lang.String) throws javax.management.AttributeNotFoundException, javax.management.MBeanException, javax.management.ReflectionException;\n  public void setAttribute(javax.management.Attribute) throws javax.management.AttributeNotFoundException, javax.management.InvalidAttributeValueException, javax.management.MBeanException, javax.management.ReflectionException;\n  public javax.management.AttributeList getAttributes(java.lang.String[]);\n  public javax.management.AttributeList setAttributes(javax.management.AttributeList);\n  public java.lang.Object invoke(java.lang.String, java.lang.Object[], java.lang.String[]) throws javax.management.MBeanException, javax.management.ReflectionException;\n  public javax.management.MBeanInfo getMBeanInfo();\n  java.lang.Iterable<org.apache.hadoop.metrics2.impl.MetricsRecordImpl> getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl, boolean);\n  synchronized void stop();\n  synchronized void startMBeans();\n  synchronized void stopMBeans();\n  javax.management.ObjectName getMBeanName();\n  java.lang.String name();\n  org.apache.hadoop.metrics2.MetricsSource source();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAStateChangeRequestInfoProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$RemoveSpanReceiverResponseProtoOrBuilder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/ReadaheadPool.class": "Compiled from \"ReadaheadPool.java\"\npublic class org.apache.hadoop.io.ReadaheadPool {\n  static final org.apache.commons.logging.Log LOG;\n  public static org.apache.hadoop.io.ReadaheadPool getInstance();\n  public org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest readaheadStream(java.lang.String, java.io.FileDescriptor, long, long, long, org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest);\n  public org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest submitReadahead(java.lang.String, java.io.FileDescriptor, long, long);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$ReplicationOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$14.class": "", 
  "org/apache/hadoop/metrics2/util/Contracts.class": "Compiled from \"Contracts.java\"\npublic class org.apache.hadoop.metrics2.util.Contracts {\n  public static <T extends java/lang/Object> T checkArg(T, boolean, java.lang.Object);\n  public static int checkArg(int, boolean, java.lang.Object);\n  public static long checkArg(long, boolean, java.lang.Object);\n  public static float checkArg(float, boolean, java.lang.Object);\n  public static double checkArg(double, boolean, java.lang.Object);\n}\n", 
  "org/apache/hadoop/fs/shell/CommandFactory.class": "Compiled from \"CommandFactory.java\"\npublic class org.apache.hadoop.fs.shell.CommandFactory extends org.apache.hadoop.conf.Configured {\n  public org.apache.hadoop.fs.shell.CommandFactory();\n  public org.apache.hadoop.fs.shell.CommandFactory(org.apache.hadoop.conf.Configuration);\n  public void registerCommands(java.lang.Class<?>);\n  public void addClass(java.lang.Class<? extends org.apache.hadoop.fs.shell.Command>, java.lang.String...);\n  public void addObject(org.apache.hadoop.fs.shell.Command, java.lang.String...);\n  public org.apache.hadoop.fs.shell.Command getInstance(java.lang.String);\n  public org.apache.hadoop.fs.shell.Command getInstance(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public java.lang.String[] getNames();\n}\n", 
  "org/apache/hadoop/util/RunJar$1.class": "Compiled from \"RunJar.java\"\npublic class org.apache.hadoop.util.RunJar {\n  public static final java.util.regex.Pattern MATCH_ANY;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  public static final java.lang.String HADOOP_USE_CLIENT_CLASSLOADER;\n  public static final java.lang.String HADOOP_CLASSPATH;\n  public static final java.lang.String HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES;\n  public org.apache.hadoop.util.RunJar();\n  public static void unJar(java.io.File, java.io.File) throws java.io.IOException;\n  public static void unJar(java.io.File, java.io.File, java.util.regex.Pattern) throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Throwable;\n  public void run(java.lang.String[]) throws java.lang.Throwable;\n  boolean useClientClassLoader();\n  java.lang.String getHadoopClasspath();\n  java.lang.String getSystemClasses();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RefreshRegistry$RegistryHolder.class": "Compiled from \"RefreshRegistry.java\"\npublic class org.apache.hadoop.ipc.RefreshRegistry {\n  public static final org.apache.commons.logging.Log LOG;\n  public static org.apache.hadoop.ipc.RefreshRegistry defaultRegistry();\n  public org.apache.hadoop.ipc.RefreshRegistry();\n  public synchronized void register(java.lang.String, org.apache.hadoop.ipc.RefreshHandler);\n  public synchronized boolean unregister(java.lang.String, org.apache.hadoop.ipc.RefreshHandler);\n  public synchronized void unregisterAll(java.lang.String);\n  public synchronized java.util.Collection<org.apache.hadoop.ipc.RefreshResponse> dispatch(java.lang.String, java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/NoEmitMetricsContext.class": "Compiled from \"NoEmitMetricsContext.java\"\npublic class org.apache.hadoop.metrics.spi.NoEmitMetricsContext extends org.apache.hadoop.metrics.spi.AbstractMetricsContext {\n  public org.apache.hadoop.metrics.spi.NoEmitMetricsContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  protected void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord);\n}\n", 
  "org/apache/hadoop/security/authorize/Service.class": "Compiled from \"Service.java\"\npublic class org.apache.hadoop.security.authorize.Service {\n  public org.apache.hadoop.security.authorize.Service(java.lang.String, java.lang.Class<?>);\n  public java.lang.String getServiceKey();\n  public java.lang.Class<?> getProtocol();\n}\n", 
  "org/apache/hadoop/ipc/RetriableException.class": "Compiled from \"RetriableException.java\"\npublic class org.apache.hadoop.ipc.RetriableException extends java.io.IOException {\n  public org.apache.hadoop.ipc.RetriableException(java.lang.Exception);\n  public org.apache.hadoop.ipc.RetriableException(java.lang.String);\n}\n", 
  "org/apache/hadoop/conf/ReconfigurableBase$ReconfigurationThread.class": "Compiled from \"ReconfigurableBase.java\"\npublic abstract class org.apache.hadoop.conf.ReconfigurableBase extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.conf.Reconfigurable {\n  public org.apache.hadoop.conf.ReconfigurableBase();\n  public org.apache.hadoop.conf.ReconfigurableBase(org.apache.hadoop.conf.Configuration);\n  public void setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil);\n  public java.util.Collection<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange> getChangedProperties(org.apache.hadoop.conf.Configuration, org.apache.hadoop.conf.Configuration);\n  public void startReconfigurationTask() throws java.io.IOException;\n  public org.apache.hadoop.conf.ReconfigurationTaskStatus getReconfigurationTaskStatus();\n  public void shutdownReconfigurationTask();\n  public final java.lang.String reconfigureProperty(java.lang.String, java.lang.String) throws org.apache.hadoop.conf.ReconfigurationException;\n  public abstract java.util.Collection<java.lang.String> getReconfigurableProperties();\n  public boolean isPropertyReconfigurable(java.lang.String);\n  protected abstract void reconfigurePropertyImpl(java.lang.String, java.lang.String) throws org.apache.hadoop.conf.ReconfigurationException;\n  static org.apache.commons.logging.Log access$000();\n  static java.lang.Object access$100(org.apache.hadoop.conf.ReconfigurableBase);\n  static long access$202(org.apache.hadoop.conf.ReconfigurableBase, long);\n  static java.util.Map access$302(org.apache.hadoop.conf.ReconfigurableBase, java.util.Map);\n  static java.lang.Thread access$402(org.apache.hadoop.conf.ReconfigurableBase, java.lang.Thread);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/zlib/ZlibFactory.class": "Compiled from \"ZlibFactory.java\"\npublic class org.apache.hadoop.io.compress.zlib.ZlibFactory {\n  public org.apache.hadoop.io.compress.zlib.ZlibFactory();\n  public static boolean isNativeZlibLoaded(org.apache.hadoop.conf.Configuration);\n  public static java.lang.String getLibraryName();\n  public static java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getZlibCompressorType(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.io.compress.Compressor getZlibCompressor(org.apache.hadoop.conf.Configuration);\n  public static java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getZlibDecompressorType(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.io.compress.Decompressor getZlibDecompressor(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.io.compress.DirectDecompressor getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration);\n  public static void setCompressionStrategy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy);\n  public static org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy getCompressionStrategy(org.apache.hadoop.conf.Configuration);\n  public static void setCompressionLevel(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel);\n  public static org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel getCompressionLevel(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Cache$ClientFinalizer.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HealthCheckFailedException.class": "Compiled from \"HealthCheckFailedException.java\"\npublic class org.apache.hadoop.ha.HealthCheckFailedException extends java.io.IOException {\n  public org.apache.hadoop.ha.HealthCheckFailedException(java.lang.String);\n  public org.apache.hadoop.ha.HealthCheckFailedException(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/ipc/Server$WrappedRpcServerException.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$10.class": "", 
  "org/apache/hadoop/crypto/key/KeyProviderCryptoExtension$1.class": "Compiled from \"KeyProviderCryptoExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderCryptoExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension> {\n  public static final java.lang.String EEK;\n  public static final java.lang.String EK;\n  protected org.apache.hadoop.crypto.key.KeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider, org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension);\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public static org.apache.hadoop.crypto.key.KeyProviderCryptoExtension createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$ZKAction.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.class": "Compiled from \"OpensslAesCtrCryptoCodec.java\"\npublic class org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec extends org.apache.hadoop.crypto.AesCtrCryptoCodec {\n  public org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  protected void finalize() throws java.lang.Throwable;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public org.apache.hadoop.crypto.Encryptor createEncryptor() throws java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.Decryptor createDecryptor() throws java.security.GeneralSecurityException;\n  public void generateSecureRandom(byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.class": "Compiled from \"AbstractJavaKeyStoreProvider.java\"\npublic abstract class org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider extends org.apache.hadoop.security.alias.CredentialProvider {\n  public static final java.lang.String CREDENTIAL_PASSWORD_NAME;\n  public static final java.lang.String KEYSTORE_PASSWORD_FILE_KEY;\n  public static final java.lang.String KEYSTORE_PASSWORD_DEFAULT;\n  protected org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getPath();\n  public void setPath(org.apache.hadoop.fs.Path);\n  public char[] getPassword();\n  public void setPassword(char[]);\n  public boolean isChanged();\n  public void setChanged(boolean);\n  public java.util.concurrent.locks.Lock getReadLock();\n  public void setReadLock(java.util.concurrent.locks.Lock);\n  public java.util.concurrent.locks.Lock getWriteLock();\n  public void setWriteLock(java.util.concurrent.locks.Lock);\n  public java.net.URI getUri();\n  public java.security.KeyStore getKeyStore();\n  public java.util.Map<java.lang.String, org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry> getCache();\n  protected final java.lang.String getPathAsString();\n  protected abstract java.lang.String getSchemeName();\n  protected abstract java.io.OutputStream getOutputStreamForKeystore() throws java.io.IOException;\n  protected abstract boolean keystoreExists() throws java.io.IOException;\n  protected abstract java.io.InputStream getInputStreamForFile() throws java.io.IOException;\n  protected abstract void createPermissions(java.lang.String) throws java.io.IOException;\n  protected abstract void stashOriginalFilePermissions() throws java.io.IOException;\n  protected void initFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry getCredentialEntry(java.lang.String) throws java.io.IOException;\n  public static char[] bytesToChars(byte[]) throws java.io.IOException;\n  public java.util.List<java.lang.String> getAliases() throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry createCredentialEntry(java.lang.String, char[]) throws java.io.IOException;\n  public void deleteCredentialEntry(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry innerSetCredential(java.lang.String, char[]) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/BZip2DummyCompressor.class": "Compiled from \"BZip2DummyCompressor.java\"\npublic class org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor implements org.apache.hadoop.io.compress.Compressor {\n  public org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor();\n  public int compress(byte[], int, int) throws java.io.IOException;\n  public void end();\n  public void finish();\n  public boolean finished();\n  public long getBytesRead();\n  public long getBytesWritten();\n  public boolean needsInput();\n  public void reset();\n  public void setDictionary(byte[], int, int);\n  public void setInput(byte[], int, int);\n  public void reinit(org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicy$RetryAction.class": "Compiled from \"RetryPolicy.java\"\npublic interface org.apache.hadoop.io.retry.RetryPolicy {\n  public abstract org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception, int, int, boolean) throws java.lang.Exception;\n}\n", 
  "org/apache/hadoop/fs/TrashPolicyDefault.class": "Compiled from \"TrashPolicyDefault.java\"\npublic class org.apache.hadoop.fs.TrashPolicyDefault extends org.apache.hadoop.fs.TrashPolicy {\n  public org.apache.hadoop.fs.TrashPolicyDefault();\n  public void initialize(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public boolean isEnabled();\n  public boolean moveToTrash(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void createCheckpoint() throws java.io.IOException;\n  public void deleteCheckpoint() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getCurrentTrashDir();\n  public java.lang.Runnable getEmptier() throws java.io.IOException;\n  static org.apache.commons.logging.Log access$000();\n  static org.apache.hadoop.fs.Path access$100(org.apache.hadoop.fs.TrashPolicyDefault);\n  org.apache.hadoop.fs.TrashPolicyDefault(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.TrashPolicyDefault$1) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/DirectDecompressionCodec.class": "Compiled from \"DirectDecompressionCodec.java\"\npublic interface org.apache.hadoop.io.compress.DirectDecompressionCodec extends org.apache.hadoop.io.compress.CompressionCodec {\n  public abstract org.apache.hadoop.io.compress.DirectDecompressor createDirectDecompressor();\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticationHandler.class": "Compiled from \"PseudoDelegationTokenAuthenticationHandler.java\"\npublic class org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler {\n  public org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricGaugeLong.class": "Compiled from \"MetricGaugeLong.java\"\nclass org.apache.hadoop.metrics2.impl.MetricGaugeLong extends org.apache.hadoop.metrics2.AbstractMetric {\n  final long value;\n  org.apache.hadoop.metrics2.impl.MetricGaugeLong(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public java.lang.Long value();\n  public org.apache.hadoop.metrics2.MetricType type();\n  public void visit(org.apache.hadoop.metrics2.MetricsVisitor);\n  public java.lang.Number value();\n}\n", 
  "org/apache/hadoop/fs/HarFileSystem.class": "Compiled from \"HarFileSystem.java\"\npublic class org.apache.hadoop.fs.HarFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final java.lang.String METADATA_CACHE_ENTRIES_KEY;\n  public static final int METADATA_CACHE_ENTRIES_DEFAULT;\n  public static final int VERSION;\n  public org.apache.hadoop.fs.HarFileSystem();\n  public java.lang.String getScheme();\n  public org.apache.hadoop.fs.HarFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public int getHarVersion() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  public java.net.URI getUri();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  static org.apache.hadoop.fs.BlockLocation[] fixBlockLocations(org.apache.hadoop.fs.BlockLocation[], long, long, long);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public static int getHarHash(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long);\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  org.apache.hadoop.fs.HarFileSystem$HarMetaData getMetadata();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  static java.lang.String access$200(org.apache.hadoop.fs.HarFileSystem, java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.hadoop.fs.HarFileSystem$HarMetaData access$300(org.apache.hadoop.fs.HarFileSystem);\n  static java.lang.String access$400(java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.commons.logging.Log access$500();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts$BytesPerChecksum.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation$UgiMetrics.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ListSpanReceiversResponseProto$Builder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshProtocolService$Stub.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/CredentialProvider.class": "Compiled from \"CredentialProvider.java\"\npublic abstract class org.apache.hadoop.security.alias.CredentialProvider {\n  public static final java.lang.String CLEAR_TEXT_FALLBACK;\n  public org.apache.hadoop.security.alias.CredentialProvider();\n  public boolean isTransient();\n  public abstract void flush() throws java.io.IOException;\n  public abstract org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry getCredentialEntry(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> getAliases() throws java.io.IOException;\n  public abstract org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry createCredentialEntry(java.lang.String, char[]) throws java.io.IOException;\n  public abstract void deleteCredentialEntry(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$TraceAdminService$2.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider$Factory.class": "Compiled from \"LocalJavaKeyStoreProvider.java\"\npublic final class org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider extends org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider {\n  public static final java.lang.String SCHEME_NAME;\n  protected java.lang.String getSchemeName();\n  protected java.io.OutputStream getOutputStreamForKeystore() throws java.io.IOException;\n  protected boolean keystoreExists() throws java.io.IOException;\n  protected java.io.InputStream getInputStreamForFile() throws java.io.IOException;\n  protected void createPermissions(java.lang.String) throws java.io.IOException;\n  protected void stashOriginalFilePermissions() throws java.io.IOException;\n  protected void initFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ListSpanReceiversRequestProto$Builder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/MapWritable.class": "Compiled from \"MapWritable.java\"\npublic class org.apache.hadoop.io.MapWritable extends org.apache.hadoop.io.AbstractMapWritable implements java.util.Map<org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable> {\n  public org.apache.hadoop.io.MapWritable();\n  public org.apache.hadoop.io.MapWritable(org.apache.hadoop.io.MapWritable);\n  public void clear();\n  public boolean containsKey(java.lang.Object);\n  public boolean containsValue(java.lang.Object);\n  public java.util.Set<java.util.Map$Entry<org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable>> entrySet();\n  public boolean equals(java.lang.Object);\n  public org.apache.hadoop.io.Writable get(java.lang.Object);\n  public int hashCode();\n  public boolean isEmpty();\n  public java.util.Set<org.apache.hadoop.io.Writable> keySet();\n  public org.apache.hadoop.io.Writable put(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable);\n  public void putAll(java.util.Map<? extends org.apache.hadoop.io.Writable, ? extends org.apache.hadoop.io.Writable>);\n  public org.apache.hadoop.io.Writable remove(java.lang.Object);\n  public int size();\n  public java.util.Collection<org.apache.hadoop.io.Writable> values();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public java.lang.Object remove(java.lang.Object);\n  public java.lang.Object put(java.lang.Object, java.lang.Object);\n  public java.lang.Object get(java.lang.Object);\n}\n", 
  "org/apache/hadoop/ipc/RpcClientException.class": "Compiled from \"RpcClientException.java\"\npublic class org.apache.hadoop.ipc.RpcClientException extends org.apache.hadoop.ipc.RpcException {\n  org.apache.hadoop.ipc.RpcClientException(java.lang.String);\n  org.apache.hadoop.ipc.RpcClientException(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/security/HadoopKerberosName$1.class": "Compiled from \"HadoopKerberosName.java\"\npublic class org.apache.hadoop.security.HadoopKerberosName extends org.apache.hadoop.security.authentication.util.KerberosName {\n  public org.apache.hadoop.security.HadoopKerberosName(java.lang.String);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n", 
  "org/apache/hadoop/fs/FileContext$Util$1.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics2.lib.package-info {\n}\n", 
  "org/apache/hadoop/security/token/Token.class": "Compiled from \"Token.java\"\npublic class org.apache.hadoop.security.token.Token<T extends org.apache.hadoop.security.token.TokenIdentifier> implements org.apache.hadoop.io.Writable {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.token.Token(T, org.apache.hadoop.security.token.SecretManager<T>);\n  public org.apache.hadoop.security.token.Token(byte[], byte[], org.apache.hadoop.io.Text, org.apache.hadoop.io.Text);\n  public org.apache.hadoop.security.token.Token();\n  public org.apache.hadoop.security.token.Token(org.apache.hadoop.security.token.Token<T>);\n  public byte[] getIdentifier();\n  public T decodeIdentifier() throws java.io.IOException;\n  public byte[] getPassword();\n  public synchronized org.apache.hadoop.io.Text getKind();\n  public synchronized void setKind(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text getService();\n  public void setService(org.apache.hadoop.io.Text);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.lang.String encodeToUrlString() throws java.io.IOException;\n  public void decodeFromUrlString(java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public boolean isManaged() throws java.io.IOException;\n  public long renew(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException;\n  public void cancel(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.io.Text access$000(org.apache.hadoop.security.token.Token);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Touch$Touchz.class": "Compiled from \"Touchz.java\"\nclass org.apache.hadoop.fs.shell.Touch extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.Touch();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ipc/DecayRpcSchedulerMXBean.class": "Compiled from \"DecayRpcSchedulerMXBean.java\"\npublic interface org.apache.hadoop.ipc.DecayRpcSchedulerMXBean {\n  public abstract java.lang.String getSchedulingDecisionSummary();\n  public abstract java.lang.String getCallVolumeSummary();\n  public abstract int getUniqueIdentityCount();\n  public abstract long getTotalCallVolume();\n}\n", 
  "org/apache/hadoop/record/compiler/JLong$CppLong.class": "Compiled from \"JLong.java\"\npublic class org.apache.hadoop.record.compiler.JLong extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JLong();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.class": "Compiled from \"MD5MD5CRC32CastagnoliFileChecksum.java\"\npublic class org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum extends org.apache.hadoop.fs.MD5MD5CRC32FileChecksum {\n  public org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum();\n  public org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum(int, long, org.apache.hadoop.io.MD5Hash);\n  public org.apache.hadoop.util.DataChecksum$Type getCrcType();\n}\n", 
  "org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.class": "Compiled from \"HAServiceProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB implements org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB {\n  public org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB(org.apache.hadoop.ha.HAServiceProtocol);\n  public org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthResponseProto monitorHealth(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveResponseProto transitionToActive(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyResponseProto transitionToStandby(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto getServiceStatus(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto) throws com.google.protobuf.ServiceException;\n  public long getProtocolVersion(java.lang.String, long) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(java.lang.String, long, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HAServiceProtocol$HAServiceState.class": "Compiled from \"HAServiceProtocol.java\"\npublic interface org.apache.hadoop.ha.HAServiceProtocol {\n  public static final long versionID;\n  public abstract void monitorHealth() throws org.apache.hadoop.ha.HealthCheckFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract void transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws org.apache.hadoop.ha.ServiceFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract void transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws org.apache.hadoop.ha.ServiceFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract org.apache.hadoop.ha.HAServiceStatus getServiceStatus() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n}\n", 
  "org/apache/hadoop/net/NetworkTopology.class": "Compiled from \"NetworkTopology.java\"\npublic class org.apache.hadoop.net.NetworkTopology {\n  public static final java.lang.String DEFAULT_RACK;\n  public static final int DEFAULT_HOST_LEVEL;\n  public static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.net.NetworkTopology$InnerNode clusterMap;\n  protected int numOfRacks;\n  protected java.util.concurrent.locks.ReadWriteLock netlock;\n  public static org.apache.hadoop.net.NetworkTopology getInstance(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.net.NetworkTopology();\n  public void add(org.apache.hadoop.net.Node);\n  protected org.apache.hadoop.net.Node getNodeForNetworkLocation(org.apache.hadoop.net.Node);\n  public java.util.List<org.apache.hadoop.net.Node> getDatanodesInRack(java.lang.String);\n  public void remove(org.apache.hadoop.net.Node);\n  public boolean contains(org.apache.hadoop.net.Node);\n  public org.apache.hadoop.net.Node getNode(java.lang.String);\n  public java.lang.String getRack(java.lang.String);\n  public int getNumOfRacks();\n  public int getNumOfLeaves();\n  public int getDistance(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isOnSameRack(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isNodeGroupAware();\n  public boolean isOnSameNodeGroup(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  protected boolean isSameParents(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  void setRandomSeed(long);\n  public org.apache.hadoop.net.Node chooseRandom(java.lang.String);\n  public java.util.List<org.apache.hadoop.net.Node> getLeaves(java.lang.String);\n  public int countNumOfAvailableNodes(java.lang.String, java.util.Collection<org.apache.hadoop.net.Node>);\n  public java.lang.String toString();\n  public static java.lang.String getFirstHalf(java.lang.String);\n  public static java.lang.String getLastHalf(java.lang.String);\n  protected int getWeight(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public void sortByDistance(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node[], int);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RefreshRegistry.class": "Compiled from \"RefreshRegistry.java\"\npublic class org.apache.hadoop.ipc.RefreshRegistry {\n  public static final org.apache.commons.logging.Log LOG;\n  public static org.apache.hadoop.ipc.RefreshRegistry defaultRegistry();\n  public org.apache.hadoop.ipc.RefreshRegistry();\n  public synchronized void register(java.lang.String, org.apache.hadoop.ipc.RefreshHandler);\n  public synchronized boolean unregister(java.lang.String, org.apache.hadoop.ipc.RefreshHandler);\n  public synchronized void unregisterAll(java.lang.String);\n  public synchronized java.util.Collection<org.apache.hadoop.ipc.RefreshResponse> dispatch(java.lang.String, java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/OpensslCipher$Padding.class": "Compiled from \"OpensslCipher.java\"\npublic final class org.apache.hadoop.crypto.OpensslCipher {\n  public static final int ENCRYPT_MODE;\n  public static final int DECRYPT_MODE;\n  public static java.lang.String getLoadingFailureReason();\n  public static final org.apache.hadoop.crypto.OpensslCipher getInstance(java.lang.String) throws java.security.NoSuchAlgorithmException, javax.crypto.NoSuchPaddingException;\n  public void init(int, byte[], byte[]);\n  public int update(java.nio.ByteBuffer, java.nio.ByteBuffer) throws javax.crypto.ShortBufferException;\n  public int doFinal(java.nio.ByteBuffer) throws javax.crypto.ShortBufferException, javax.crypto.IllegalBlockSizeException, javax.crypto.BadPaddingException;\n  public void clean();\n  protected void finalize() throws java.lang.Throwable;\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec$OpensslAesCtrCipher.class": "Compiled from \"OpensslAesCtrCryptoCodec.java\"\npublic class org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec extends org.apache.hadoop.crypto.AesCtrCryptoCodec {\n  public org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  protected void finalize() throws java.lang.Throwable;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public org.apache.hadoop.crypto.Encryptor createEncryptor() throws java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.Decryptor createDecryptor() throws java.security.GeneralSecurityException;\n  public void generateSecureRandom(byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/Result.class": "Compiled from \"Result.java\"\npublic final class org.apache.hadoop.fs.shell.find.Result {\n  public static final org.apache.hadoop.fs.shell.find.Result PASS;\n  public static final org.apache.hadoop.fs.shell.find.Result FAIL;\n  public static final org.apache.hadoop.fs.shell.find.Result STOP;\n  public boolean isDescend();\n  public boolean isPass();\n  public org.apache.hadoop.fs.shell.find.Result combine(org.apache.hadoop.fs.shell.find.Result);\n  public org.apache.hadoop.fs.shell.find.Result negate();\n  public java.lang.String toString();\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/io/ReadaheadPool$ReadaheadRequest.class": "Compiled from \"ReadaheadPool.java\"\npublic class org.apache.hadoop.io.ReadaheadPool {\n  static final org.apache.commons.logging.Log LOG;\n  public static org.apache.hadoop.io.ReadaheadPool getInstance();\n  public org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest readaheadStream(java.lang.String, java.io.FileDescriptor, long, long, long, org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest);\n  public org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest submitReadahead(java.lang.String, java.io.FileDescriptor, long, long);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolInfoService$BlockingStub.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Client$Connection$2.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$AddSpanReceiverResponseProto$Builder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsShell$Help.class": "Compiled from \"FsShell.java\"\npublic class org.apache.hadoop.fs.FsShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.fs.shell.CommandFactory commandFactory;\n  public org.apache.hadoop.fs.FsShell();\n  public org.apache.hadoop.fs.FsShell(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.fs.FileSystem getFS() throws java.io.IOException;\n  protected org.apache.hadoop.fs.Trash getTrash() throws java.io.IOException;\n  protected void init() throws java.io.IOException;\n  protected void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.Path getCurrentTrashDir() throws java.io.IOException;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public void close() throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  protected static org.apache.hadoop.fs.FsShell newShellInstance();\n  static void access$000(org.apache.hadoop.fs.FsShell, java.io.PrintStream);\n  static void access$100(org.apache.hadoop.fs.FsShell, java.io.PrintStream, java.lang.String);\n  static void access$200(org.apache.hadoop.fs.FsShell, java.io.PrintStream);\n  static void access$300(org.apache.hadoop.fs.FsShell, java.io.PrintStream, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$State.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$8.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController$1.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/io/MapFile$Reader.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$UncompressedBytes.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/ElasticByteBufferPool$Key.class": "Compiled from \"ElasticByteBufferPool.java\"\npublic final class org.apache.hadoop.io.ElasticByteBufferPool implements org.apache.hadoop.io.ByteBufferPool {\n  public org.apache.hadoop.io.ElasticByteBufferPool();\n  public synchronized java.nio.ByteBuffer getBuffer(boolean, int);\n  public synchronized void putBuffer(java.nio.ByteBuffer);\n}\n", 
  "org/apache/hadoop/io/Writable.class": "Compiled from \"Writable.java\"\npublic interface org.apache.hadoop.io.Writable {\n  public abstract void write(java.io.DataOutput) throws java.io.IOException;\n  public abstract void readFields(java.io.DataInput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$Writer$ValueRegister.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/log/metrics/EventCounter$1.class": "Compiled from \"EventCounter.java\"\npublic class org.apache.hadoop.log.metrics.EventCounter extends org.apache.log4j.AppenderSkeleton {\n  public org.apache.hadoop.log.metrics.EventCounter();\n  public static long getFatal();\n  public static long getError();\n  public static long getWarn();\n  public static long getInfo();\n  public void append(org.apache.log4j.spi.LoggingEvent);\n  public void close();\n  public boolean requiresLayout();\n  static {};\n}\n", 
  "org/apache/hadoop/security/WhitelistBasedResolver.class": "Compiled from \"WhitelistBasedResolver.java\"\npublic class org.apache.hadoop.security.WhitelistBasedResolver extends org.apache.hadoop.security.SaslPropertiesResolver {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String HADOOP_SECURITY_SASL_FIXEDWHITELIST_FILE;\n  public static final java.lang.String HADOOP_SECURITY_SASL_VARIABLEWHITELIST_ENABLE;\n  public static final java.lang.String HADOOP_SECURITY_SASL_VARIABLEWHITELIST_FILE;\n  public static final java.lang.String HADOOP_SECURITY_SASL_VARIABLEWHITELIST_CACHE_SECS;\n  public static final java.lang.String HADOOP_RPC_PROTECTION_NON_WHITELIST;\n  public org.apache.hadoop.security.WhitelistBasedResolver();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public java.util.Map<java.lang.String, java.lang.String> getServerProperties(java.net.InetAddress);\n  public java.util.Map<java.lang.String, java.lang.String> getServerProperties(java.lang.String) throws java.net.UnknownHostException;\n  static java.util.Map<java.lang.String, java.lang.String> getSaslProperties(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.class": "Compiled from \"Bzip2Compressor.java\"\npublic class org.apache.hadoop.io.compress.bzip2.Bzip2Compressor implements org.apache.hadoop.io.compress.Compressor {\n  static final int DEFAULT_BLOCK_SIZE;\n  static final int DEFAULT_WORK_FACTOR;\n  public org.apache.hadoop.io.compress.bzip2.Bzip2Compressor();\n  public org.apache.hadoop.io.compress.bzip2.Bzip2Compressor(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.io.compress.bzip2.Bzip2Compressor(int, int, int);\n  public synchronized void reinit(org.apache.hadoop.conf.Configuration);\n  public synchronized void setInput(byte[], int, int);\n  synchronized void setInputFromSavedData();\n  public synchronized void setDictionary(byte[], int, int);\n  public synchronized boolean needsInput();\n  public synchronized void finish();\n  public synchronized boolean finished();\n  public synchronized int compress(byte[], int, int) throws java.io.IOException;\n  public synchronized long getBytesWritten();\n  public synchronized long getBytesRead();\n  public synchronized void reset();\n  public synchronized void end();\n  static void initSymbols(java.lang.String);\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/util/DiskChecker$DiskErrorException.class": "Compiled from \"DiskChecker.java\"\npublic class org.apache.hadoop.util.DiskChecker {\n  public org.apache.hadoop.util.DiskChecker();\n  public static boolean mkdirsWithExistsCheck(java.io.File);\n  public static void checkDirs(java.io.File) throws org.apache.hadoop.util.DiskChecker$DiskErrorException;\n  public static void checkDir(java.io.File) throws org.apache.hadoop.util.DiskChecker$DiskErrorException;\n  public static void mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static void checkDir(org.apache.hadoop.fs.LocalFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.util.DiskChecker$DiskErrorException, java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$Server.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenManager$ZKSecretManager.class": "Compiled from \"DelegationTokenManager.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenManager {\n  public static final java.lang.String ENABLE_ZK_KEY;\n  public static final java.lang.String PREFIX;\n  public static final java.lang.String UPDATE_INTERVAL;\n  public static final long UPDATE_INTERVAL_DEFAULT;\n  public static final java.lang.String MAX_LIFETIME;\n  public static final long MAX_LIFETIME_DEFAULT;\n  public static final java.lang.String RENEW_INTERVAL;\n  public static final long RENEW_INTERVAL_DEFAULT;\n  public static final java.lang.String REMOVAL_SCAN_INTERVAL;\n  public static final long REMOVAL_SCAN_INTERVAL_DEFAULT;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.Text);\n  public void setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  public void init();\n  public void destroy();\n  public org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> createToken(org.apache.hadoop.security.UserGroupInformation, java.lang.String);\n  public long renewToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public void cancelToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.UserGroupInformation verifyToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>) throws java.io.IOException;\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager getDelegationTokenSecretManager();\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier access$000(org.apache.hadoop.security.token.Token, org.apache.hadoop.io.Text) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/shell/Touch.class": "Compiled from \"Touchz.java\"\nclass org.apache.hadoop.fs.shell.Touch extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.Touch();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/io/compress/zlib/BuiltInZlibDeflater.class": "Compiled from \"BuiltInZlibDeflater.java\"\npublic class org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater extends java.util.zip.Deflater implements org.apache.hadoop.io.compress.Compressor {\n  public org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater(int, boolean);\n  public org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater(int);\n  public org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater();\n  public synchronized int compress(byte[], int, int) throws java.io.IOException;\n  public void reinit(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$GracefulFailoverRequestProto$1.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Stat.class": "Compiled from \"Stat.java\"\npublic class org.apache.hadoop.fs.Stat extends org.apache.hadoop.util.Shell {\n  public org.apache.hadoop.fs.Stat(org.apache.hadoop.fs.Path, long, boolean, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus() throws java.io.IOException;\n  public static boolean isAvailable();\n  org.apache.hadoop.fs.FileStatus getFileStatusForTesting();\n  protected java.lang.String[] getExecString();\n  protected void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/StringInterner.class": "Compiled from \"StringInterner.java\"\npublic class org.apache.hadoop.util.StringInterner {\n  public org.apache.hadoop.util.StringInterner();\n  public static java.lang.String strongIntern(java.lang.String);\n  public static java.lang.String weakIntern(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RpcServerException.class": "Compiled from \"RpcServerException.java\"\npublic class org.apache.hadoop.ipc.RpcServerException extends org.apache.hadoop.ipc.RpcException {\n  public org.apache.hadoop.ipc.RpcServerException(java.lang.String);\n  public org.apache.hadoop.ipc.RpcServerException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto getRpcStatusProto();\n  public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto getRpcErrorCodeProto();\n}\n", 
  "org/apache/hadoop/crypto/key/CachingKeyProvider$KeyNotFoundException.class": "Compiled from \"CachingKeyProvider.java\"\npublic class org.apache.hadoop.crypto.key.CachingKeyProvider extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension> {\n  public org.apache.hadoop.crypto.key.CachingKeyProvider(org.apache.hadoop.crypto.key.KeyProvider, long, long);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/shell/SnapshotCommands$RenameSnapshot.class": "Compiled from \"SnapshotCommands.java\"\nclass org.apache.hadoop.fs.shell.SnapshotCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.SnapshotCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/io/ByteWritable.class": "Compiled from \"ByteWritable.java\"\npublic class org.apache.hadoop.io.ByteWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.ByteWritable> {\n  public org.apache.hadoop.io.ByteWritable();\n  public org.apache.hadoop.io.ByteWritable(byte);\n  public void set(byte);\n  public byte get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.ByteWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$ConnectionManager$1.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/io/GenericWritable.class": "Compiled from \"GenericWritable.java\"\npublic abstract class org.apache.hadoop.io.GenericWritable implements org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configurable {\n  public org.apache.hadoop.io.GenericWritable();\n  public void set(org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.io.Writable get();\n  public java.lang.String toString();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  protected abstract java.lang.Class<? extends org.apache.hadoop.io.Writable>[] getTypes();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/metrics2/lib/MetricsAnnotations.class": "Compiled from \"MetricsAnnotations.java\"\npublic class org.apache.hadoop.metrics2.lib.MetricsAnnotations {\n  public org.apache.hadoop.metrics2.lib.MetricsAnnotations();\n  public static org.apache.hadoop.metrics2.MetricsSource makeSource(java.lang.Object);\n  public static org.apache.hadoop.metrics2.lib.MetricsSourceBuilder newSourceBuilder(java.lang.Object);\n}\n", 
  "org/apache/hadoop/fs/shell/Display$Checksum.class": "Compiled from \"Display.java\"\nclass org.apache.hadoop.fs.shell.Display extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.Display();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/io/BooleanWritable.class": "Compiled from \"BooleanWritable.java\"\npublic class org.apache.hadoop.io.BooleanWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.BooleanWritable> {\n  public org.apache.hadoop.io.BooleanWritable();\n  public org.apache.hadoop.io.BooleanWritable(boolean);\n  public void set(boolean);\n  public boolean get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.BooleanWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/OutputRecord.class": "Compiled from \"OutputRecord.java\"\npublic class org.apache.hadoop.metrics.spi.OutputRecord {\n  org.apache.hadoop.metrics.spi.OutputRecord(org.apache.hadoop.metrics.spi.AbstractMetricsContext$TagMap, org.apache.hadoop.metrics.spi.AbstractMetricsContext$MetricMap);\n  public java.util.Set<java.lang.String> getTagNames();\n  public java.lang.Object getTag(java.lang.String);\n  public java.util.Set<java.lang.String> getMetricNames();\n  public java.lang.Number getMetric(java.lang.String);\n  public org.apache.hadoop.metrics.spi.AbstractMetricsContext$TagMap getTagsCopy();\n  public org.apache.hadoop.metrics.spi.AbstractMetricsContext$MetricMap getMetricsCopy();\n}\n", 
  "org/apache/hadoop/util/ChunkedArrayList.class": "Compiled from \"ChunkedArrayList.java\"\npublic class org.apache.hadoop.util.ChunkedArrayList<T> extends java.util.AbstractList<T> {\n  public org.apache.hadoop.util.ChunkedArrayList();\n  public org.apache.hadoop.util.ChunkedArrayList(int, int);\n  public java.util.Iterator<T> iterator();\n  public boolean add(T);\n  public void clear();\n  public boolean isEmpty();\n  public int size();\n  int getNumChunks();\n  int getMaxChunkSize();\n  public T get(int);\n  static int access$010(org.apache.hadoop.util.ChunkedArrayList);\n}\n", 
  "org/apache/hadoop/service/ServiceStateModel.class": "Compiled from \"ServiceStateModel.java\"\npublic class org.apache.hadoop.service.ServiceStateModel {\n  public org.apache.hadoop.service.ServiceStateModel(java.lang.String);\n  public org.apache.hadoop.service.ServiceStateModel(java.lang.String, org.apache.hadoop.service.Service$STATE);\n  public org.apache.hadoop.service.Service$STATE getState();\n  public boolean isInState(org.apache.hadoop.service.Service$STATE);\n  public void ensureCurrentState(org.apache.hadoop.service.Service$STATE);\n  public synchronized org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE);\n  public static void checkStateTransition(java.lang.String, org.apache.hadoop.service.Service$STATE, org.apache.hadoop.service.Service$STATE);\n  public static boolean isValidStateTransition(org.apache.hadoop.service.Service$STATE, org.apache.hadoop.service.Service$STATE);\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ClientCache.class": "Compiled from \"ClientCache.java\"\npublic class org.apache.hadoop.ipc.ClientCache {\n  public org.apache.hadoop.ipc.ClientCache();\n  public synchronized org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, java.lang.Class<? extends org.apache.hadoop.io.Writable>);\n  public synchronized org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public synchronized org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public void stopClient(org.apache.hadoop.ipc.Client);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolInfoService$2.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/IndexedSortable.class": "Compiled from \"IndexedSortable.java\"\npublic interface org.apache.hadoop.util.IndexedSortable {\n  public abstract int compare(int, int);\n  public abstract void swap(int, int);\n}\n", 
  "org/apache/hadoop/ipc/DecayRpcScheduler$DecayTask.class": "Compiled from \"DecayRpcScheduler.java\"\npublic class org.apache.hadoop.ipc.DecayRpcScheduler implements org.apache.hadoop.ipc.RpcScheduler,org.apache.hadoop.ipc.DecayRpcSchedulerMXBean {\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_PERIOD_KEY;\n  public static final long IPC_CALLQUEUE_DECAYSCHEDULER_PERIOD_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_FACTOR_KEY;\n  public static final double IPC_CALLQUEUE_DECAYSCHEDULER_FACTOR_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_THRESHOLDS_KEY;\n  public static final java.lang.String DECAYSCHEDULER_UNKNOWN_IDENTITY;\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.DecayRpcScheduler(int, java.lang.String, org.apache.hadoop.conf.Configuration);\n  public int getPriorityLevel(org.apache.hadoop.ipc.Schedulable);\n  public double getDecayFactor();\n  public long getDecayPeriodMillis();\n  public double[] getThresholds();\n  public void forceDecay();\n  public java.util.Map<java.lang.Object, java.lang.Long> getCallCountSnapshot();\n  public long getTotalCallSnapshot();\n  public int getUniqueIdentityCount();\n  public long getTotalCallVolume();\n  public java.lang.String getSchedulingDecisionSummary();\n  public java.lang.String getCallVolumeSummary();\n  static void access$000(org.apache.hadoop.ipc.DecayRpcScheduler);\n  static {};\n}\n", 
  "org/apache/hadoop/io/ArrayFile.class": "Compiled from \"ArrayFile.java\"\npublic class org.apache.hadoop.io.ArrayFile extends org.apache.hadoop.io.MapFile {\n  protected org.apache.hadoop.io.ArrayFile();\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$WrapperException.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HAServiceProtocolHelper.class": "Compiled from \"HAServiceProtocolHelper.java\"\npublic class org.apache.hadoop.ha.HAServiceProtocolHelper {\n  public org.apache.hadoop.ha.HAServiceProtocolHelper();\n  public static void monitorHealth(org.apache.hadoop.ha.HAServiceProtocol, org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws java.io.IOException;\n  public static void transitionToActive(org.apache.hadoop.ha.HAServiceProtocol, org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws java.io.IOException;\n  public static void transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol, org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/bloom/CountingBloomFilter.class": "Compiled from \"CountingBloomFilter.java\"\npublic final class org.apache.hadoop.util.bloom.CountingBloomFilter extends org.apache.hadoop.util.bloom.Filter {\n  public org.apache.hadoop.util.bloom.CountingBloomFilter();\n  public org.apache.hadoop.util.bloom.CountingBloomFilter(int, int, int);\n  public void add(org.apache.hadoop.util.bloom.Key);\n  public void delete(org.apache.hadoop.util.bloom.Key);\n  public void and(org.apache.hadoop.util.bloom.Filter);\n  public boolean membershipTest(org.apache.hadoop.util.bloom.Key);\n  public int approximateCount(org.apache.hadoop.util.bloom.Key);\n  public void not();\n  public void or(org.apache.hadoop.util.bloom.Filter);\n  public void xor(org.apache.hadoop.util.bloom.Filter);\n  public java.lang.String toString();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$RpcWrapper.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/util/curator/ChildReaper.class": "Compiled from \"ChildReaper.java\"\npublic class org.apache.hadoop.util.curator.ChildReaper implements java.io.Closeable {\n  static final int DEFAULT_REAPING_THRESHOLD_MS;\n  public static <E extends java/lang/Object> java.util.Set<E> newConcurrentHashSet();\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, int);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, java.util.concurrent.ScheduledExecutorService, int);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, java.util.concurrent.ScheduledExecutorService, int, java.lang.String);\n  public void start() throws java.lang.Exception;\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.util.curator.ChildReaper addPath(java.lang.String);\n  public boolean removePath(java.lang.String);\n  static void access$000(org.apache.hadoop.util.curator.ChildReaper);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HealthMonitor$1.class": "Compiled from \"HealthMonitor.java\"\npublic class org.apache.hadoop.ha.HealthMonitor {\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.HealthMonitor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  public void addCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public void removeCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public synchronized void addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public synchronized void removeServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public void shutdown();\n  public synchronized org.apache.hadoop.ha.HAServiceProtocol getProxy();\n  protected org.apache.hadoop.ha.HAServiceProtocol createProxy() throws java.io.IOException;\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getHealthState();\n  synchronized org.apache.hadoop.ha.HAServiceStatus getLastServiceStatus();\n  boolean isAlive();\n  void join() throws java.lang.InterruptedException;\n  void start();\n  static org.apache.hadoop.ha.HAServiceTarget access$100(org.apache.hadoop.ha.HealthMonitor);\n  static org.apache.commons.logging.Log access$200();\n  static void access$300(org.apache.hadoop.ha.HealthMonitor, org.apache.hadoop.ha.HealthMonitor$State);\n  static boolean access$400(org.apache.hadoop.ha.HealthMonitor);\n  static void access$500(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static void access$600(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetUserMappingsProtocolService$Stub.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/Closeable.class": "Compiled from \"Closeable.java\"\npublic interface org.apache.hadoop.io.Closeable extends java.io.Closeable {\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsServerDefaults.class": "Compiled from \"FsServerDefaults.java\"\npublic class org.apache.hadoop.fs.FsServerDefaults implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.fs.FsServerDefaults();\n  public org.apache.hadoop.fs.FsServerDefaults(long, int, int, short, int, boolean, long, org.apache.hadoop.util.DataChecksum$Type);\n  public long getBlockSize();\n  public int getBytesPerChecksum();\n  public int getWritePacketSize();\n  public short getReplication();\n  public int getFileBufferSize();\n  public boolean getEncryptDataTransfer();\n  public long getTrashInterval();\n  public org.apache.hadoop.util.DataChecksum$Type getChecksumType();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$10.class": "", 
  "org/apache/hadoop/io/file/tfile/TFile$Reader.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/Token$PrivateToken.class": "Compiled from \"Token.java\"\npublic class org.apache.hadoop.security.token.Token<T extends org.apache.hadoop.security.token.TokenIdentifier> implements org.apache.hadoop.io.Writable {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.token.Token(T, org.apache.hadoop.security.token.SecretManager<T>);\n  public org.apache.hadoop.security.token.Token(byte[], byte[], org.apache.hadoop.io.Text, org.apache.hadoop.io.Text);\n  public org.apache.hadoop.security.token.Token();\n  public org.apache.hadoop.security.token.Token(org.apache.hadoop.security.token.Token<T>);\n  public byte[] getIdentifier();\n  public T decodeIdentifier() throws java.io.IOException;\n  public byte[] getPassword();\n  public synchronized org.apache.hadoop.io.Text getKind();\n  public synchronized void setKind(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text getService();\n  public void setService(org.apache.hadoop.io.Text);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.lang.String encodeToUrlString() throws java.io.IOException;\n  public void decodeFromUrlString(java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public boolean isManaged() throws java.io.IOException;\n  public long renew(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException;\n  public void cancel(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.io.Text access$000(org.apache.hadoop.security.token.Token);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HasEnhancedByteBufferAccess.class": "Compiled from \"HasEnhancedByteBufferAccess.java\"\npublic interface org.apache.hadoop.fs.HasEnhancedByteBufferAccess {\n  public abstract java.nio.ByteBuffer read(org.apache.hadoop.io.ByteBufferPool, int, java.util.EnumSet<org.apache.hadoop.fs.ReadOption>) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public abstract void releaseBuffer(java.nio.ByteBuffer);\n}\n", 
  "org/apache/hadoop/metrics/util/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics.util.package-info {\n}\n", 
  "org/apache/hadoop/fs/PathIsNotDirectoryException.class": "Compiled from \"PathIsNotDirectoryException.java\"\npublic class org.apache.hadoop.fs.PathIsNotDirectoryException extends org.apache.hadoop.fs.PathExistsException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.PathIsNotDirectoryException(java.lang.String);\n}\n", 
  "org/apache/hadoop/record/XmlRecordInput.class": "Compiled from \"XmlRecordInput.java\"\npublic class org.apache.hadoop.record.XmlRecordInput implements org.apache.hadoop.record.RecordInput {\n  public org.apache.hadoop.record.XmlRecordInput(java.io.InputStream);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  static int access$000(org.apache.hadoop.record.XmlRecordInput);\n  static java.util.ArrayList access$100(org.apache.hadoop.record.XmlRecordInput);\n  static int access$008(org.apache.hadoop.record.XmlRecordInput);\n}\n", 
  "org/apache/hadoop/io/serializer/DeserializerComparator.class": "Compiled from \"DeserializerComparator.java\"\npublic abstract class org.apache.hadoop.io.serializer.DeserializerComparator<T> implements org.apache.hadoop.io.RawComparator<T> {\n  protected org.apache.hadoop.io.serializer.DeserializerComparator(org.apache.hadoop.io.serializer.Deserializer<T>) throws java.io.IOException;\n  public int compare(byte[], int, int, byte[], int, int);\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$ProgressableOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension.class": "Compiled from \"KeyProviderDelegationTokenExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension> {\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public static org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/IdentityProvider.class": "Compiled from \"IdentityProvider.java\"\npublic interface org.apache.hadoop.ipc.IdentityProvider {\n  public abstract java.lang.String makeIdentity(org.apache.hadoop.ipc.Schedulable);\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Metadata.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$Writer.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/CompressorStream.class": "Compiled from \"CompressorStream.java\"\npublic class org.apache.hadoop.io.compress.CompressorStream extends org.apache.hadoop.io.compress.CompressionOutputStream {\n  protected org.apache.hadoop.io.compress.Compressor compressor;\n  protected byte[] buffer;\n  protected boolean closed;\n  public org.apache.hadoop.io.compress.CompressorStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor, int);\n  public org.apache.hadoop.io.compress.CompressorStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor);\n  protected org.apache.hadoop.io.compress.CompressorStream(java.io.OutputStream);\n  public void write(byte[], int, int) throws java.io.IOException;\n  protected void compress() throws java.io.IOException;\n  public void finish() throws java.io.IOException;\n  public void resetState() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void write(int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/net/SocketOutputStream.class": "Compiled from \"SocketOutputStream.java\"\npublic class org.apache.hadoop.net.SocketOutputStream extends java.io.OutputStream implements java.nio.channels.WritableByteChannel {\n  public org.apache.hadoop.net.SocketOutputStream(java.nio.channels.WritableByteChannel, long) throws java.io.IOException;\n  public org.apache.hadoop.net.SocketOutputStream(java.net.Socket, long) throws java.io.IOException;\n  public void write(int) throws java.io.IOException;\n  public void write(byte[], int, int) throws java.io.IOException;\n  public synchronized void close() throws java.io.IOException;\n  public java.nio.channels.WritableByteChannel getChannel();\n  public boolean isOpen();\n  public int write(java.nio.ByteBuffer) throws java.io.IOException;\n  public void waitForWritable() throws java.io.IOException;\n  public void transferToFully(java.nio.channels.FileChannel, long, int, org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.LongWritable) throws java.io.IOException;\n  public void transferToFully(java.nio.channels.FileChannel, long, int) throws java.io.IOException;\n  public void setTimeout(int);\n}\n", 
  "org/apache/hadoop/io/serializer/avro/AvroSerialization.class": "Compiled from \"AvroSerialization.java\"\npublic abstract class org.apache.hadoop.io.serializer.avro.AvroSerialization<T> extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.io.serializer.Serialization<T> {\n  public static final java.lang.String AVRO_SCHEMA_KEY;\n  public org.apache.hadoop.io.serializer.avro.AvroSerialization();\n  public org.apache.hadoop.io.serializer.Deserializer<T> getDeserializer(java.lang.Class<T>);\n  public org.apache.hadoop.io.serializer.Serializer<T> getSerializer(java.lang.Class<T>);\n  public abstract org.apache.avro.Schema getSchema(T);\n  public abstract org.apache.avro.io.DatumWriter<T> getWriter(java.lang.Class<T>);\n  public abstract org.apache.avro.io.DatumReader<T> getReader(java.lang.Class<T>);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolVersionsRequestProto.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.class": "Compiled from \"ProtocolMetaInfoServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB implements org.apache.hadoop.ipc.ProtocolMetaInfoPB {\n  org.apache.hadoop.ipc.RPC$Server server;\n  public org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB(org.apache.hadoop.ipc.RPC$Server);\n  public org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto getProtocolVersions(com.google.protobuf.RpcController, org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto getProtocolSignature(com.google.protobuf.RpcController, org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto) throws com.google.protobuf.ServiceException;\n}\n", 
  "org/apache/hadoop/security/token/TokenIdentifier.class": "Compiled from \"TokenIdentifier.java\"\npublic abstract class org.apache.hadoop.security.token.TokenIdentifier implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.security.token.TokenIdentifier();\n  public abstract org.apache.hadoop.io.Text getKind();\n  public abstract org.apache.hadoop.security.UserGroupInformation getUser();\n  public byte[] getBytes();\n  public java.lang.String getTrackingId();\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFileSystem$1.class": "Compiled from \"ViewFileSystem.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFileSystem extends org.apache.hadoop.fs.FileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  java.net.URI myUri;\n  org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.FileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem() throws java.io.IOException;\n  public java.lang.String getScheme();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  org.apache.hadoop.fs.viewfs.ViewFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getTrashCanLocation(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException;\n  public java.net.URI getUri();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public long getDefaultBlockSize();\n  public short getDefaultReplication();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint[] getMountPoints();\n  static org.apache.hadoop.fs.Path access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/ArrayFile$Reader.class": "Compiled from \"ArrayFile.java\"\npublic class org.apache.hadoop.io.ArrayFile extends org.apache.hadoop.io.MapFile {\n  protected org.apache.hadoop.io.ArrayFile();\n}\n", 
  "org/apache/hadoop/io/FastByteComparisons.class": "Compiled from \"FastByteComparisons.java\"\nabstract class org.apache.hadoop.io.FastByteComparisons {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.io.FastByteComparisons();\n  public static int compareTo(byte[], int, int, byte[], int, int);\n  static org.apache.hadoop.io.FastByteComparisons$Comparer access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/security/SecurityUtil$HostResolver.class": "Compiled from \"SecurityUtil.java\"\npublic class org.apache.hadoop.security.SecurityUtil {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String HOSTNAME_PATTERN;\n  public static final java.lang.String FAILED_TO_GET_UGI_MSG_HEADER;\n  static boolean useIpForTokenService;\n  static org.apache.hadoop.security.SecurityUtil$HostResolver hostResolver;\n  public org.apache.hadoop.security.SecurityUtil();\n  public static void setTokenServiceUseIp(boolean);\n  static boolean isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal);\n  protected static boolean isOriginalTGT(javax.security.auth.kerberos.KerberosTicket);\n  public static java.lang.String getServerPrincipal(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String getServerPrincipal(java.lang.String, java.net.InetAddress) throws java.io.IOException;\n  static java.lang.String getLocalHostName() throws java.net.UnknownHostException;\n  public static void login(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void login(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String buildDTServiceName(java.net.URI, int);\n  public static java.lang.String getHostFromPrincipal(java.lang.String);\n  public static void setSecurityInfoProviders(org.apache.hadoop.security.SecurityInfo...);\n  public static org.apache.hadoop.security.KerberosInfo getKerberosInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.security.token.TokenInfo getTokenInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static java.net.InetSocketAddress getTokenServiceAddr(org.apache.hadoop.security.token.Token<?>);\n  public static void setTokenService(org.apache.hadoop.security.token.Token<?>, java.net.InetSocketAddress);\n  public static org.apache.hadoop.io.Text buildTokenService(java.net.InetSocketAddress);\n  public static org.apache.hadoop.io.Text buildTokenService(java.net.URI);\n  public static <T extends java/lang/Object> T doAsLoginUserOrFatal(java.security.PrivilegedAction<T>);\n  public static <T extends java/lang/Object> T doAsLoginUser(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException;\n  public static <T extends java/lang/Object> T doAsCurrentUser(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException;\n  public static java.net.InetAddress getByName(java.lang.String) throws java.net.UnknownHostException;\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod(org.apache.hadoop.conf.Configuration);\n  public static void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod, org.apache.hadoop.conf.Configuration);\n  public static boolean isPrivilegedPort(int);\n  static {};\n}\n", 
  "org/apache/hadoop/io/ByteBufferPool.class": "Compiled from \"ByteBufferPool.java\"\npublic interface org.apache.hadoop.io.ByteBufferPool {\n  public abstract java.nio.ByteBuffer getBuffer(boolean, int);\n  public abstract void putBuffer(java.nio.ByteBuffer);\n}\n", 
  "org/apache/hadoop/ha/SshFenceByTcpPort$LogAdapter.class": "Compiled from \"SshFenceByTcpPort.java\"\npublic class org.apache.hadoop.ha.SshFenceByTcpPort extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.ha.FenceMethod {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String CONF_CONNECT_TIMEOUT_KEY;\n  static final java.lang.String CONF_IDENTITIES_KEY;\n  public org.apache.hadoop.ha.SshFenceByTcpPort();\n  public void checkArgs(java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public boolean tryFence(org.apache.hadoop.ha.HAServiceTarget, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/DataOutputByteBuffer.class": "Compiled from \"DataOutputByteBuffer.java\"\npublic class org.apache.hadoop.io.DataOutputByteBuffer extends java.io.DataOutputStream {\n  public org.apache.hadoop.io.DataOutputByteBuffer();\n  public org.apache.hadoop.io.DataOutputByteBuffer(int);\n  public org.apache.hadoop.io.DataOutputByteBuffer(int, boolean);\n  public java.nio.ByteBuffer[] getData();\n  public int getLength();\n  public void reset();\n}\n", 
  "org/apache/hadoop/record/CsvRecordOutput.class": "Compiled from \"CsvRecordOutput.java\"\npublic class org.apache.hadoop.record.CsvRecordOutput implements org.apache.hadoop.record.RecordOutput {\n  public org.apache.hadoop.record.CsvRecordOutput(java.io.OutputStream);\n  public void writeByte(byte, java.lang.String) throws java.io.IOException;\n  public void writeBool(boolean, java.lang.String) throws java.io.IOException;\n  public void writeInt(int, java.lang.String) throws java.io.IOException;\n  public void writeLong(long, java.lang.String) throws java.io.IOException;\n  public void writeFloat(float, java.lang.String) throws java.io.IOException;\n  public void writeDouble(double, java.lang.String) throws java.io.IOException;\n  public void writeString(java.lang.String, java.lang.String) throws java.io.IOException;\n  public void writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String) throws java.io.IOException;\n  public void startRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public void endRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public void startVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public void endVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public void startMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n  public void endMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/SaslRpcServer$SaslDigestCallbackHandler.class": "Compiled from \"SaslRpcServer.java\"\npublic class org.apache.hadoop.security.SaslRpcServer {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String SASL_DEFAULT_REALM;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod authMethod;\n  public java.lang.String mechanism;\n  public java.lang.String protocol;\n  public java.lang.String serverId;\n  public org.apache.hadoop.security.SaslRpcServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod) throws java.io.IOException;\n  public javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection, java.util.Map<java.lang.String, ?>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void init(org.apache.hadoop.conf.Configuration);\n  static java.lang.String encodeIdentifier(byte[]);\n  static byte[] decodeIdentifier(java.lang.String);\n  public static <T extends org/apache/hadoop/security/token/TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  static char[] encodePassword(byte[]);\n  public static java.lang.String[] splitKerberosName(java.lang.String);\n  static javax.security.sasl.SaslServerFactory access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/LocalDirAllocator.class": "Compiled from \"LocalDirAllocator.java\"\npublic class org.apache.hadoop.fs.LocalDirAllocator {\n  public static final int SIZE_UNKNOWN;\n  public org.apache.hadoop.fs.LocalDirAllocator(java.lang.String);\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathToRead(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.Iterable<org.apache.hadoop.fs.Path> getAllLocalPathsToRead(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.io.File createTmpFileForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean isContextValid(java.lang.String);\n  public static void removeContext(java.lang.String);\n  public boolean ifExists(java.lang.String, org.apache.hadoop.conf.Configuration);\n  int getCurrentDirectoryIndex();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/PathData$PathType.class": "Compiled from \"PathData.java\"\npublic class org.apache.hadoop.fs.shell.PathData implements java.lang.Comparable<org.apache.hadoop.fs.shell.PathData> {\n  protected final java.net.URI uri;\n  public final org.apache.hadoop.fs.FileSystem fs;\n  public final org.apache.hadoop.fs.Path path;\n  public org.apache.hadoop.fs.FileStatus stat;\n  public boolean exists;\n  public org.apache.hadoop.fs.shell.PathData(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus refreshStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData suffix(java.lang.String) throws java.io.IOException;\n  public boolean parentExists() throws java.io.IOException;\n  public boolean representsDirectory();\n  public org.apache.hadoop.fs.shell.PathData[] getDirectoryContents() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData getPathDataForChild(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  public static org.apache.hadoop.fs.shell.PathData[] expandAsGlob(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String toString();\n  public java.io.File toFile();\n  public int compareTo(org.apache.hadoop.fs.shell.PathData);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Shell$OSType.class": "Compiled from \"Shell.java\"\npublic abstract class org.apache.hadoop.util.Shell {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int WINDOWS_MAX_SHELL_LENGHT;\n  public static final java.lang.String USER_NAME_COMMAND;\n  public static final java.lang.Object WindowsProcessLaunchLock;\n  public static final org.apache.hadoop.util.Shell$OSType osType;\n  public static final boolean WINDOWS;\n  public static final boolean SOLARIS;\n  public static final boolean MAC;\n  public static final boolean FREEBSD;\n  public static final boolean LINUX;\n  public static final boolean OTHER;\n  public static final boolean PPC_64;\n  public static final java.lang.String SET_PERMISSION_COMMAND;\n  public static final java.lang.String SET_OWNER_COMMAND;\n  public static final java.lang.String SET_GROUP_COMMAND;\n  public static final java.lang.String LINK_COMMAND;\n  public static final java.lang.String READ_LINK_COMMAND;\n  protected long timeOutInterval;\n  public static final java.lang.String WINUTILS;\n  public static final boolean isSetsidAvailable;\n  public static final java.lang.String TOKEN_SEPARATOR_REGEX;\n  public static boolean isJava7OrAbove();\n  public static void checkWindowsCommandLineLength(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String[] getGroupsCommand();\n  public static java.lang.String[] getGroupsForUserCommand(java.lang.String);\n  public static java.lang.String[] getUsersForNetgroupCommand(java.lang.String);\n  public static java.lang.String[] getGetPermissionCommand();\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean);\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean, java.lang.String);\n  public static java.lang.String[] getSetOwnerCommand(java.lang.String);\n  public static java.lang.String[] getSymlinkCommand(java.lang.String, java.lang.String);\n  public static java.lang.String[] getReadlinkCommand(java.lang.String);\n  public static java.lang.String[] getCheckProcessIsAliveCommand(java.lang.String);\n  public static java.lang.String[] getSignalKillCommand(int, java.lang.String);\n  public static java.lang.String getEnvironmentVariableRegex();\n  public static java.io.File appendScriptExtension(java.io.File, java.lang.String);\n  public static java.lang.String appendScriptExtension(java.lang.String);\n  public static java.lang.String[] getRunScriptCommand(java.io.File);\n  public static final java.lang.String getHadoopHome() throws java.io.IOException;\n  public static final java.lang.String getQualifiedBinPath(java.lang.String) throws java.io.IOException;\n  public static final java.lang.String getWinUtilsPath();\n  public org.apache.hadoop.util.Shell();\n  public org.apache.hadoop.util.Shell(long);\n  public org.apache.hadoop.util.Shell(long, boolean);\n  protected void setEnvironment(java.util.Map<java.lang.String, java.lang.String>);\n  protected void setWorkingDirectory(java.io.File);\n  protected void run() throws java.io.IOException;\n  protected abstract java.lang.String[] getExecString();\n  protected abstract void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public java.lang.String getEnvironment(java.lang.String);\n  public java.lang.Process getProcess();\n  public int getExitCode();\n  public boolean isTimedOut();\n  public static java.lang.String execCommand(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String[], long) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String...) throws java.io.IOException;\n  static java.util.concurrent.atomic.AtomicBoolean access$000(org.apache.hadoop.util.Shell);\n  static void access$100(org.apache.hadoop.util.Shell);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolVersionProto.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$GetDelegationTokenRequestProto$Builder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSinkAdapter$1.class": "Compiled from \"MetricsSinkAdapter.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsSinkAdapter implements org.apache.hadoop.metrics2.impl.SinkQueue$Consumer<org.apache.hadoop.metrics2.impl.MetricsBuffer> {\n  org.apache.hadoop.metrics2.impl.MetricsSinkAdapter(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, java.lang.String, org.apache.hadoop.metrics2.MetricsFilter, org.apache.hadoop.metrics2.MetricsFilter, org.apache.hadoop.metrics2.MetricsFilter, int, int, int, float, int);\n  boolean putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, long);\n  public boolean putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer);\n  void publishMetricsFromQueue();\n  public void consume(org.apache.hadoop.metrics2.impl.MetricsBuffer);\n  void start();\n  void stop();\n  java.lang.String name();\n  java.lang.String description();\n  void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  org.apache.hadoop.metrics2.MetricsSink sink();\n  public void consume(java.lang.Object) throws java.lang.InterruptedException;\n}\n", 
  "org/apache/hadoop/fs/FileContext$3.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricType.class": "Compiled from \"MetricType.java\"\npublic final class org.apache.hadoop.metrics2.MetricType extends java.lang.Enum<org.apache.hadoop.metrics2.MetricType> {\n  public static final org.apache.hadoop.metrics2.MetricType COUNTER;\n  public static final org.apache.hadoop.metrics2.MetricType GAUGE;\n  public static org.apache.hadoop.metrics2.MetricType[] values();\n  public static org.apache.hadoop.metrics2.MetricType valueOf(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/ReconfigurationTaskStatus.class": "Compiled from \"ReconfigurationTaskStatus.java\"\npublic class org.apache.hadoop.conf.ReconfigurationTaskStatus {\n  long startTime;\n  long endTime;\n  final java.util.Map<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange, com.google.common.base.Optional<java.lang.String>> status;\n  public org.apache.hadoop.conf.ReconfigurationTaskStatus(long, long, java.util.Map<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange, com.google.common.base.Optional<java.lang.String>>);\n  public boolean hasTask();\n  public boolean stopped();\n  public long getStartTime();\n  public long getEndTime();\n  public final java.util.Map<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange, com.google.common.base.Optional<java.lang.String>> getStatus();\n}\n", 
  "org/apache/hadoop/net/SocksSocketFactory.class": "Compiled from \"SocksSocketFactory.java\"\npublic class org.apache.hadoop.net.SocksSocketFactory extends javax.net.SocketFactory implements org.apache.hadoop.conf.Configurable {\n  public org.apache.hadoop.net.SocksSocketFactory();\n  public org.apache.hadoop.net.SocksSocketFactory(java.net.Proxy);\n  public java.net.Socket createSocket() throws java.io.IOException;\n  public java.net.Socket createSocket(java.net.InetAddress, int) throws java.io.IOException;\n  public java.net.Socket createSocket(java.net.InetAddress, int, java.net.InetAddress, int) throws java.io.IOException;\n  public java.net.Socket createSocket(java.lang.String, int) throws java.io.IOException, java.net.UnknownHostException;\n  public java.net.Socket createSocket(java.lang.String, int, java.net.InetAddress, int) throws java.io.IOException, java.net.UnknownHostException;\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Reader$StartOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricGaugeDouble.class": "Compiled from \"MetricGaugeDouble.java\"\nclass org.apache.hadoop.metrics2.impl.MetricGaugeDouble extends org.apache.hadoop.metrics2.AbstractMetric {\n  final double value;\n  org.apache.hadoop.metrics2.impl.MetricGaugeDouble(org.apache.hadoop.metrics2.MetricsInfo, double);\n  public java.lang.Double value();\n  public org.apache.hadoop.metrics2.MetricType type();\n  public void visit(org.apache.hadoop.metrics2.MetricsVisitor);\n  public java.lang.Number value();\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$TraceAdminService$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$Util.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/CallQueueManager.class": "Compiled from \"CallQueueManager.java\"\npublic class org.apache.hadoop.ipc.CallQueueManager<E> {\n  public static final org.apache.commons.logging.Log LOG;\n  static <E extends java/lang/Object> java.lang.Class<? extends java.util.concurrent.BlockingQueue<E>> convertQueueClass(java.lang.Class<?>, java.lang.Class<E>);\n  public org.apache.hadoop.ipc.CallQueueManager(java.lang.Class<? extends java.util.concurrent.BlockingQueue<E>>, int, java.lang.String, org.apache.hadoop.conf.Configuration);\n  public void put(E) throws java.lang.InterruptedException;\n  public E take() throws java.lang.InterruptedException;\n  public int size();\n  public synchronized void swapQueue(java.lang.Class<? extends java.util.concurrent.BlockingQueue<E>>, int, java.lang.String, org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslRpcServer$SaslGssCallbackHandler.class": "Compiled from \"SaslRpcServer.java\"\npublic class org.apache.hadoop.security.SaslRpcServer {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String SASL_DEFAULT_REALM;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod authMethod;\n  public java.lang.String mechanism;\n  public java.lang.String protocol;\n  public java.lang.String serverId;\n  public org.apache.hadoop.security.SaslRpcServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod) throws java.io.IOException;\n  public javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection, java.util.Map<java.lang.String, ?>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void init(org.apache.hadoop.conf.Configuration);\n  static java.lang.String encodeIdentifier(byte[]);\n  static byte[] decodeIdentifier(java.lang.String);\n  public static <T extends org/apache/hadoop/security/token/TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  static char[] encodePassword(byte[]);\n  public static java.lang.String[] splitKerberosName(java.lang.String);\n  static javax.security.sasl.SaslServerFactory access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtocolMetaInterface.class": "Compiled from \"ProtocolMetaInterface.java\"\npublic interface org.apache.hadoop.ipc.ProtocolMetaInterface {\n  public abstract boolean isMethodSupported(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$RpcRequestMessageWrapper.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ListSpanReceiversResponseProto$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Compression$1.class": "Compiled from \"Compression.java\"\nfinal class org.apache.hadoop.io.file.tfile.Compression {\n  static final org.apache.commons.logging.Log LOG;\n  static org.apache.hadoop.io.file.tfile.Compression$Algorithm getCompressionAlgorithmByName(java.lang.String);\n  static java.lang.String[] getSupportedAlgorithms();\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer2$StackServlet.class": "Compiled from \"HttpServer2.java\"\npublic final class org.apache.hadoop.http.HttpServer2 implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  public static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean);\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public java.net.InetSocketAddress getConnectorAddress(int);\n  public void setThreads(int, int);\n  public void start() throws java.io.IOException;\n  void openListeners() throws java.lang.Exception;\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  org.apache.hadoop.http.HttpServer2(org.apache.hadoop.http.HttpServer2$Builder, org.apache.hadoop.http.HttpServer2$1) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.http.HttpServer2, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.http.HttpServer2, org.mortbay.jetty.Connector);\n  static void access$300(org.apache.hadoop.http.HttpServer2);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueProtocolService$Interface.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/http/lib/StaticUserWebFilter$StaticUserFilter.class": "Compiled from \"StaticUserWebFilter.java\"\npublic class org.apache.hadoop.http.lib.StaticUserWebFilter extends org.apache.hadoop.http.FilterInitializer {\n  static final java.lang.String DEPRECATED_UGI_KEY;\n  public org.apache.hadoop.http.lib.StaticUserWebFilter();\n  public void initFilter(org.apache.hadoop.http.FilterContainer, org.apache.hadoop.conf.Configuration);\n  static java.lang.String getUsernameFromConf(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/record/RecordComparator.class": "Compiled from \"RecordComparator.java\"\npublic abstract class org.apache.hadoop.record.RecordComparator extends org.apache.hadoop.io.WritableComparator {\n  protected org.apache.hadoop.record.RecordComparator(java.lang.Class<? extends org.apache.hadoop.io.WritableComparable>);\n  public abstract int compare(byte[], int, int, byte[], int, int);\n  public static synchronized void define(java.lang.Class, org.apache.hadoop.record.RecordComparator);\n}\n", 
  "org/apache/hadoop/ipc/FairCallQueue.class": "Compiled from \"FairCallQueue.java\"\npublic class org.apache.hadoop.ipc.FairCallQueue<E extends org.apache.hadoop.ipc.Schedulable> extends java.util.AbstractQueue<E> implements java.util.concurrent.BlockingQueue<E> {\n  public static final int IPC_CALLQUEUE_PRIORITY_LEVELS_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_PRIORITY_LEVELS_KEY;\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.FairCallQueue(int, java.lang.String, org.apache.hadoop.conf.Configuration);\n  public void put(E) throws java.lang.InterruptedException;\n  public boolean offer(E, long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public boolean offer(E);\n  public E take() throws java.lang.InterruptedException;\n  public E poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public E poll();\n  public E peek();\n  public int size();\n  public java.util.Iterator<E> iterator();\n  public int drainTo(java.util.Collection<? super E>, int);\n  public int drainTo(java.util.Collection<? super E>);\n  public int remainingCapacity();\n  public int[] getQueueSizes();\n  public long[] getOverflowedCalls();\n  public void setScheduler(org.apache.hadoop.ipc.RpcScheduler);\n  public void setMultiplexer(org.apache.hadoop.ipc.RpcMultiplexer);\n  public java.lang.Object peek();\n  public java.lang.Object poll();\n  public boolean offer(java.lang.Object);\n  public java.lang.Object poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public java.lang.Object take() throws java.lang.InterruptedException;\n  public boolean offer(java.lang.Object, long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public void put(java.lang.Object) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/ChRootedFs.class": "Compiled from \"ChRootedFs.java\"\nclass org.apache.hadoop.fs.viewfs.ChRootedFs extends org.apache.hadoop.fs.AbstractFileSystem {\n  protected org.apache.hadoop.fs.AbstractFileSystem getMyFs();\n  protected org.apache.hadoop.fs.Path fullPath(org.apache.hadoop.fs.Path);\n  public boolean isValidName(java.lang.String);\n  public org.apache.hadoop.fs.viewfs.ChRootedFs(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.fs.Path) throws java.net.URISyntaxException;\n  public java.net.URI getUri();\n  public java.lang.String stripOutRoot(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.Path getResolvedQualifiedPath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException;\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void setVerifyChecksum(boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/VersionedWritable.class": "Compiled from \"VersionedWritable.java\"\npublic abstract class org.apache.hadoop.io.VersionedWritable implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.io.VersionedWritable();\n  public abstract byte getVersion();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.class": "Compiled from \"DelegationTokenIdentifier.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier {\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier(org.apache.hadoop.io.Text, org.apache.hadoop.io.Text, org.apache.hadoop.io.Text, org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text getKind();\n}\n", 
  "org/apache/hadoop/io/DataOutputBuffer$Buffer.class": "Compiled from \"DataOutputBuffer.java\"\npublic class org.apache.hadoop.io.DataOutputBuffer extends java.io.DataOutputStream {\n  public org.apache.hadoop.io.DataOutputBuffer();\n  public org.apache.hadoop.io.DataOutputBuffer(int);\n  public byte[] getData();\n  public int getLength();\n  public org.apache.hadoop.io.DataOutputBuffer reset();\n  public void write(java.io.DataInput, int) throws java.io.IOException;\n  public void writeTo(java.io.OutputStream) throws java.io.IOException;\n  public void writeInt(int, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/crypto/CryptoOutputStream.class": "Compiled from \"CryptoOutputStream.java\"\npublic class org.apache.hadoop.crypto.CryptoOutputStream extends java.io.FilterOutputStream implements org.apache.hadoop.fs.Syncable,org.apache.hadoop.fs.CanSetDropBehind {\n  public org.apache.hadoop.crypto.CryptoOutputStream(java.io.OutputStream, org.apache.hadoop.crypto.CryptoCodec, int, byte[], byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.CryptoOutputStream(java.io.OutputStream, org.apache.hadoop.crypto.CryptoCodec, int, byte[], byte[], long) throws java.io.IOException;\n  public org.apache.hadoop.crypto.CryptoOutputStream(java.io.OutputStream, org.apache.hadoop.crypto.CryptoCodec, byte[], byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.CryptoOutputStream(java.io.OutputStream, org.apache.hadoop.crypto.CryptoCodec, byte[], byte[], long) throws java.io.IOException;\n  public java.io.OutputStream getWrappedStream();\n  public synchronized void write(byte[], int, int) throws java.io.IOException;\n  public synchronized void close() throws java.io.IOException;\n  public synchronized void flush() throws java.io.IOException;\n  public void write(int) throws java.io.IOException;\n  public void setDropBehind(java.lang.Boolean) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public void sync() throws java.io.IOException;\n  public void hflush() throws java.io.IOException;\n  public void hsync() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/record/meta/FieldTypeInfo.class": "Compiled from \"FieldTypeInfo.java\"\npublic class org.apache.hadoop.record.meta.FieldTypeInfo {\n  org.apache.hadoop.record.meta.FieldTypeInfo(java.lang.String, org.apache.hadoop.record.meta.TypeID);\n  public org.apache.hadoop.record.meta.TypeID getTypeID();\n  public java.lang.String getFieldID();\n  void write(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public boolean equals(org.apache.hadoop.record.meta.FieldTypeInfo);\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns$CacheWith2Keys$1.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcRequestHeaderProto.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.security.token.package-info {\n}\n", 
  "org/apache/hadoop/record/compiler/generated/ParseException.class": "Compiled from \"ParseException.java\"\npublic class org.apache.hadoop.record.compiler.generated.ParseException extends java.lang.Exception {\n  protected boolean specialConstructor;\n  public org.apache.hadoop.record.compiler.generated.Token currentToken;\n  public int[][] expectedTokenSequences;\n  public java.lang.String[] tokenImage;\n  protected java.lang.String eol;\n  public org.apache.hadoop.record.compiler.generated.ParseException(org.apache.hadoop.record.compiler.generated.Token, int[][], java.lang.String[]);\n  public org.apache.hadoop.record.compiler.generated.ParseException();\n  public org.apache.hadoop.record.compiler.generated.ParseException(java.lang.String);\n  public java.lang.String getMessage();\n  protected java.lang.String add_escapes(java.lang.String);\n}\n", 
  "org/apache/hadoop/io/file/tfile/Chunk.class": "Compiled from \"Chunk.java\"\nfinal class org.apache.hadoop.io.file.tfile.Chunk {\n}\n", 
  "org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.class": "Compiled from \"DefaultMetricsFactory.java\"\npublic final class org.apache.hadoop.metrics2.lib.DefaultMetricsFactory extends java.lang.Enum<org.apache.hadoop.metrics2.lib.DefaultMetricsFactory> {\n  public static final org.apache.hadoop.metrics2.lib.DefaultMetricsFactory INSTANCE;\n  public static org.apache.hadoop.metrics2.lib.DefaultMetricsFactory[] values();\n  public static org.apache.hadoop.metrics2.lib.DefaultMetricsFactory valueOf(java.lang.String);\n  public static org.apache.hadoop.metrics2.lib.MutableMetricsFactory getAnnotatedMetricsFactory();\n  public synchronized <T extends java/lang/Object> T getInstance(java.lang.Class<T>);\n  public synchronized void setInstance(org.apache.hadoop.metrics2.lib.MutableMetricsFactory);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$ZKFCProtocolService.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsConfig.class": "Compiled from \"MetricsConfig.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsConfig extends org.apache.commons.configuration.SubsetConfiguration {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String DEFAULT_FILE_NAME;\n  static final java.lang.String PREFIX_DEFAULT;\n  static final java.lang.String PERIOD_KEY;\n  static final int PERIOD_DEFAULT;\n  static final java.lang.String QUEUE_CAPACITY_KEY;\n  static final int QUEUE_CAPACITY_DEFAULT;\n  static final java.lang.String RETRY_DELAY_KEY;\n  static final int RETRY_DELAY_DEFAULT;\n  static final java.lang.String RETRY_BACKOFF_KEY;\n  static final int RETRY_BACKOFF_DEFAULT;\n  static final java.lang.String RETRY_COUNT_KEY;\n  static final int RETRY_COUNT_DEFAULT;\n  static final java.lang.String JMX_CACHE_TTL_KEY;\n  static final java.lang.String START_MBEANS_KEY;\n  static final java.lang.String PLUGIN_URLS_KEY;\n  static final java.lang.String CONTEXT_KEY;\n  static final java.lang.String NAME_KEY;\n  static final java.lang.String DESC_KEY;\n  static final java.lang.String SOURCE_KEY;\n  static final java.lang.String SINK_KEY;\n  static final java.lang.String METRIC_FILTER_KEY;\n  static final java.lang.String RECORD_FILTER_KEY;\n  static final java.lang.String SOURCE_FILTER_KEY;\n  static final java.util.regex.Pattern INSTANCE_REGEX;\n  static final com.google.common.base.Splitter SPLITTER;\n  org.apache.hadoop.metrics2.impl.MetricsConfig(org.apache.commons.configuration.Configuration, java.lang.String);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig create(java.lang.String);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig create(java.lang.String, java.lang.String...);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig loadFirst(java.lang.String, java.lang.String...);\n  public org.apache.hadoop.metrics2.impl.MetricsConfig subset(java.lang.String);\n  java.util.Map<java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig> getInstanceConfigs(java.lang.String);\n  java.lang.Iterable<java.lang.String> keys();\n  public java.lang.Object getProperty(java.lang.String);\n  <T extends org/apache/hadoop/metrics2/MetricsPlugin> T getPlugin(java.lang.String);\n  java.lang.String getClassName(java.lang.String);\n  java.lang.ClassLoader getPluginLoader();\n  public void clear();\n  org.apache.hadoop.metrics2.MetricsFilter getFilter(java.lang.String);\n  public java.lang.String toString();\n  static java.lang.String toString(org.apache.commons.configuration.Configuration);\n  public org.apache.commons.configuration.Configuration subset(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto$1.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/zlib/ZlibCompressor$CompressionHeader.class": "Compiled from \"ZlibCompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.ZlibCompressor implements org.apache.hadoop.io.compress.Compressor {\n  static boolean isNativeZlibLoaded();\n  protected final void construct(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader, int);\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor();\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader, int);\n  public void reinit(org.apache.hadoop.conf.Configuration);\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public void finish();\n  public boolean finished();\n  public int compress(byte[], int, int) throws java.io.IOException;\n  public long getBytesWritten();\n  public long getBytesRead();\n  public void reset();\n  public void end();\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/CryptoProtocolVersion.class": "Compiled from \"CryptoProtocolVersion.java\"\npublic final class org.apache.hadoop.crypto.CryptoProtocolVersion extends java.lang.Enum<org.apache.hadoop.crypto.CryptoProtocolVersion> {\n  public static final org.apache.hadoop.crypto.CryptoProtocolVersion UNKNOWN;\n  public static final org.apache.hadoop.crypto.CryptoProtocolVersion ENCRYPTION_ZONES;\n  public static org.apache.hadoop.crypto.CryptoProtocolVersion[] values();\n  public static org.apache.hadoop.crypto.CryptoProtocolVersion valueOf(java.lang.String);\n  public static org.apache.hadoop.crypto.CryptoProtocolVersion[] supported();\n  public static boolean supports(org.apache.hadoop.crypto.CryptoProtocolVersion);\n  public void setUnknownValue(int);\n  public int getUnknownValue();\n  public java.lang.String getDescription();\n  public int getVersion();\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto$1.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/ValueQueue$QueueRefiller.class": "Compiled from \"ValueQueue.java\"\npublic class org.apache.hadoop.crypto.key.kms.ValueQueue<E> {\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public void initializeQueuesForKeys(java.lang.String...) throws java.util.concurrent.ExecutionException;\n  public E getNext(java.lang.String) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void drain(java.lang.String);\n  public int getSize(java.lang.String) throws java.util.concurrent.ExecutionException;\n  public java.util.List<E> getAtMost(java.lang.String, int) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void shutdown();\n  static int access$200(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static float access$300(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller access$400(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$1.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/WritableFactory.class": "Compiled from \"WritableFactory.java\"\npublic interface org.apache.hadoop.io.WritableFactory {\n  public abstract org.apache.hadoop.io.Writable newInstance();\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup.class": "Compiled from \"NetworkTopologyWithNodeGroup.java\"\npublic class org.apache.hadoop.net.NetworkTopologyWithNodeGroup extends org.apache.hadoop.net.NetworkTopology {\n  public static final java.lang.String DEFAULT_NODEGROUP;\n  public org.apache.hadoop.net.NetworkTopologyWithNodeGroup();\n  protected org.apache.hadoop.net.Node getNodeForNetworkLocation(org.apache.hadoop.net.Node);\n  public java.lang.String getRack(java.lang.String);\n  public java.lang.String getNodeGroup(java.lang.String);\n  public boolean isOnSameRack(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isOnSameNodeGroup(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isNodeGroupAware();\n  public void add(org.apache.hadoop.net.Node);\n  public void remove(org.apache.hadoop.net.Node);\n  protected int getWeight(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public void sortByDistance(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node[], int);\n}\n", 
  "org/apache/hadoop/ipc/RpcClientUtil$ProtoSigCacheKey.class": "Compiled from \"RpcClientUtil.java\"\npublic class org.apache.hadoop.ipc.RpcClientUtil {\n  public org.apache.hadoop.ipc.RpcClientUtil();\n  public static boolean isMethodSupported(java.lang.Object, java.lang.Class<?>, org.apache.hadoop.ipc.RPC$RpcKind, long, java.lang.String) throws java.io.IOException;\n  public static java.lang.String methodToTraceString(java.lang.reflect.Method);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.class": "Compiled from \"MetricsTimeVaryingInt.java\"\npublic class org.apache.hadoop.metrics.util.MetricsTimeVaryingInt extends org.apache.hadoop.metrics.util.MetricsBase {\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingInt(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry, java.lang.String);\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingInt(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry);\n  public synchronized void inc(int);\n  public synchronized void inc();\n  public synchronized void pushMetric(org.apache.hadoop.metrics.MetricsRecord);\n  public synchronized int getPreviousIntervalValue();\n  public synchronized int getCurrentIntervalValue();\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/JavaSerialization$JavaSerializationSerializer$1.class": "Compiled from \"JavaSerialization.java\"\npublic class org.apache.hadoop.io.serializer.JavaSerialization implements org.apache.hadoop.io.serializer.Serialization<java.io.Serializable> {\n  public org.apache.hadoop.io.serializer.JavaSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.hadoop.io.serializer.Deserializer<java.io.Serializable> getDeserializer(java.lang.Class<java.io.Serializable>);\n  public org.apache.hadoop.io.serializer.Serializer<java.io.Serializable> getSerializer(java.lang.Class<java.io.Serializable>);\n}\n", 
  "org/apache/hadoop/fs/DU.class": "Compiled from \"DU.java\"\npublic class org.apache.hadoop.fs.DU extends org.apache.hadoop.util.Shell {\n  public org.apache.hadoop.fs.DU(java.io.File, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.DU(java.io.File, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.DU(java.io.File, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.DU(java.io.File, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public void decDfsUsed(long);\n  public void incDfsUsed(long);\n  public long getUsed() throws java.io.IOException;\n  public java.lang.String getDirPath();\n  protected void run() throws java.io.IOException;\n  public void start();\n  public void shutdown();\n  public java.lang.String toString();\n  protected java.lang.String[] getExecString();\n  protected void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$000(org.apache.hadoop.fs.DU);\n  static long access$100(org.apache.hadoop.fs.DU);\n  static java.io.IOException access$202(org.apache.hadoop.fs.DU, java.io.IOException);\n}\n", 
  "org/apache/hadoop/fs/FileContext$16.class": "", 
  "org/apache/hadoop/util/Options$BooleanOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager$1.class": "Compiled from \"AbstractDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.SecretManager<TokenIdent> {\n  protected final java.util.Map<TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation> currentTokens;\n  protected int delegationTokenSequenceNumber;\n  protected final java.util.Map<java.lang.Integer, org.apache.hadoop.security.token.delegation.DelegationKey> allKeys;\n  protected int currentId;\n  protected boolean storeTokenTrackingId;\n  protected volatile boolean running;\n  protected java.lang.Object noInterruptsLock;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager(long, long, long, long);\n  public void startThreads() throws java.io.IOException;\n  public synchronized void reset();\n  public synchronized void addKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  public synchronized org.apache.hadoop.security.token.delegation.DelegationKey[] getAllKeys();\n  protected void logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void logExpireToken(TokenIdent) throws java.io.IOException;\n  protected void storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeNewToken(TokenIdent, long) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  protected void updateStoredToken(TokenIdent, long) throws java.io.IOException;\n  protected synchronized int getCurrentKeyId();\n  protected synchronized int incrementCurrentKeyId();\n  protected synchronized void setCurrentKeyId(int);\n  protected synchronized int getDelegationTokenSeqNum();\n  protected synchronized int incrementDelegationTokenSeqNum();\n  protected synchronized void setDelegationTokenSeqNum(int);\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  public synchronized void addPersistedDelegationToken(TokenIdent, long) throws java.io.IOException;\n  void rollMasterKey() throws java.io.IOException;\n  protected synchronized byte[] createPassword(TokenIdent);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation checkToken(TokenIdent) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public synchronized byte[] retrievePassword(TokenIdent) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  protected java.lang.String getTrackingIdIfEnabled(TokenIdent);\n  public synchronized java.lang.String getTokenTrackingId(TokenIdent);\n  public synchronized void verifyToken(TokenIdent, byte[]) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public synchronized long renewToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  public static javax.crypto.SecretKey createSecretKey(byte[]);\n  public void stopThreads();\n  public synchronized boolean isRunning();\n  public TokenIdent decodeTokenIdentifier(org.apache.hadoop.security.token.Token<TokenIdent>) throws java.io.IOException;\n  public byte[] retrievePassword(org.apache.hadoop.security.token.TokenIdentifier) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  protected byte[] createPassword(org.apache.hadoop.security.token.TokenIdentifier);\n  static long access$100(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  static org.apache.commons.logging.Log access$200();\n  static long access$300(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  static void access$400(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Options$1.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$13.class": "", 
  "org/apache/hadoop/io/DefaultStringifier.class": "Compiled from \"DefaultStringifier.java\"\npublic class org.apache.hadoop.io.DefaultStringifier<T> implements org.apache.hadoop.io.Stringifier<T> {\n  public org.apache.hadoop.io.DefaultStringifier(org.apache.hadoop.conf.Configuration, java.lang.Class<T>);\n  public T fromString(java.lang.String) throws java.io.IOException;\n  public java.lang.String toString(T) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public static <K extends java/lang/Object> void store(org.apache.hadoop.conf.Configuration, K, java.lang.String) throws java.io.IOException;\n  public static <K extends java/lang/Object> K load(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.Class<K>) throws java.io.IOException;\n  public static <K extends java/lang/Object> void storeArray(org.apache.hadoop.conf.Configuration, K[], java.lang.String) throws java.io.IOException;\n  public static <K extends java/lang/Object> K[] loadArray(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.Class<K>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/ZKUtil.class": "Compiled from \"ZKUtil.java\"\npublic class org.apache.hadoop.util.ZKUtil {\n  public org.apache.hadoop.util.ZKUtil();\n  public static int removeSpecificPerms(int, int);\n  public static java.util.List<org.apache.zookeeper.data.ACL> parseACLs(java.lang.String) throws org.apache.hadoop.util.ZKUtil$BadAclFormatException;\n  public static java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo> parseAuth(java.lang.String) throws org.apache.hadoop.util.ZKUtil$BadAuthFormatException;\n  public static java.lang.String resolveConfIndirection(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider.class": "Compiled from \"LocalJavaKeyStoreProvider.java\"\npublic final class org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider extends org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider {\n  public static final java.lang.String SCHEME_NAME;\n  protected java.lang.String getSchemeName();\n  protected java.io.OutputStream getOutputStreamForKeystore() throws java.io.IOException;\n  protected boolean keystoreExists() throws java.io.IOException;\n  protected java.io.InputStream getInputStreamForFile() throws java.io.IOException;\n  protected void createPermissions(java.lang.String) throws java.io.IOException;\n  protected void stashOriginalFilePermissions() throws java.io.IOException;\n  protected void initFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/metrics/util/MBeanUtil.class": "Compiled from \"MBeanUtil.java\"\npublic class org.apache.hadoop.metrics.util.MBeanUtil {\n  public org.apache.hadoop.metrics.util.MBeanUtil();\n  public static javax.management.ObjectName registerMBean(java.lang.String, java.lang.String, java.lang.Object);\n  public static void unregisterMBean(javax.management.ObjectName);\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.class": "Compiled from \"MetricsTimeVaryingRate.java\"\npublic class org.apache.hadoop.metrics.util.MetricsTimeVaryingRate extends org.apache.hadoop.metrics.util.MetricsBase {\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingRate(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry, java.lang.String);\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingRate(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry);\n  public synchronized void inc(int, long);\n  public synchronized void inc(long);\n  public synchronized void pushMetric(org.apache.hadoop.metrics.MetricsRecord);\n  public synchronized int getPreviousIntervalNumOps();\n  public synchronized long getPreviousIntervalAverageTime();\n  public synchronized long getMinTime();\n  public synchronized long getMaxTime();\n  public synchronized void resetMinMax();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/Constants.class": "Compiled from \"Constants.java\"\npublic interface org.apache.hadoop.fs.viewfs.Constants {\n  public static final java.lang.String CONFIG_VIEWFS_PREFIX;\n  public static final java.lang.String CONFIG_VIEWFS_HOMEDIR;\n  public static final java.lang.String CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE;\n  public static final java.lang.String CONFIG_VIEWFS_PREFIX_DEFAULT_MOUNT_TABLE;\n  public static final java.lang.String CONFIG_VIEWFS_LINK;\n  public static final java.lang.String CONFIG_VIEWFS_LINK_MERGE;\n  public static final java.lang.String CONFIG_VIEWFS_LINK_MERGE_SLASH;\n  public static final org.apache.hadoop.fs.permission.FsPermission PERMISSION_555;\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$GetDelegationTokenResponseProtoOrBuilder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.class": "Compiled from \"MetricsBufferBuilder.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsBufferBuilder extends java.util.ArrayList<org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry> {\n  org.apache.hadoop.metrics2.impl.MetricsBufferBuilder();\n  boolean add(java.lang.String, java.lang.Iterable<org.apache.hadoop.metrics2.impl.MetricsRecordImpl>);\n  org.apache.hadoop.metrics2.impl.MetricsBuffer get();\n}\n", 
  "org/apache/hadoop/tracing/SpanReceiverInfoBuilder.class": "Compiled from \"SpanReceiverInfoBuilder.java\"\npublic class org.apache.hadoop.tracing.SpanReceiverInfoBuilder {\n  public org.apache.hadoop.tracing.SpanReceiverInfoBuilder(java.lang.String);\n  public void addConfigurationPair(java.lang.String, java.lang.String);\n  public org.apache.hadoop.tracing.SpanReceiverInfo build();\n}\n", 
  "org/apache/hadoop/fs/PathPermissionException.class": "Compiled from \"PathPermissionException.java\"\npublic class org.apache.hadoop.fs.PathPermissionException extends org.apache.hadoop.fs.PathIOException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.PathPermissionException(java.lang.String);\n}\n", 
  "org/apache/hadoop/ha/SshFenceByTcpPort$Args.class": "Compiled from \"SshFenceByTcpPort.java\"\npublic class org.apache.hadoop.ha.SshFenceByTcpPort extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.ha.FenceMethod {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String CONF_CONNECT_TIMEOUT_KEY;\n  static final java.lang.String CONF_IDENTITIES_KEY;\n  public org.apache.hadoop.ha.SshFenceByTcpPort();\n  public void checkArgs(java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public boolean tryFence(org.apache.hadoop.ha.HAServiceTarget, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/PathData.class": "Compiled from \"PathData.java\"\npublic class org.apache.hadoop.fs.shell.PathData implements java.lang.Comparable<org.apache.hadoop.fs.shell.PathData> {\n  protected final java.net.URI uri;\n  public final org.apache.hadoop.fs.FileSystem fs;\n  public final org.apache.hadoop.fs.Path path;\n  public org.apache.hadoop.fs.FileStatus stat;\n  public boolean exists;\n  public org.apache.hadoop.fs.shell.PathData(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus refreshStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData suffix(java.lang.String) throws java.io.IOException;\n  public boolean parentExists() throws java.io.IOException;\n  public boolean representsDirectory();\n  public org.apache.hadoop.fs.shell.PathData[] getDirectoryContents() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData getPathDataForChild(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  public static org.apache.hadoop.fs.shell.PathData[] expandAsGlob(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String toString();\n  public java.io.File toFile();\n  public int compareTo(org.apache.hadoop.fs.shell.PathData);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/zlib/ZlibCompressor$CompressionLevel.class": "Compiled from \"ZlibCompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.ZlibCompressor implements org.apache.hadoop.io.compress.Compressor {\n  static boolean isNativeZlibLoaded();\n  protected final void construct(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader, int);\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor();\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader, int);\n  public void reinit(org.apache.hadoop.conf.Configuration);\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public void finish();\n  public boolean finished();\n  public int compress(byte[], int, int) throws java.io.IOException;\n  public long getBytesWritten();\n  public long getBytesRead();\n  public void reset();\n  public void end();\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Writer$BlockAppender.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$MetaIndex.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/util/RunJar.class": "Compiled from \"RunJar.java\"\npublic class org.apache.hadoop.util.RunJar {\n  public static final java.util.regex.Pattern MATCH_ANY;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  public static final java.lang.String HADOOP_USE_CLIENT_CLASSLOADER;\n  public static final java.lang.String HADOOP_CLASSPATH;\n  public static final java.lang.String HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES;\n  public org.apache.hadoop.util.RunJar();\n  public static void unJar(java.io.File, java.io.File) throws java.io.IOException;\n  public static void unJar(java.io.File, java.io.File, java.util.regex.Pattern) throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Throwable;\n  public void run(java.lang.String[]) throws java.lang.Throwable;\n  boolean useClientClassLoader();\n  java.lang.String getHadoopClasspath();\n  java.lang.String getSystemClasses();\n  static {};\n}\n", 
  "org/apache/hadoop/util/Progress.class": "Compiled from \"Progress.java\"\npublic class org.apache.hadoop.util.Progress {\n  public org.apache.hadoop.util.Progress();\n  public org.apache.hadoop.util.Progress addPhase(java.lang.String);\n  public synchronized org.apache.hadoop.util.Progress addPhase();\n  public org.apache.hadoop.util.Progress addPhase(java.lang.String, float);\n  public synchronized org.apache.hadoop.util.Progress addPhase(float);\n  public synchronized void addPhases(int);\n  float getProgressWeightage(int);\n  synchronized org.apache.hadoop.util.Progress getParent();\n  synchronized void setParent(org.apache.hadoop.util.Progress);\n  public synchronized void startNextPhase();\n  public synchronized org.apache.hadoop.util.Progress phase();\n  public void complete();\n  public synchronized void set(float);\n  public synchronized float get();\n  public synchronized float getProgress();\n  public synchronized void setStatus(java.lang.String);\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/CompressionCodecFactory.class": "Compiled from \"CompressionCodecFactory.java\"\npublic class org.apache.hadoop.io.compress.CompressionCodecFactory {\n  public static final org.apache.commons.logging.Log LOG;\n  public java.lang.String toString();\n  public static java.util.List<java.lang.Class<? extends org.apache.hadoop.io.compress.CompressionCodec>> getCodecClasses(org.apache.hadoop.conf.Configuration);\n  public static void setCodecClasses(org.apache.hadoop.conf.Configuration, java.util.List<java.lang.Class>);\n  public org.apache.hadoop.io.compress.CompressionCodecFactory(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.io.compress.CompressionCodec getCodec(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.io.compress.CompressionCodec getCodecByClassName(java.lang.String);\n  public org.apache.hadoop.io.compress.CompressionCodec getCodecByName(java.lang.String);\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.CompressionCodec> getCodecClassByName(java.lang.String);\n  public static java.lang.String removeSuffix(java.lang.String, java.lang.String);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/CppGenerator.class": "Compiled from \"CppGenerator.java\"\nclass org.apache.hadoop.record.compiler.CppGenerator extends org.apache.hadoop.record.compiler.CodeGenerator {\n  org.apache.hadoop.record.compiler.CppGenerator();\n  void genCode(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JFile>, java.util.ArrayList<org.apache.hadoop.record.compiler.JRecord>, java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/DataChecksum.class": "Compiled from \"DataChecksum.java\"\npublic class org.apache.hadoop.util.DataChecksum implements java.util.zip.Checksum {\n  public static final int CHECKSUM_NULL;\n  public static final int CHECKSUM_CRC32;\n  public static final int CHECKSUM_CRC32C;\n  public static final int CHECKSUM_DEFAULT;\n  public static final int CHECKSUM_MIXED;\n  public static final int SIZE_OF_INTEGER;\n  public static java.util.zip.Checksum newCrc32();\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(org.apache.hadoop.util.DataChecksum$Type, int);\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(byte[], int);\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(java.io.DataInputStream) throws java.io.IOException;\n  public void writeHeader(java.io.DataOutputStream) throws java.io.IOException;\n  public byte[] getHeader();\n  public int writeValue(java.io.DataOutputStream, boolean) throws java.io.IOException;\n  public int writeValue(byte[], int, boolean) throws java.io.IOException;\n  public boolean compare(byte[], int);\n  public org.apache.hadoop.util.DataChecksum$Type getChecksumType();\n  public int getChecksumSize();\n  public int getChecksumSize(int);\n  public int getBytesPerChecksum();\n  public int getNumBytesInSum();\n  public static int getChecksumHeaderSize();\n  public long getValue();\n  public void reset();\n  public void update(byte[], int, int);\n  public void update(int);\n  public void verifyChunkedSums(java.nio.ByteBuffer, java.nio.ByteBuffer, java.lang.String, long) throws org.apache.hadoop.fs.ChecksumException;\n  public void calculateChunkedSums(java.nio.ByteBuffer, java.nio.ByteBuffer);\n  public void calculateChunkedSums(byte[], int, int, byte[], int);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.class": "Compiled from \"AbstractGangliaSink.java\"\npublic abstract class org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink implements org.apache.hadoop.metrics2.MetricsSink {\n  public final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String DEFAULT_UNITS;\n  public static final int DEFAULT_TMAX;\n  public static final int DEFAULT_DMAX;\n  public static final org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope DEFAULT_SLOPE;\n  public static final int DEFAULT_PORT;\n  public static final boolean DEFAULT_MULTICAST_ENABLED;\n  public static final int DEFAULT_MULTICAST_TTL;\n  public static final java.lang.String SERVERS_PROPERTY;\n  public static final java.lang.String MULTICAST_ENABLED_PROPERTY;\n  public static final java.lang.String MULTICAST_TTL_PROPERTY;\n  public static final int BUFFER_SIZE;\n  public static final java.lang.String SUPPORT_SPARSE_METRICS_PROPERTY;\n  public static final boolean SUPPORT_SPARSE_METRICS_DEFAULT;\n  public static final java.lang.String EQUAL;\n  protected final org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor gangliaMetricVisitor;\n  public org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  public void flush();\n  protected org.apache.hadoop.metrics2.sink.ganglia.GangliaConf getGangliaConfForMetric(java.lang.String);\n  protected java.lang.String getHostName();\n  protected void xdr_string(java.lang.String);\n  protected void xdr_int(int);\n  protected void emitToGangliaHosts() throws java.io.IOException;\n  void resetBuffer();\n  protected boolean isSupportSparseMetrics();\n  void setDatagramSocket(java.net.DatagramSocket);\n  java.net.DatagramSocket getDatagramSocket();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/util/SampleStat$MinMax.class": "Compiled from \"SampleStat.java\"\npublic class org.apache.hadoop.metrics2.util.SampleStat {\n  public org.apache.hadoop.metrics2.util.SampleStat();\n  public void reset();\n  void reset(long, double, double, double, double, org.apache.hadoop.metrics2.util.SampleStat$MinMax);\n  public void copyTo(org.apache.hadoop.metrics2.util.SampleStat);\n  public org.apache.hadoop.metrics2.util.SampleStat add(double);\n  public org.apache.hadoop.metrics2.util.SampleStat add(long, double);\n  public long numSamples();\n  public double mean();\n  public double variance();\n  public double stddev();\n  public double min();\n  public double max();\n}\n", 
  "org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager$2.class": "Compiled from \"ZKDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent> {\n  public static final java.lang.String ZK_DTSM_ZK_NUM_RETRIES;\n  public static final java.lang.String ZK_DTSM_ZK_SESSION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_SHUTDOWN_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH;\n  public static final java.lang.String ZK_DTSM_ZK_AUTH_TYPE;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_STRING;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_KEYTAB;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_PRINCIPAL;\n  public static final int ZK_DTSM_ZK_NUM_RETRIES_DEFAULT;\n  public static final int ZK_DTSM_ZK_SESSION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_CONNECTION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_SHUTDOWN_TIMEOUT_DEFAULT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH_DEAFULT;\n  public static void setCurator(org.apache.curator.framework.CuratorFramework);\n  public org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager(org.apache.hadoop.conf.Configuration);\n  public void startThreads() throws java.io.IOException;\n  public void stopThreads();\n  protected int getDelegationTokenSeqNum();\n  protected int incrementDelegationTokenSeqNum();\n  protected void setDelegationTokenSeqNum(int);\n  protected int getCurrentKeyId();\n  protected int incrementCurrentKeyId();\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  static java.lang.String getNodePath(java.lang.String, java.lang.String);\n  public java.util.concurrent.ExecutorService getListenerThreadPool();\n  static void access$100(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, byte[]) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, java.lang.String);\n  static void access$300(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static void access$400(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/ValueQueue$NamedRunnable.class": "Compiled from \"ValueQueue.java\"\npublic class org.apache.hadoop.crypto.key.kms.ValueQueue<E> {\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public void initializeQueuesForKeys(java.lang.String...) throws java.util.concurrent.ExecutionException;\n  public E getNext(java.lang.String) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void drain(java.lang.String);\n  public int getSize(java.lang.String) throws java.util.concurrent.ExecutionException;\n  public java.util.List<E> getAtMost(java.lang.String, int) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void shutdown();\n  static int access$200(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static float access$300(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller access$400(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HarFileSystem$HarStatus.class": "Compiled from \"HarFileSystem.java\"\npublic class org.apache.hadoop.fs.HarFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final java.lang.String METADATA_CACHE_ENTRIES_KEY;\n  public static final int METADATA_CACHE_ENTRIES_DEFAULT;\n  public static final int VERSION;\n  public org.apache.hadoop.fs.HarFileSystem();\n  public java.lang.String getScheme();\n  public org.apache.hadoop.fs.HarFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public int getHarVersion() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  public java.net.URI getUri();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  static org.apache.hadoop.fs.BlockLocation[] fixBlockLocations(org.apache.hadoop.fs.BlockLocation[], long, long, long);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public static int getHarHash(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long);\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  org.apache.hadoop.fs.HarFileSystem$HarMetaData getMetadata();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  static java.lang.String access$200(org.apache.hadoop.fs.HarFileSystem, java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.hadoop.fs.HarFileSystem$HarMetaData access$300(org.apache.hadoop.fs.HarFileSystem);\n  static java.lang.String access$400(java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.commons.logging.Log access$500();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/DU$DURefreshThread.class": "Compiled from \"DU.java\"\npublic class org.apache.hadoop.fs.DU extends org.apache.hadoop.util.Shell {\n  public org.apache.hadoop.fs.DU(java.io.File, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.DU(java.io.File, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.DU(java.io.File, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.DU(java.io.File, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public void decDfsUsed(long);\n  public void incDfsUsed(long);\n  public long getUsed() throws java.io.IOException;\n  public java.lang.String getDirPath();\n  protected void run() throws java.io.IOException;\n  public void start();\n  public void shutdown();\n  public java.lang.String toString();\n  protected java.lang.String[] getExecString();\n  protected void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$000(org.apache.hadoop.fs.DU);\n  static long access$100(org.apache.hadoop.fs.DU);\n  static java.io.IOException access$202(org.apache.hadoop.fs.DU, java.io.IOException);\n}\n", 
  "org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.class": "Compiled from \"BuiltInGzipDecompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor implements org.apache.hadoop.io.compress.Decompressor {\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor();\n  public synchronized boolean needsInput();\n  public synchronized void setInput(byte[], int, int);\n  public synchronized int decompress(byte[], int, int) throws java.io.IOException;\n  public synchronized long getBytesRead();\n  public synchronized int getRemaining();\n  public synchronized boolean needsDictionary();\n  public synchronized void setDictionary(byte[], int, int);\n  public synchronized boolean finished();\n  public synchronized void reset();\n  public synchronized void end();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$Util$2.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ftp/FTPException.class": "Compiled from \"FTPException.java\"\npublic class org.apache.hadoop.fs.ftp.FTPException extends java.lang.RuntimeException {\n  public org.apache.hadoop.fs.ftp.FTPException(java.lang.String);\n  public org.apache.hadoop.fs.ftp.FTPException(java.lang.Throwable);\n  public org.apache.hadoop.fs.ftp.FTPException(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/fs/ChecksumFileSystem.class": "Compiled from \"ChecksumFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.ChecksumFileSystem extends org.apache.hadoop.fs.FilterFileSystem {\n  public static double getApproxChkSumLength(long);\n  public org.apache.hadoop.fs.ChecksumFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem getRawFileSystem();\n  public org.apache.hadoop.fs.Path getChecksumFile(org.apache.hadoop.fs.Path);\n  public static boolean isChecksumFile(org.apache.hadoop.fs.Path);\n  public long getChecksumFileLength(org.apache.hadoop.fs.Path, long);\n  public int getBytesPerSum();\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public static long getChecksumLength(long, int);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  static int access$000(org.apache.hadoop.fs.ChecksumFileSystem, int, int);\n  static byte[] access$100();\n  static boolean access$200(org.apache.hadoop.fs.ChecksumFileSystem);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Writer$DataBlockRegister.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JByte$CppByte.class": "Compiled from \"JByte.java\"\npublic class org.apache.hadoop.record.compiler.JByte extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JByte();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/util/CacheableIPList.class": "Compiled from \"CacheableIPList.java\"\npublic class org.apache.hadoop.util.CacheableIPList implements org.apache.hadoop.util.IPList {\n  public org.apache.hadoop.util.CacheableIPList(org.apache.hadoop.util.FileBasedIPList, long);\n  public void refresh();\n  public boolean isIn(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/shell/FsUsage.class": "Compiled from \"FsUsage.java\"\nclass org.apache.hadoop.fs.shell.FsUsage extends org.apache.hadoop.fs.shell.FsCommand {\n  protected boolean humanReadable;\n  protected org.apache.hadoop.fs.shell.FsUsage$TableBuilder usagesTable;\n  org.apache.hadoop.fs.shell.FsUsage();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected java.lang.String formatSize(long);\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$TryOnceThenFail.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider$KMSKeyVersion.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/http/lib/StaticUserWebFilter$StaticUserFilter$1.class": "Compiled from \"StaticUserWebFilter.java\"\npublic class org.apache.hadoop.http.lib.StaticUserWebFilter extends org.apache.hadoop.http.FilterInitializer {\n  static final java.lang.String DEPRECATED_UGI_KEY;\n  public org.apache.hadoop.http.lib.StaticUserWebFilter();\n  public void initFilter(org.apache.hadoop.http.FilterContainer, org.apache.hadoop.conf.Configuration);\n  static java.lang.String getUsernameFromConf(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$4.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Display$Cat.class": "Compiled from \"Display.java\"\nclass org.apache.hadoop.fs.shell.Display extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.Display();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ipc/DecayRpcScheduler.class": "Compiled from \"DecayRpcScheduler.java\"\npublic class org.apache.hadoop.ipc.DecayRpcScheduler implements org.apache.hadoop.ipc.RpcScheduler,org.apache.hadoop.ipc.DecayRpcSchedulerMXBean {\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_PERIOD_KEY;\n  public static final long IPC_CALLQUEUE_DECAYSCHEDULER_PERIOD_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_FACTOR_KEY;\n  public static final double IPC_CALLQUEUE_DECAYSCHEDULER_FACTOR_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_THRESHOLDS_KEY;\n  public static final java.lang.String DECAYSCHEDULER_UNKNOWN_IDENTITY;\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.DecayRpcScheduler(int, java.lang.String, org.apache.hadoop.conf.Configuration);\n  public int getPriorityLevel(org.apache.hadoop.ipc.Schedulable);\n  public double getDecayFactor();\n  public long getDecayPeriodMillis();\n  public double[] getThresholds();\n  public void forceDecay();\n  public java.util.Map<java.lang.Object, java.lang.Long> getCallCountSnapshot();\n  public long getTotalCallSnapshot();\n  public int getUniqueIdentityCount();\n  public long getTotalCallVolume();\n  public java.lang.String getSchedulingDecisionSummary();\n  public java.lang.String getCallVolumeSummary();\n  static void access$000(org.apache.hadoop.ipc.DecayRpcScheduler);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$RetryUpToMaximumCountWithFixedSleep.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsRecordFiltered.class": "Compiled from \"MetricsRecordFiltered.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsRecordFiltered extends org.apache.hadoop.metrics2.impl.AbstractMetricsRecord {\n  org.apache.hadoop.metrics2.impl.MetricsRecordFiltered(org.apache.hadoop.metrics2.MetricsRecord, org.apache.hadoop.metrics2.MetricsFilter);\n  public long timestamp();\n  public java.lang.String name();\n  public java.lang.String description();\n  public java.lang.String context();\n  public java.util.Collection<org.apache.hadoop.metrics2.MetricsTag> tags();\n  public java.lang.Iterable<org.apache.hadoop.metrics2.AbstractMetric> metrics();\n  static org.apache.hadoop.metrics2.MetricsRecord access$000(org.apache.hadoop.metrics2.impl.MetricsRecordFiltered);\n  static org.apache.hadoop.metrics2.MetricsFilter access$100(org.apache.hadoop.metrics2.impl.MetricsRecordFiltered);\n}\n", 
  "org/apache/hadoop/io/compress/BZip2Codec$BZip2CompressionInputStream$POS_ADVERTISEMENT_STATE_MACHINE.class": "Compiled from \"BZip2Codec.java\"\npublic class org.apache.hadoop.io.compress.BZip2Codec implements org.apache.hadoop.conf.Configurable,org.apache.hadoop.io.compress.SplittableCompressionCodec {\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public org.apache.hadoop.io.compress.BZip2Codec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.SplitCompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, long, long, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.String getDefaultExtension();\n  static int access$000();\n  static int access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetGroupsForUserRequestProtoOrBuilder.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$POSIX$NoMlockCacheManipulator.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics2.package-info {\n}\n", 
  "org/apache/hadoop/fs/shell/Test.class": "Compiled from \"Test.java\"\nclass org.apache.hadoop.fs.shell.Test extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  org.apache.hadoop.fs.shell.Test();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected void processOptions(java.util.LinkedList<java.lang.String>);\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processNonexistentPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.class": "Compiled from \"MetricsRecordBuilderImpl.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl extends org.apache.hadoop.metrics2.MetricsRecordBuilder {\n  org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl(org.apache.hadoop.metrics2.MetricsCollector, org.apache.hadoop.metrics2.MetricsInfo, org.apache.hadoop.metrics2.MetricsFilter, org.apache.hadoop.metrics2.MetricsFilter, boolean);\n  public org.apache.hadoop.metrics2.MetricsCollector parent();\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl add(org.apache.hadoop.metrics2.MetricsTag);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl add(org.apache.hadoop.metrics2.AbstractMetric);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl addCounter(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl addCounter(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl addGauge(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl addGauge(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl addGauge(org.apache.hadoop.metrics2.MetricsInfo, float);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl addGauge(org.apache.hadoop.metrics2.MetricsInfo, double);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl setContext(java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordImpl getRecord();\n  java.util.List<org.apache.hadoop.metrics2.MetricsTag> tags();\n  java.util.List<org.apache.hadoop.metrics2.AbstractMetric> metrics();\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder addGauge(org.apache.hadoop.metrics2.MetricsInfo, double);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder addGauge(org.apache.hadoop.metrics2.MetricsInfo, float);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder addGauge(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder addGauge(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder addCounter(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder addCounter(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder setContext(java.lang.String);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder add(org.apache.hadoop.metrics2.AbstractMetric);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder add(org.apache.hadoop.metrics2.MetricsTag);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n}\n", 
  "org/apache/hadoop/util/IdentityHashStore$Visitor.class": "Compiled from \"IdentityHashStore.java\"\npublic final class org.apache.hadoop.util.IdentityHashStore<K, V> {\n  public org.apache.hadoop.util.IdentityHashStore(int);\n  public void put(K, V);\n  public V get(K);\n  public V remove(K);\n  public boolean isEmpty();\n  public int numElements();\n  public int capacity();\n  public void visitAll(org.apache.hadoop.util.IdentityHashStore$Visitor<K, V>);\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Sorter$MergeQueue.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/UTF8$1.class": "Compiled from \"UTF8.java\"\npublic class org.apache.hadoop.io.UTF8 implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.UTF8> {\n  public org.apache.hadoop.io.UTF8();\n  public org.apache.hadoop.io.UTF8(java.lang.String);\n  public org.apache.hadoop.io.UTF8(org.apache.hadoop.io.UTF8);\n  public byte[] getBytes();\n  public int getLength();\n  public void set(java.lang.String);\n  public void set(org.apache.hadoop.io.UTF8);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static void skip(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public int compareTo(org.apache.hadoop.io.UTF8);\n  public java.lang.String toString();\n  public java.lang.String toStringChecked() throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static byte[] getBytes(java.lang.String);\n  public static java.lang.String fromBytes(byte[]) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/annotation/Metric.class": "Compiled from \"Metric.java\"\npublic interface org.apache.hadoop.metrics2.annotation.Metric extends java.lang.annotation.Annotation {\n  public abstract java.lang.String[] value();\n  public abstract java.lang.String about();\n  public abstract java.lang.String sampleName();\n  public abstract java.lang.String valueName();\n  public abstract boolean always();\n  public abstract org.apache.hadoop.metrics2.annotation.Metric$Type type();\n}\n", 
  "org/apache/hadoop/metrics2/util/Quantile.class": "Compiled from \"Quantile.java\"\npublic class org.apache.hadoop.metrics2.util.Quantile implements java.lang.Comparable<org.apache.hadoop.metrics2.util.Quantile> {\n  public final double quantile;\n  public final double error;\n  public org.apache.hadoop.metrics2.util.Quantile(double, double);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.metrics2.util.Quantile);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/fs/HasFileDescriptor.class": "Compiled from \"HasFileDescriptor.java\"\npublic interface org.apache.hadoop.fs.HasFileDescriptor {\n  public abstract java.io.FileDescriptor getFileDescriptor() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/ssl/SSLHostnameVerifier$2.class": "Compiled from \"SSLHostnameVerifier.java\"\npublic interface org.apache.hadoop.security.ssl.SSLHostnameVerifier extends javax.net.ssl.HostnameVerifier {\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT_AND_LOCALHOST;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT_IE6;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier ALLOW_ALL;\n  public abstract boolean verify(java.lang.String, javax.net.ssl.SSLSession);\n  public abstract void check(java.lang.String, javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String, java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String, java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String[], java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$2.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ChecksumFileSystem$ChecksumFSOutputSummer.class": "Compiled from \"ChecksumFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.ChecksumFileSystem extends org.apache.hadoop.fs.FilterFileSystem {\n  public static double getApproxChkSumLength(long);\n  public org.apache.hadoop.fs.ChecksumFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem getRawFileSystem();\n  public org.apache.hadoop.fs.Path getChecksumFile(org.apache.hadoop.fs.Path);\n  public static boolean isChecksumFile(org.apache.hadoop.fs.Path);\n  public long getChecksumFileLength(org.apache.hadoop.fs.Path, long);\n  public int getBytesPerSum();\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public static long getChecksumLength(long, int);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  static int access$000(org.apache.hadoop.fs.ChecksumFileSystem, int, int);\n  static byte[] access$100();\n  static boolean access$200(org.apache.hadoop.fs.ChecksumFileSystem);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProtoOrBuilder.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MetricsInfoImpl.class": "Compiled from \"MetricsInfoImpl.java\"\nclass org.apache.hadoop.metrics2.lib.MetricsInfoImpl implements org.apache.hadoop.metrics2.MetricsInfo {\n  org.apache.hadoop.metrics2.lib.MetricsInfoImpl(java.lang.String, java.lang.String);\n  public java.lang.String name();\n  public java.lang.String description();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/io/serializer/JavaSerialization$JavaSerializationSerializer.class": "Compiled from \"JavaSerialization.java\"\npublic class org.apache.hadoop.io.serializer.JavaSerialization implements org.apache.hadoop.io.serializer.Serialization<java.io.Serializable> {\n  public org.apache.hadoop.io.serializer.JavaSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.hadoop.io.serializer.Deserializer<java.io.Serializable> getDeserializer(java.lang.Class<java.io.Serializable>);\n  public org.apache.hadoop.io.serializer.Serializer<java.io.Serializable> getSerializer(java.lang.Class<java.io.Serializable>);\n}\n", 
  "org/apache/hadoop/fs/viewfs/InodeTree$INode.class": "Compiled from \"InodeTree.java\"\nabstract class org.apache.hadoop.fs.viewfs.InodeTree<T> {\n  static final org.apache.hadoop.fs.Path SlashPath;\n  final org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T> root;\n  final java.lang.String homedirPrefix;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> mountPoints;\n  static final boolean $assertionsDisabled;\n  static java.lang.String[] breakIntoPathComponents(java.lang.String);\n  protected abstract T getTargetFileSystem(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, java.io.IOException;\n  protected abstract T getTargetFileSystem(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T>) throws java.net.URISyntaxException;\n  protected abstract T getTargetFileSystem(java.net.URI[]) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException;\n  protected org.apache.hadoop.fs.viewfs.InodeTree(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.IOException;\n  org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult<T> resolve(java.lang.String, boolean) throws java.io.FileNotFoundException;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> getMountPoints();\n  java.lang.String getHomeDirPrefixValue();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$MonitorHealthResponseProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/Name$Iname.class": "Compiled from \"Name.java\"\nfinal class org.apache.hadoop.fs.shell.find.Name extends org.apache.hadoop.fs.shell.find.BaseExpression {\n  public static void registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Name();\n  public void addArguments(java.util.Deque<java.lang.String>);\n  public void prepare() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  org.apache.hadoop.fs.shell.find.Name(boolean, org.apache.hadoop.fs.shell.find.Name$1);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto$1.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.class": "Compiled from \"GenericRefreshProtocolClientSideTranslatorPB.java\"\npublic class org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB implements org.apache.hadoop.ipc.ProtocolMetaInterface,org.apache.hadoop.ipc.GenericRefreshProtocol,java.io.Closeable {\n  public org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB(org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB);\n  public void close() throws java.io.IOException;\n  public java.util.Collection<org.apache.hadoop.ipc.RefreshResponse> refresh(java.lang.String, java.lang.String[]) throws java.io.IOException;\n  public boolean isMethodSupported(java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$GetDelegationTokenResponseProto$1.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/filter/AbstractPatternFilter.class": "Compiled from \"AbstractPatternFilter.java\"\npublic abstract class org.apache.hadoop.metrics2.filter.AbstractPatternFilter extends org.apache.hadoop.metrics2.MetricsFilter {\n  protected static final java.lang.String INCLUDE_KEY;\n  protected static final java.lang.String EXCLUDE_KEY;\n  protected static final java.lang.String INCLUDE_TAGS_KEY;\n  protected static final java.lang.String EXCLUDE_TAGS_KEY;\n  org.apache.hadoop.metrics2.filter.AbstractPatternFilter();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  void setIncludePattern(java.util.regex.Pattern);\n  void setExcludePattern(java.util.regex.Pattern);\n  void setIncludeTagPattern(java.lang.String, java.util.regex.Pattern);\n  void setExcludeTagPattern(java.lang.String, java.util.regex.Pattern);\n  public boolean accepts(org.apache.hadoop.metrics2.MetricsTag);\n  public boolean accepts(java.lang.Iterable<org.apache.hadoop.metrics2.MetricsTag>);\n  public boolean accepts(java.lang.String);\n  protected abstract java.util.regex.Pattern compile(java.lang.String);\n}\n", 
  "org/apache/hadoop/security/ssl/ReloadingX509TrustManager.class": "Compiled from \"ReloadingX509TrustManager.java\"\npublic final class org.apache.hadoop.security.ssl.ReloadingX509TrustManager implements javax.net.ssl.X509TrustManager,java.lang.Runnable {\n  public org.apache.hadoop.security.ssl.ReloadingX509TrustManager(java.lang.String, java.lang.String, java.lang.String, long) throws java.io.IOException, java.security.GeneralSecurityException;\n  public void init();\n  public void destroy();\n  public long getReloadInterval();\n  public void checkClientTrusted(java.security.cert.X509Certificate[], java.lang.String) throws java.security.cert.CertificateException;\n  public void checkServerTrusted(java.security.cert.X509Certificate[], java.lang.String) throws java.security.cert.CertificateException;\n  public java.security.cert.X509Certificate[] getAcceptedIssuers();\n  boolean needsReload();\n  javax.net.ssl.X509TrustManager loadTrustManager() throws java.io.IOException, java.security.GeneralSecurityException;\n  public void run();\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslPlainServer.class": "Compiled from \"SaslPlainServer.java\"\npublic class org.apache.hadoop.security.SaslPlainServer implements javax.security.sasl.SaslServer {\n  org.apache.hadoop.security.SaslPlainServer(javax.security.auth.callback.CallbackHandler);\n  public java.lang.String getMechanismName();\n  public byte[] evaluateResponse(byte[]) throws javax.security.sasl.SaslException;\n  public boolean isComplete();\n  public java.lang.String getAuthorizationID();\n  public java.lang.Object getNegotiatedProperty(java.lang.String);\n  public byte[] wrap(byte[], int, int) throws javax.security.sasl.SaslException;\n  public byte[] unwrap(byte[], int, int) throws javax.security.sasl.SaslException;\n  public void dispose() throws javax.security.sasl.SaslException;\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$GetDelegationTokenRequestProto$1.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/CompareUtils$ScalarLong.class": "Compiled from \"CompareUtils.java\"\nclass org.apache.hadoop.io.file.tfile.CompareUtils {\n}\n", 
  "org/apache/hadoop/util/ServicePlugin.class": "Compiled from \"ServicePlugin.java\"\npublic interface org.apache.hadoop.util.ServicePlugin extends java.io.Closeable {\n  public abstract void start(java.lang.Object);\n  public abstract void stop();\n}\n", 
  "org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager$DelegationTokenInformation.class": "Compiled from \"AbstractDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.SecretManager<TokenIdent> {\n  protected final java.util.Map<TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation> currentTokens;\n  protected int delegationTokenSequenceNumber;\n  protected final java.util.Map<java.lang.Integer, org.apache.hadoop.security.token.delegation.DelegationKey> allKeys;\n  protected int currentId;\n  protected boolean storeTokenTrackingId;\n  protected volatile boolean running;\n  protected java.lang.Object noInterruptsLock;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager(long, long, long, long);\n  public void startThreads() throws java.io.IOException;\n  public synchronized void reset();\n  public synchronized void addKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  public synchronized org.apache.hadoop.security.token.delegation.DelegationKey[] getAllKeys();\n  protected void logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void logExpireToken(TokenIdent) throws java.io.IOException;\n  protected void storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeNewToken(TokenIdent, long) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  protected void updateStoredToken(TokenIdent, long) throws java.io.IOException;\n  protected synchronized int getCurrentKeyId();\n  protected synchronized int incrementCurrentKeyId();\n  protected synchronized void setCurrentKeyId(int);\n  protected synchronized int getDelegationTokenSeqNum();\n  protected synchronized int incrementDelegationTokenSeqNum();\n  protected synchronized void setDelegationTokenSeqNum(int);\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  public synchronized void addPersistedDelegationToken(TokenIdent, long) throws java.io.IOException;\n  void rollMasterKey() throws java.io.IOException;\n  protected synchronized byte[] createPassword(TokenIdent);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation checkToken(TokenIdent) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public synchronized byte[] retrievePassword(TokenIdent) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  protected java.lang.String getTrackingIdIfEnabled(TokenIdent);\n  public synchronized java.lang.String getTokenTrackingId(TokenIdent);\n  public synchronized void verifyToken(TokenIdent, byte[]) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public synchronized long renewToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  public static javax.crypto.SecretKey createSecretKey(byte[]);\n  public void stopThreads();\n  public synchronized boolean isRunning();\n  public TokenIdent decodeTokenIdentifier(org.apache.hadoop.security.token.Token<TokenIdent>) throws java.io.IOException;\n  public byte[] retrievePassword(org.apache.hadoop.security.token.TokenIdentifier) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  protected byte[] createPassword(org.apache.hadoop.security.token.TokenIdentifier);\n  static long access$100(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  static org.apache.commons.logging.Log access$200();\n  static long access$300(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  static void access$400(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.class": "Compiled from \"GangliaSink31.java\"\npublic class org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31 extends org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30 {\n  public final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31();\n  protected void emitMetric(java.lang.String, java.lang.String, java.lang.String, java.lang.String, org.apache.hadoop.metrics2.sink.ganglia.GangliaConf, org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/record/compiler/ant/RccTask.class": "Compiled from \"RccTask.java\"\npublic class org.apache.hadoop.record.compiler.ant.RccTask extends org.apache.tools.ant.Task {\n  public org.apache.hadoop.record.compiler.ant.RccTask();\n  public void setLanguage(java.lang.String);\n  public void setFile(java.io.File);\n  public void setFailonerror(boolean);\n  public void setDestdir(java.io.File);\n  public void addFileset(org.apache.tools.ant.types.FileSet);\n  public void execute() throws org.apache.tools.ant.BuildException;\n}\n", 
  "org/apache/hadoop/io/DataInputByteBuffer$Buffer.class": "Compiled from \"DataInputByteBuffer.java\"\npublic class org.apache.hadoop.io.DataInputByteBuffer extends java.io.DataInputStream {\n  public org.apache.hadoop.io.DataInputByteBuffer();\n  public void reset(java.nio.ByteBuffer...);\n  public java.nio.ByteBuffer[] getData();\n  public int getPosition();\n  public int getLength();\n}\n", 
  "org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager$3.class": "Compiled from \"ZKDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent> {\n  public static final java.lang.String ZK_DTSM_ZK_NUM_RETRIES;\n  public static final java.lang.String ZK_DTSM_ZK_SESSION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_SHUTDOWN_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH;\n  public static final java.lang.String ZK_DTSM_ZK_AUTH_TYPE;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_STRING;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_KEYTAB;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_PRINCIPAL;\n  public static final int ZK_DTSM_ZK_NUM_RETRIES_DEFAULT;\n  public static final int ZK_DTSM_ZK_SESSION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_CONNECTION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_SHUTDOWN_TIMEOUT_DEFAULT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH_DEAFULT;\n  public static void setCurator(org.apache.curator.framework.CuratorFramework);\n  public org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager(org.apache.hadoop.conf.Configuration);\n  public void startThreads() throws java.io.IOException;\n  public void stopThreads();\n  protected int getDelegationTokenSeqNum();\n  protected int incrementDelegationTokenSeqNum();\n  protected void setDelegationTokenSeqNum(int);\n  protected int getCurrentKeyId();\n  protected int incrementCurrentKeyId();\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  static java.lang.String getNodePath(java.lang.String, java.lang.String);\n  public java.util.concurrent.ExecutorService getListenerThreadPool();\n  static void access$100(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, byte[]) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, java.lang.String);\n  static void access$300(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static void access$400(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ChecksumException.class": "Compiled from \"ChecksumException.java\"\npublic class org.apache.hadoop.fs.ChecksumException extends java.io.IOException {\n  public org.apache.hadoop.fs.ChecksumException(java.lang.String, long);\n  public long getPos();\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$Reader$Scanner.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/zlib/ZlibDecompressor.class": "Compiled from \"ZlibDecompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.ZlibDecompressor implements org.apache.hadoop.io.compress.Decompressor {\n  static final boolean $assertionsDisabled;\n  static boolean isNativeZlibLoaded();\n  public org.apache.hadoop.io.compress.zlib.ZlibDecompressor(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader, int);\n  public org.apache.hadoop.io.compress.zlib.ZlibDecompressor();\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public boolean needsDictionary();\n  public boolean finished();\n  public int decompress(byte[], int, int) throws java.io.IOException;\n  public long getBytesWritten();\n  public long getBytesRead();\n  public int getRemaining();\n  public void reset();\n  public void end();\n  protected void finalize();\n  int inflateDirect(java.nio.ByteBuffer, java.nio.ByteBuffer) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ftp/FTPFileSystem.class": "Compiled from \"FTPFileSystem.java\"\npublic class org.apache.hadoop.fs.ftp.FTPFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int DEFAULT_BUFFER_SIZE;\n  public static final int DEFAULT_BLOCK_SIZE;\n  public static final java.lang.String FS_FTP_USER_PREFIX;\n  public static final java.lang.String FS_FTP_HOST;\n  public static final java.lang.String FS_FTP_HOST_PORT;\n  public static final java.lang.String FS_FTP_PASSWORD_PREFIX;\n  public static final java.lang.String E_SAME_DIRECTORY_ONLY;\n  public org.apache.hadoop.fs.ftp.FTPFileSystem();\n  public java.lang.String getScheme();\n  protected int getDefaultPort();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public java.net.URI getUri();\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  static void access$000(org.apache.hadoop.fs.ftp.FTPFileSystem, org.apache.commons.net.ftp.FTPClient) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableCounterLong.class": "Compiled from \"MutableCounterLong.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableCounterLong extends org.apache.hadoop.metrics2.lib.MutableCounter {\n  org.apache.hadoop.metrics2.lib.MutableCounterLong(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public void incr();\n  public void incr(long);\n  public long value();\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n}\n", 
  "org/apache/hadoop/conf/ConfServlet$BadFormatException.class": "Compiled from \"ConfServlet.java\"\npublic class org.apache.hadoop.conf.ConfServlet extends javax.servlet.http.HttpServlet {\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.conf.ConfServlet();\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException;\n  static void writeResponse(org.apache.hadoop.conf.Configuration, java.io.Writer, java.lang.String) throws java.io.IOException, org.apache.hadoop.conf.ConfServlet$BadFormatException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JMap$JavaMap.class": "Compiled from \"JMap.java\"\npublic class org.apache.hadoop.record.compiler.JMap extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JMap(org.apache.hadoop.record.compiler.JType, org.apache.hadoop.record.compiler.JType);\n  java.lang.String getSignature();\n  static java.lang.String access$000(java.lang.String);\n  static void access$100();\n  static void access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto$Builder.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/SocketIOWithTimeout$SelectorPool.class": "Compiled from \"SocketIOWithTimeout.java\"\nabstract class org.apache.hadoop.net.SocketIOWithTimeout {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.net.SocketIOWithTimeout(java.nio.channels.SelectableChannel, long) throws java.io.IOException;\n  void close();\n  boolean isOpen();\n  java.nio.channels.SelectableChannel getChannel();\n  static void checkChannelValidity(java.lang.Object) throws java.io.IOException;\n  abstract int performIO(java.nio.ByteBuffer) throws java.io.IOException;\n  int doIO(java.nio.ByteBuffer, int) throws java.io.IOException;\n  static void connect(java.nio.channels.SocketChannel, java.net.SocketAddress, int) throws java.io.IOException;\n  void waitForIO(int) throws java.io.IOException;\n  public void setTimeout(long);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/sink/FileSink.class": "Compiled from \"FileSink.java\"\npublic class org.apache.hadoop.metrics2.sink.FileSink implements org.apache.hadoop.metrics2.MetricsSink,java.io.Closeable {\n  public org.apache.hadoop.metrics2.sink.FileSink();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  public void putMetrics(org.apache.hadoop.metrics2.MetricsRecord);\n  public void flush();\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/BZip2DummyDecompressor.class": "Compiled from \"BZip2DummyDecompressor.java\"\npublic class org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor implements org.apache.hadoop.io.compress.Decompressor {\n  public org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor();\n  public int decompress(byte[], int, int) throws java.io.IOException;\n  public void end();\n  public boolean finished();\n  public boolean needsDictionary();\n  public boolean needsInput();\n  public int getRemaining();\n  public void reset();\n  public void setDictionary(byte[], int, int);\n  public void setInput(byte[], int, int);\n}\n", 
  "org/apache/hadoop/io/SequenceFile.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetUserMappingsProtocolService$Interface.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL$Token.class": "Compiled from \"DelegationTokenAuthenticatedURL.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL extends org.apache.hadoop.security.authentication.client.AuthenticatedURL {\n  static final java.lang.String DO_AS;\n  public static void setDefaultDelegationTokenAuthenticator(java.lang.Class<? extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator>);\n  public static java.lang.Class<? extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator> getDefaultDelegationTokenAuthenticator();\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL();\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator);\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL(org.apache.hadoop.security.authentication.client.ConnectionConfigurator);\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator, org.apache.hadoop.security.authentication.client.ConnectionConfigurator);\n  protected void setUseQueryStringForDelegationToken(boolean);\n  public boolean useQueryStringForDelegationToken();\n  public java.net.HttpURLConnection openConnection(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public java.net.HttpURLConnection openConnection(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public java.net.HttpURLConnection openConnection(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> getDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> getDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public long renewDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public long renewDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public void cancelDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token) throws java.io.IOException;\n  public void cancelDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/PermissionStatus.class": "Compiled from \"PermissionStatus.java\"\npublic class org.apache.hadoop.fs.permission.PermissionStatus implements org.apache.hadoop.io.Writable {\n  static final org.apache.hadoop.io.WritableFactory FACTORY;\n  public static org.apache.hadoop.fs.permission.PermissionStatus createImmutable(java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.permission.PermissionStatus(java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission);\n  public java.lang.String getUserName();\n  public java.lang.String getGroupName();\n  public org.apache.hadoop.fs.permission.FsPermission getPermission();\n  public org.apache.hadoop.fs.permission.PermissionStatus applyUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public static org.apache.hadoop.fs.permission.PermissionStatus read(java.io.DataInput) throws java.io.IOException;\n  public static void write(java.io.DataOutput, java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public java.lang.String toString();\n  org.apache.hadoop.fs.permission.PermissionStatus(org.apache.hadoop.fs.permission.PermissionStatus$1);\n  static {};\n}\n", 
  "org/apache/hadoop/io/ObjectWritable.class": "Compiled from \"ObjectWritable.java\"\npublic class org.apache.hadoop.io.ObjectWritable implements org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configurable {\n  public org.apache.hadoop.io.ObjectWritable();\n  public org.apache.hadoop.io.ObjectWritable(java.lang.Object);\n  public org.apache.hadoop.io.ObjectWritable(java.lang.Class, java.lang.Object);\n  public java.lang.Object get();\n  public java.lang.Class getDeclaredClass();\n  public void set(java.lang.Object);\n  public java.lang.String toString();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public static void writeObject(java.io.DataOutput, java.lang.Object, java.lang.Class, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void writeObject(java.io.DataOutput, java.lang.Object, java.lang.Class, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public static java.lang.Object readObject(java.io.DataInput, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.lang.Object readObject(java.io.DataInput, org.apache.hadoop.io.ObjectWritable, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static java.lang.reflect.Method getStaticProtobufMethod(java.lang.Class<?>, java.lang.String, java.lang.Class<?>...);\n  public static java.lang.Class<?> loadClass(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  static java.util.Map access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshRequestProto$Builder.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$RemoveSpanReceiverRequestProtoOrBuilder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$CompressedBytes.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFs$2.class": "Compiled from \"ViewFs.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFs extends org.apache.hadoop.fs.AbstractFileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  final org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.AbstractFileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  org.apache.hadoop.fs.viewfs.ViewFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus() throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setVerifyChecksum(boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFs$MountPoint[] getMountPoints();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public boolean isValidName(java.lang.String);\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/IntrusiveCollection$Element.class": "Compiled from \"IntrusiveCollection.java\"\npublic class org.apache.hadoop.util.IntrusiveCollection<E extends org.apache.hadoop.util.IntrusiveCollection$Element> implements java.util.Collection<E> {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.util.IntrusiveCollection();\n  public java.util.Iterator<E> iterator();\n  public int size();\n  public boolean isEmpty();\n  public boolean contains(java.lang.Object);\n  public java.lang.Object[] toArray();\n  public <T extends java/lang/Object> T[] toArray(T[]);\n  public boolean add(E);\n  public boolean addFirst(org.apache.hadoop.util.IntrusiveCollection$Element);\n  public boolean remove(java.lang.Object);\n  public boolean containsAll(java.util.Collection<?>);\n  public boolean addAll(java.util.Collection<? extends E>);\n  public boolean removeAll(java.util.Collection<?>);\n  public boolean retainAll(java.util.Collection<?>);\n  public void clear();\n  public boolean add(java.lang.Object);\n  static org.apache.hadoop.util.IntrusiveCollection$Element access$000(org.apache.hadoop.util.IntrusiveCollection);\n  static org.apache.hadoop.util.IntrusiveCollection$Element access$100(org.apache.hadoop.util.IntrusiveCollection, org.apache.hadoop.util.IntrusiveCollection$Element);\n  static {};\n}\n", 
  "org/apache/hadoop/util/LogAdapter.class": "Compiled from \"LogAdapter.java\"\nclass org.apache.hadoop.util.LogAdapter {\n  public static org.apache.hadoop.util.LogAdapter create(org.apache.commons.logging.Log);\n  public static org.apache.hadoop.util.LogAdapter create(org.slf4j.Logger);\n  public void info(java.lang.String);\n  public void warn(java.lang.String, java.lang.Throwable);\n  public void debug(java.lang.Throwable);\n  public void error(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/XAttrSetFlag.class": "Compiled from \"XAttrSetFlag.java\"\npublic final class org.apache.hadoop.fs.XAttrSetFlag extends java.lang.Enum<org.apache.hadoop.fs.XAttrSetFlag> {\n  public static final org.apache.hadoop.fs.XAttrSetFlag CREATE;\n  public static final org.apache.hadoop.fs.XAttrSetFlag REPLACE;\n  public static org.apache.hadoop.fs.XAttrSetFlag[] values();\n  public static org.apache.hadoop.fs.XAttrSetFlag valueOf(java.lang.String);\n  short getFlag();\n  public static void validate(java.lang.String, boolean, java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/hash/MurmurHash.class": "Compiled from \"MurmurHash.java\"\npublic class org.apache.hadoop.util.hash.MurmurHash extends org.apache.hadoop.util.hash.Hash {\n  public org.apache.hadoop.util.hash.MurmurHash();\n  public static org.apache.hadoop.util.hash.Hash getInstance();\n  public int hash(byte[], int, int);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/Compressor.class": "Compiled from \"Compressor.java\"\npublic interface org.apache.hadoop.io.compress.Compressor {\n  public abstract void setInput(byte[], int, int);\n  public abstract boolean needsInput();\n  public abstract void setDictionary(byte[], int, int);\n  public abstract long getBytesRead();\n  public abstract long getBytesWritten();\n  public abstract void finish();\n  public abstract boolean finished();\n  public abstract int compress(byte[], int, int) throws java.io.IOException;\n  public abstract void reset();\n  public abstract void end();\n  public abstract void reinit(org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/fs/GlobFilter$1.class": "Compiled from \"GlobFilter.java\"\npublic class org.apache.hadoop.fs.GlobFilter implements org.apache.hadoop.fs.PathFilter {\n  public org.apache.hadoop.fs.GlobFilter(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.fs.GlobFilter(java.lang.String, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  void init(java.lang.String, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  boolean hasPattern();\n  public boolean accept(org.apache.hadoop.fs.Path);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/snappy/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.io.compress.snappy.package-info {\n}\n", 
  "org/apache/hadoop/security/SaslRpcClient$1.class": "Compiled from \"SaslRpcClient.java\"\npublic class org.apache.hadoop.security.SaslRpcClient {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.SaslRpcClient(org.apache.hadoop.security.UserGroupInformation, java.lang.Class<?>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration);\n  public java.lang.Object getNegotiatedProperty(java.lang.String);\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod getAuthMethod();\n  java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth) throws java.io.IOException;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod saslConnect(java.io.InputStream, java.io.OutputStream) throws java.io.IOException;\n  public java.io.InputStream getInputStream(java.io.InputStream) throws java.io.IOException;\n  public java.io.OutputStream getOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public void dispose() throws javax.security.sasl.SaslException;\n  static javax.security.sasl.SaslClient access$000(org.apache.hadoop.security.SaslRpcClient);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsRecordImpl.class": "Compiled from \"MetricsRecordImpl.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsRecordImpl extends org.apache.hadoop.metrics2.impl.AbstractMetricsRecord {\n  protected static final java.lang.String DEFAULT_CONTEXT;\n  public org.apache.hadoop.metrics2.impl.MetricsRecordImpl(org.apache.hadoop.metrics2.MetricsInfo, long, java.util.List<org.apache.hadoop.metrics2.MetricsTag>, java.lang.Iterable<org.apache.hadoop.metrics2.AbstractMetric>);\n  public long timestamp();\n  public java.lang.String name();\n  org.apache.hadoop.metrics2.MetricsInfo info();\n  public java.lang.String description();\n  public java.lang.String context();\n  public java.util.List<org.apache.hadoop.metrics2.MetricsTag> tags();\n  public java.lang.Iterable<org.apache.hadoop.metrics2.AbstractMetric> metrics();\n  public java.util.Collection tags();\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFileSystem$MountPoint.class": "Compiled from \"ViewFileSystem.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFileSystem extends org.apache.hadoop.fs.FileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  java.net.URI myUri;\n  org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.FileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem() throws java.io.IOException;\n  public java.lang.String getScheme();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  org.apache.hadoop.fs.viewfs.ViewFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getTrashCanLocation(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException;\n  public java.net.URI getUri();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public long getDefaultBlockSize();\n  public short getDefaultReplication();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint[] getMountPoints();\n  static org.apache.hadoop.fs.Path access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsSystem$AbstractCallback.class": "Compiled from \"MetricsSystem.java\"\npublic abstract class org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSystemMXBean {\n  public org.apache.hadoop.metrics2.MetricsSystem();\n  public abstract org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public abstract <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public abstract void unregisterSource(java.lang.String);\n  public <T extends java/lang/Object> T register(T);\n  public abstract org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  public abstract <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  public abstract void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public abstract void publishMetricsNow();\n  public abstract boolean shutdown();\n}\n", 
  "org/apache/hadoop/security/alias/JavaKeyStoreProvider.class": "Compiled from \"JavaKeyStoreProvider.java\"\npublic class org.apache.hadoop.security.alias.JavaKeyStoreProvider extends org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider {\n  public static final java.lang.String SCHEME_NAME;\n  protected java.lang.String getSchemeName();\n  protected java.io.OutputStream getOutputStreamForKeystore() throws java.io.IOException;\n  protected boolean keystoreExists() throws java.io.IOException;\n  protected java.io.InputStream getInputStreamForFile() throws java.io.IOException;\n  protected void createPermissions(java.lang.String);\n  protected void stashOriginalFilePermissions() throws java.io.IOException;\n  protected void initFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  org.apache.hadoop.security.alias.JavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.alias.JavaKeyStoreProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/DataChecksum$1.class": "Compiled from \"DataChecksum.java\"\npublic class org.apache.hadoop.util.DataChecksum implements java.util.zip.Checksum {\n  public static final int CHECKSUM_NULL;\n  public static final int CHECKSUM_CRC32;\n  public static final int CHECKSUM_CRC32C;\n  public static final int CHECKSUM_DEFAULT;\n  public static final int CHECKSUM_MIXED;\n  public static final int SIZE_OF_INTEGER;\n  public static java.util.zip.Checksum newCrc32();\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(org.apache.hadoop.util.DataChecksum$Type, int);\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(byte[], int);\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(java.io.DataInputStream) throws java.io.IOException;\n  public void writeHeader(java.io.DataOutputStream) throws java.io.IOException;\n  public byte[] getHeader();\n  public int writeValue(java.io.DataOutputStream, boolean) throws java.io.IOException;\n  public int writeValue(byte[], int, boolean) throws java.io.IOException;\n  public boolean compare(byte[], int);\n  public org.apache.hadoop.util.DataChecksum$Type getChecksumType();\n  public int getChecksumSize();\n  public int getChecksumSize(int);\n  public int getBytesPerChecksum();\n  public int getNumBytesInSum();\n  public static int getChecksumHeaderSize();\n  public long getValue();\n  public void reset();\n  public void update(byte[], int, int);\n  public void update(int);\n  public void verifyChunkedSums(java.nio.ByteBuffer, java.nio.ByteBuffer, java.lang.String, long) throws org.apache.hadoop.fs.ChecksumException;\n  public void calculateChunkedSums(java.nio.ByteBuffer, java.nio.ByteBuffer);\n  public void calculateChunkedSums(byte[], int, int, byte[], int);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/io/MD5Hash$Comparator.class": "Compiled from \"MD5Hash.java\"\npublic class org.apache.hadoop.io.MD5Hash implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.MD5Hash> {\n  public static final int MD5_LEN;\n  public org.apache.hadoop.io.MD5Hash();\n  public org.apache.hadoop.io.MD5Hash(java.lang.String);\n  public org.apache.hadoop.io.MD5Hash(byte[]);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static org.apache.hadoop.io.MD5Hash read(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void set(org.apache.hadoop.io.MD5Hash);\n  public byte[] getDigest();\n  public static org.apache.hadoop.io.MD5Hash digest(byte[]);\n  public static java.security.MessageDigest getDigester();\n  public static org.apache.hadoop.io.MD5Hash digest(java.io.InputStream) throws java.io.IOException;\n  public static org.apache.hadoop.io.MD5Hash digest(byte[], int, int);\n  public static org.apache.hadoop.io.MD5Hash digest(java.lang.String);\n  public static org.apache.hadoop.io.MD5Hash digest(org.apache.hadoop.io.UTF8);\n  public long halfDigest();\n  public int quarterDigest();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.MD5Hash);\n  public java.lang.String toString();\n  public void setDigest(java.lang.String);\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/security/ssl/SSLHostnameVerifier$Certificates.class": "Compiled from \"SSLHostnameVerifier.java\"\npublic interface org.apache.hadoop.security.ssl.SSLHostnameVerifier extends javax.net.ssl.HostnameVerifier {\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT_AND_LOCALHOST;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT_IE6;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier ALLOW_ALL;\n  public abstract boolean verify(java.lang.String, javax.net.ssl.SSLSession);\n  public abstract void check(java.lang.String, javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String, java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String, java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String[], java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RPC$RpcInvoker.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/AbstractFileSystem$2.class": "Compiled from \"AbstractFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.AbstractFileSystem {\n  static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  static final java.lang.String NO_ABSTRACT_FS_ERROR;\n  public org.apache.hadoop.fs.FileSystem$Statistics getStatistics();\n  public boolean isValidName(java.lang.String);\n  static <T extends java/lang/Object> T newInstance(java.lang.Class<T>, java.net.URI, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.fs.AbstractFileSystem createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  protected static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics();\n  protected static synchronized java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static org.apache.hadoop.fs.AbstractFileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem(java.net.URI, java.lang.String, boolean, int) throws java.net.URISyntaxException;\n  public void checkScheme(java.net.URI, java.lang.String);\n  public abstract int getUriDefaultPort();\n  public java.net.URI getUri();\n  public void checkPath(org.apache.hadoop.fs.Path);\n  public java.lang.String getUriPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public final org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public final void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FsStatus getFsStatus() throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract void setVerifyChecksum(boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.class": "Compiled from \"KeyProviderCryptoExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderCryptoExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension> {\n  public static final java.lang.String EEK;\n  public static final java.lang.String EK;\n  protected org.apache.hadoop.crypto.key.KeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider, org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension);\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public static org.apache.hadoop.crypto.key.KeyProviderCryptoExtension createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$WatcherWithClientRef.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$ParsedTimeDuration$3.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolVersionsRequestProtoOrBuilder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JBoolean.class": "Compiled from \"JBoolean.java\"\npublic class org.apache.hadoop.record.compiler.JBoolean extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JBoolean();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/local/LocalFs.class": "Compiled from \"LocalFs.java\"\npublic class org.apache.hadoop.fs.local.LocalFs extends org.apache.hadoop.fs.ChecksumFs {\n  org.apache.hadoop.fs.local.LocalFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  org.apache.hadoop.fs.local.LocalFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n}\n", 
  "org/apache/hadoop/service/AbstractService.class": "Compiled from \"AbstractService.java\"\npublic abstract class org.apache.hadoop.service.AbstractService implements org.apache.hadoop.service.Service {\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.service.AbstractService(java.lang.String);\n  public final org.apache.hadoop.service.Service$STATE getServiceState();\n  public final synchronized java.lang.Throwable getFailureCause();\n  public synchronized org.apache.hadoop.service.Service$STATE getFailureState();\n  protected void setConfig(org.apache.hadoop.conf.Configuration);\n  public void init(org.apache.hadoop.conf.Configuration);\n  public void start();\n  public void stop();\n  public final void close() throws java.io.IOException;\n  protected final void noteFailure(java.lang.Exception);\n  public final boolean waitForServiceToStop(long);\n  protected void serviceInit(org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  protected void serviceStart() throws java.lang.Exception;\n  protected void serviceStop() throws java.lang.Exception;\n  public void registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener);\n  public void unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener);\n  public static void registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener);\n  public static boolean unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener);\n  static void resetGlobalListeners();\n  public java.lang.String getName();\n  public synchronized org.apache.hadoop.conf.Configuration getConfig();\n  public long getStartTime();\n  public synchronized java.util.List<org.apache.hadoop.service.LifecycleEvent> getLifecycleHistory();\n  public final boolean isInState(org.apache.hadoop.service.Service$STATE);\n  public java.lang.String toString();\n  protected void putBlocker(java.lang.String, java.lang.String);\n  public void removeBlocker(java.lang.String);\n  public java.util.Map<java.lang.String, java.lang.String> getBlockers();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToStandbyRequestProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/MetricsContext.class": "Compiled from \"MetricsContext.java\"\npublic interface org.apache.hadoop.metrics.MetricsContext {\n  public static final int DEFAULT_PERIOD;\n  public abstract void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  public abstract java.lang.String getContextName();\n  public abstract void startMonitoring() throws java.io.IOException;\n  public abstract void stopMonitoring();\n  public abstract boolean isMonitoring();\n  public abstract void close();\n  public abstract org.apache.hadoop.metrics.MetricsRecord createRecord(java.lang.String);\n  public abstract void registerUpdater(org.apache.hadoop.metrics.Updater);\n  public abstract void unregisterUpdater(org.apache.hadoop.metrics.Updater);\n  public abstract int getPeriod();\n  public abstract java.util.Map<java.lang.String, java.util.Collection<org.apache.hadoop.metrics.spi.OutputRecord>> getAllRecords();\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$Windows.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/CBZip2InputStream$STATE.class": "Compiled from \"CBZip2InputStream.java\"\npublic class org.apache.hadoop.io.compress.bzip2.CBZip2InputStream extends java.io.InputStream implements org.apache.hadoop.io.compress.bzip2.BZip2Constants {\n  public static final long BLOCK_DELIMITER;\n  public static final long EOS_DELIMITER;\n  org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE readMode;\n  public long getProcessedByteCount();\n  protected void updateProcessedByteCount(int);\n  public void updateReportedByteCount(int);\n  public boolean skipToNextMarker(long, int) throws java.io.IOException, java.lang.IllegalArgumentException;\n  protected void reportCRCError() throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2InputStream(java.io.InputStream, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n  public static long numberOfBytesTillNextMarker(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2InputStream(java.io.InputStream) throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/record/compiler/JInt$CppInt.class": "Compiled from \"JInt.java\"\npublic class org.apache.hadoop.record.compiler.JInt extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JInt();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.class": "Compiled from \"MBeanInfoBuilder.java\"\nclass org.apache.hadoop.metrics2.impl.MBeanInfoBuilder implements org.apache.hadoop.metrics2.MetricsVisitor {\n  org.apache.hadoop.metrics2.impl.MBeanInfoBuilder(java.lang.String, java.lang.String);\n  org.apache.hadoop.metrics2.impl.MBeanInfoBuilder reset(java.lang.Iterable<org.apache.hadoop.metrics2.impl.MetricsRecordImpl>);\n  javax.management.MBeanAttributeInfo newAttrInfo(java.lang.String, java.lang.String, java.lang.String);\n  javax.management.MBeanAttributeInfo newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public void gauge(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public void gauge(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public void gauge(org.apache.hadoop.metrics2.MetricsInfo, float);\n  public void gauge(org.apache.hadoop.metrics2.MetricsInfo, double);\n  public void counter(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public void counter(org.apache.hadoop.metrics2.MetricsInfo, long);\n  java.lang.String getAttrName(java.lang.String);\n  javax.management.MBeanInfo get();\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController$ActiveAttemptRecord.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$POSIX$Stat.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts$Perms.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsTimeVaryingRate$MinMax.class": "Compiled from \"MetricsTimeVaryingRate.java\"\npublic class org.apache.hadoop.metrics.util.MetricsTimeVaryingRate extends org.apache.hadoop.metrics.util.MetricsBase {\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingRate(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry, java.lang.String);\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingRate(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry);\n  public synchronized void inc(int, long);\n  public synchronized void inc(long);\n  public synchronized void pushMetric(org.apache.hadoop.metrics.MetricsRecord);\n  public synchronized int getPreviousIntervalNumOps();\n  public synchronized long getPreviousIntervalAverageTime();\n  public synchronized long getMinTime();\n  public synchronized long getMaxTime();\n  public synchronized void resetMinMax();\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/BZip2Codec$BZip2CompressionInputStream.class": "Compiled from \"BZip2Codec.java\"\npublic class org.apache.hadoop.io.compress.BZip2Codec implements org.apache.hadoop.conf.Configurable,org.apache.hadoop.io.compress.SplittableCompressionCodec {\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public org.apache.hadoop.io.compress.BZip2Codec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.SplitCompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, long, long, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.String getDefaultExtension();\n  static int access$000();\n  static int access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/JavaKeyStoreProvider$KeyMetadata.class": "Compiled from \"JavaKeyStoreProvider.java\"\npublic class org.apache.hadoop.crypto.key.JavaKeyStoreProvider extends org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String KEYSTORE_PASSWORD_FILE_KEY;\n  public static final java.lang.String KEYSTORE_PASSWORD_ENV_VAR;\n  public static final char[] KEYSTORE_PASSWORD_DEFAULT;\n  org.apache.hadoop.crypto.key.JavaKeyStoreProvider(org.apache.hadoop.crypto.key.JavaKeyStoreProvider);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.crypto.key.KeyProvider$KeyVersion innerSetKeyVersion(java.lang.String, java.lang.String, byte[], java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  protected void writeToNew(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean backupToOld(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.lang.String toString();\n  org.apache.hadoop.crypto.key.JavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.crypto.key.JavaKeyStoreProvider$1) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FSLinkResolver.class": "Compiled from \"FSLinkResolver.java\"\npublic abstract class org.apache.hadoop.fs.FSLinkResolver<T> {\n  public org.apache.hadoop.fs.FSLinkResolver();\n  public static org.apache.hadoop.fs.Path qualifySymlinkTarget(java.net.URI, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public abstract T next(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public T resolve(org.apache.hadoop.fs.FileContext, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/compress/lz4/Lz4Compressor.class": "Compiled from \"Lz4Compressor.java\"\npublic class org.apache.hadoop.io.compress.lz4.Lz4Compressor implements org.apache.hadoop.io.compress.Compressor {\n  public org.apache.hadoop.io.compress.lz4.Lz4Compressor(int, boolean);\n  public org.apache.hadoop.io.compress.lz4.Lz4Compressor(int);\n  public org.apache.hadoop.io.compress.lz4.Lz4Compressor();\n  public synchronized void setInput(byte[], int, int);\n  synchronized void setInputFromSavedData();\n  public synchronized void setDictionary(byte[], int, int);\n  public synchronized boolean needsInput();\n  public synchronized void finish();\n  public synchronized boolean finished();\n  public synchronized int compress(byte[], int, int) throws java.io.IOException;\n  public synchronized void reset();\n  public synchronized void reinit(org.apache.hadoop.conf.Configuration);\n  public synchronized long getBytesRead();\n  public synchronized long getBytesWritten();\n  public synchronized void end();\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/FailoverController.class": "Compiled from \"FailoverController.java\"\npublic class org.apache.hadoop.ha.FailoverController {\n  public org.apache.hadoop.ha.FailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceProtocol$RequestSource);\n  static int getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration);\n  static int getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration);\n  boolean tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget);\n  public void failover(org.apache.hadoop.ha.HAServiceTarget, org.apache.hadoop.ha.HAServiceTarget, boolean, boolean) throws org.apache.hadoop.ha.FailoverFailedException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/util/MetricsCache.class": "Compiled from \"MetricsCache.java\"\npublic class org.apache.hadoop.metrics2.util.MetricsCache {\n  static final org.apache.commons.logging.Log LOG;\n  static final int MAX_RECS_PER_NAME_DEFAULT;\n  public org.apache.hadoop.metrics2.util.MetricsCache();\n  public org.apache.hadoop.metrics2.util.MetricsCache(int);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record update(org.apache.hadoop.metrics2.MetricsRecord, boolean);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record update(org.apache.hadoop.metrics2.MetricsRecord);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record get(java.lang.String, java.util.Collection<org.apache.hadoop.metrics2.MetricsTag>);\n  static int access$000(org.apache.hadoop.metrics2.util.MetricsCache);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RemoteException.class": "Compiled from \"RemoteException.java\"\npublic class org.apache.hadoop.ipc.RemoteException extends java.io.IOException {\n  public org.apache.hadoop.ipc.RemoteException(java.lang.String, java.lang.String);\n  public org.apache.hadoop.ipc.RemoteException(java.lang.String, java.lang.String, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto);\n  public java.lang.String getClassName();\n  public org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto getErrorCode();\n  public java.io.IOException unwrapRemoteException(java.lang.Class<?>...);\n  public java.io.IOException unwrapRemoteException();\n  public static org.apache.hadoop.ipc.RemoteException valueOf(org.xml.sax.Attributes);\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/tracing/SpanReceiverHost.class": "Compiled from \"SpanReceiverHost.java\"\npublic class org.apache.hadoop.tracing.SpanReceiverHost implements org.apache.hadoop.tracing.TraceAdminProtocol {\n  public static final java.lang.String SPAN_RECEIVERS_CONF_SUFFIX;\n  public static org.apache.hadoop.tracing.SpanReceiverHost get(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public synchronized void loadSpanReceivers(org.apache.hadoop.conf.Configuration);\n  public synchronized void closeReceivers();\n  public synchronized org.apache.hadoop.tracing.SpanReceiverInfo[] listSpanReceivers() throws java.io.IOException;\n  public synchronized long addSpanReceiver(org.apache.hadoop.tracing.SpanReceiverInfo) throws java.io.IOException;\n  public synchronized void removeSpanReceiver(long) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$1.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/BlockDecompressorStream.class": "Compiled from \"BlockDecompressorStream.java\"\npublic class org.apache.hadoop.io.compress.BlockDecompressorStream extends org.apache.hadoop.io.compress.DecompressorStream {\n  public org.apache.hadoop.io.compress.BlockDecompressorStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, int) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.BlockDecompressorStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  protected org.apache.hadoop.io.compress.BlockDecompressorStream(java.io.InputStream) throws java.io.IOException;\n  protected int decompress(byte[], int, int) throws java.io.IOException;\n  protected int getCompressedData() throws java.io.IOException;\n  public void resetState() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/MapFile$Writer$ComparatorOption.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$CancelDelegationTokenRequestProto.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$RenewDelegationTokenRequestProto$Builder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/WritableRpcEngine.class": "Compiled from \"WritableRpcEngine.java\"\npublic class org.apache.hadoop.ipc.WritableRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final long writableRpcVersion;\n  public org.apache.hadoop.ipc.WritableRpcEngine();\n  public static synchronized void ensureInitialized();\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$000();\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$Resource.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/VersionMismatchException.class": "Compiled from \"VersionMismatchException.java\"\npublic class org.apache.hadoop.io.VersionMismatchException extends java.io.IOException {\n  public org.apache.hadoop.io.VersionMismatchException(byte, byte);\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Reader$FileOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/service/Service.class": "Compiled from \"Service.java\"\npublic interface org.apache.hadoop.service.Service extends java.io.Closeable {\n  public abstract void init(org.apache.hadoop.conf.Configuration);\n  public abstract void start();\n  public abstract void stop();\n  public abstract void close() throws java.io.IOException;\n  public abstract void registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener);\n  public abstract void unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener);\n  public abstract java.lang.String getName();\n  public abstract org.apache.hadoop.conf.Configuration getConfig();\n  public abstract org.apache.hadoop.service.Service$STATE getServiceState();\n  public abstract long getStartTime();\n  public abstract boolean isInState(org.apache.hadoop.service.Service$STATE);\n  public abstract java.lang.Throwable getFailureCause();\n  public abstract org.apache.hadoop.service.Service$STATE getFailureState();\n  public abstract boolean waitForServiceToStop(long);\n  public abstract java.util.List<org.apache.hadoop.service.LifecycleEvent> getLifecycleHistory();\n  public abstract java.util.Map<java.lang.String, java.lang.String> getBlockers();\n}\n", 
  "org/apache/hadoop/util/Options.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/security/SaslRpcClient.class": "Compiled from \"SaslRpcClient.java\"\npublic class org.apache.hadoop.security.SaslRpcClient {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.SaslRpcClient(org.apache.hadoop.security.UserGroupInformation, java.lang.Class<?>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration);\n  public java.lang.Object getNegotiatedProperty(java.lang.String);\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod getAuthMethod();\n  java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth) throws java.io.IOException;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod saslConnect(java.io.InputStream, java.io.OutputStream) throws java.io.IOException;\n  public java.io.InputStream getInputStream(java.io.InputStream) throws java.io.IOException;\n  public java.io.OutputStream getOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public void dispose() throws javax.security.sasl.SaslException;\n  static javax.security.sasl.SaslClient access$000(org.apache.hadoop.security.SaslRpcClient);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/zlib/BuiltInZlibInflater.class": "Compiled from \"BuiltInZlibInflater.java\"\npublic class org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater extends java.util.zip.Inflater implements org.apache.hadoop.io.compress.Decompressor {\n  public org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater(boolean);\n  public org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater();\n  public synchronized int decompress(byte[], int, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Magic.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsInfo.class": "Compiled from \"MetricsInfo.java\"\npublic interface org.apache.hadoop.metrics2.MetricsInfo {\n  public abstract java.lang.String name();\n  public abstract java.lang.String description();\n}\n", 
  "org/apache/hadoop/ha/StreamPumper$StreamType.class": "Compiled from \"StreamPumper.java\"\nclass org.apache.hadoop.ha.StreamPumper {\n  final java.lang.Thread thread;\n  final java.lang.String logPrefix;\n  final org.apache.hadoop.ha.StreamPumper$StreamType type;\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.StreamPumper(org.apache.commons.logging.Log, java.lang.String, java.io.InputStream, org.apache.hadoop.ha.StreamPumper$StreamType);\n  void join() throws java.lang.InterruptedException;\n  void start();\n  protected void pump() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryProxy.class": "Compiled from \"RetryProxy.java\"\npublic class org.apache.hadoop.io.retry.RetryProxy {\n  public org.apache.hadoop.io.retry.RetryProxy();\n  public static <T extends java/lang/Object> java.lang.Object create(java.lang.Class<T>, T, org.apache.hadoop.io.retry.RetryPolicy);\n  public static <T extends java/lang/Object> java.lang.Object create(java.lang.Class<T>, org.apache.hadoop.io.retry.FailoverProxyProvider<T>, org.apache.hadoop.io.retry.RetryPolicy);\n  public static <T extends java/lang/Object> java.lang.Object create(java.lang.Class<T>, T, java.util.Map<java.lang.String, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static <T extends java/lang/Object> java.lang.Object create(java.lang.Class<T>, org.apache.hadoop.io.retry.FailoverProxyProvider<T>, java.util.Map<java.lang.String, org.apache.hadoop.io.retry.RetryPolicy>, org.apache.hadoop.io.retry.RetryPolicy);\n}\n", 
  "org/apache/hadoop/io/BytesWritable$Comparator.class": "Compiled from \"BytesWritable.java\"\npublic class org.apache.hadoop.io.BytesWritable extends org.apache.hadoop.io.BinaryComparable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.BinaryComparable> {\n  public org.apache.hadoop.io.BytesWritable();\n  public org.apache.hadoop.io.BytesWritable(byte[]);\n  public org.apache.hadoop.io.BytesWritable(byte[], int);\n  public byte[] copyBytes();\n  public byte[] getBytes();\n  public byte[] get();\n  public int getLength();\n  public int getSize();\n  public void setSize(int);\n  public int getCapacity();\n  public void setCapacity(int);\n  public void set(org.apache.hadoop.io.BytesWritable);\n  public void set(byte[], int, int);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$6.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolVersionProto$Builder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocket$DomainInputStream.class": "Compiled from \"DomainSocket.java\"\npublic class org.apache.hadoop.net.unix.DomainSocket implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final org.apache.hadoop.util.CloseableReferenceCount refCount;\n  final int fd;\n  public static final int SEND_BUFFER_SIZE;\n  public static final int RECEIVE_BUFFER_SIZE;\n  public static final int SEND_TIMEOUT;\n  public static final int RECEIVE_TIMEOUT;\n  static native void validateSocketPathSecurity0(java.lang.String, int) throws java.io.IOException;\n  public static java.lang.String getLoadingFailureReason();\n  public static void disableBindPathValidation();\n  public static java.lang.String getEffectivePath(java.lang.String, int);\n  public static org.apache.hadoop.net.unix.DomainSocket bindAndListen(java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.net.unix.DomainSocket[] socketpair() throws java.io.IOException;\n  public org.apache.hadoop.net.unix.DomainSocket accept() throws java.io.IOException;\n  public static org.apache.hadoop.net.unix.DomainSocket connect(java.lang.String) throws java.io.IOException;\n  public boolean isOpen();\n  public java.lang.String getPath();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainInputStream getInputStream();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream getOutputStream();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainChannel getChannel();\n  public void setAttribute(int, int) throws java.io.IOException;\n  public int getAttribute(int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void shutdown() throws java.io.IOException;\n  public void sendFileDescriptors(java.io.FileDescriptor[], byte[], int, int) throws java.io.IOException;\n  public int receiveFileDescriptors(java.io.FileDescriptor[], byte[], int, int) throws java.io.IOException;\n  public int recvFileInputStreams(java.io.FileInputStream[], byte[], int, int) throws java.io.IOException;\n  public java.lang.String toString();\n  static int access$000(int, byte[], int, int) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.net.unix.DomainSocket, boolean) throws java.nio.channels.ClosedChannelException;\n  static int access$200(int) throws java.io.IOException;\n  static void access$300(int, byte[], int, int) throws java.io.IOException;\n  static int access$400(int, java.nio.ByteBuffer, int, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslRpcServer$2.class": "Compiled from \"SaslRpcServer.java\"\npublic class org.apache.hadoop.security.SaslRpcServer {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String SASL_DEFAULT_REALM;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod authMethod;\n  public java.lang.String mechanism;\n  public java.lang.String protocol;\n  public java.lang.String serverId;\n  public org.apache.hadoop.security.SaslRpcServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod) throws java.io.IOException;\n  public javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection, java.util.Map<java.lang.String, ?>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void init(org.apache.hadoop.conf.Configuration);\n  static java.lang.String encodeIdentifier(byte[]);\n  static byte[] decodeIdentifier(java.lang.String);\n  public static <T extends org/apache/hadoop/security/token/TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  static char[] encodePassword(byte[]);\n  public static java.lang.String[] splitKerberosName(java.lang.String);\n  static javax.security.sasl.SaslServerFactory access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/SnapshotCommands.class": "Compiled from \"SnapshotCommands.java\"\nclass org.apache.hadoop.fs.shell.SnapshotCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.SnapshotCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$GetServiceStatusResponseProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Client$Connection$PingInputStream.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager$1.class": "Compiled from \"ZKDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent> {\n  public static final java.lang.String ZK_DTSM_ZK_NUM_RETRIES;\n  public static final java.lang.String ZK_DTSM_ZK_SESSION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_SHUTDOWN_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH;\n  public static final java.lang.String ZK_DTSM_ZK_AUTH_TYPE;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_STRING;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_KEYTAB;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_PRINCIPAL;\n  public static final int ZK_DTSM_ZK_NUM_RETRIES_DEFAULT;\n  public static final int ZK_DTSM_ZK_SESSION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_CONNECTION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_SHUTDOWN_TIMEOUT_DEFAULT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH_DEAFULT;\n  public static void setCurator(org.apache.curator.framework.CuratorFramework);\n  public org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager(org.apache.hadoop.conf.Configuration);\n  public void startThreads() throws java.io.IOException;\n  public void stopThreads();\n  protected int getDelegationTokenSeqNum();\n  protected int incrementDelegationTokenSeqNum();\n  protected void setDelegationTokenSeqNum(int);\n  protected int getCurrentKeyId();\n  protected int incrementCurrentKeyId();\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  static java.lang.String getNodePath(java.lang.String, java.lang.String);\n  public java.util.concurrent.ExecutorService getListenerThreadPool();\n  static void access$100(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, byte[]) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, java.lang.String);\n  static void access$300(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static void access$400(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JBuffer$JavaBuffer.class": "Compiled from \"JBuffer.java\"\npublic class org.apache.hadoop.record.compiler.JBuffer extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JBuffer();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/FileContext$38.class": "", 
  "org/apache/hadoop/security/proto/SecurityProtos$CancelDelegationTokenResponseProto.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/IntrusiveCollection.class": "Compiled from \"IntrusiveCollection.java\"\npublic class org.apache.hadoop.util.IntrusiveCollection<E extends org.apache.hadoop.util.IntrusiveCollection$Element> implements java.util.Collection<E> {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.util.IntrusiveCollection();\n  public java.util.Iterator<E> iterator();\n  public int size();\n  public boolean isEmpty();\n  public boolean contains(java.lang.Object);\n  public java.lang.Object[] toArray();\n  public <T extends java/lang/Object> T[] toArray(T[]);\n  public boolean add(E);\n  public boolean addFirst(org.apache.hadoop.util.IntrusiveCollection$Element);\n  public boolean remove(java.lang.Object);\n  public boolean containsAll(java.util.Collection<?>);\n  public boolean addAll(java.util.Collection<? extends E>);\n  public boolean removeAll(java.util.Collection<?>);\n  public boolean retainAll(java.util.Collection<?>);\n  public void clear();\n  public boolean add(java.lang.Object);\n  static org.apache.hadoop.util.IntrusiveCollection$Element access$000(org.apache.hadoop.util.IntrusiveCollection);\n  static org.apache.hadoop.util.IntrusiveCollection$Element access$100(org.apache.hadoop.util.IntrusiveCollection, org.apache.hadoop.util.IntrusiveCollection$Element);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFileSystem.class": "Compiled from \"ViewFileSystem.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFileSystem extends org.apache.hadoop.fs.FileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  java.net.URI myUri;\n  org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.FileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem() throws java.io.IOException;\n  public java.lang.String getScheme();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  org.apache.hadoop.fs.viewfs.ViewFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getTrashCanLocation(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException;\n  public java.net.URI getUri();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public long getDefaultBlockSize();\n  public short getDefaultReplication();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint[] getMountPoints();\n  static org.apache.hadoop.fs.Path access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/RecordOutput.class": "Compiled from \"RecordOutput.java\"\npublic interface org.apache.hadoop.record.RecordOutput {\n  public abstract void writeByte(byte, java.lang.String) throws java.io.IOException;\n  public abstract void writeBool(boolean, java.lang.String) throws java.io.IOException;\n  public abstract void writeInt(int, java.lang.String) throws java.io.IOException;\n  public abstract void writeLong(long, java.lang.String) throws java.io.IOException;\n  public abstract void writeFloat(float, java.lang.String) throws java.io.IOException;\n  public abstract void writeDouble(double, java.lang.String) throws java.io.IOException;\n  public abstract void writeString(java.lang.String, java.lang.String) throws java.io.IOException;\n  public abstract void writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String) throws java.io.IOException;\n  public abstract void startRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public abstract void endRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public abstract void startVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public abstract void endVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public abstract void startMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n  public abstract void endMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.class": "Compiled from \"RefreshUserMappingsProtocolClientSideTranslatorPB.java\"\npublic class org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB implements org.apache.hadoop.ipc.ProtocolMetaInterface,org.apache.hadoop.security.RefreshUserMappingsProtocol,java.io.Closeable {\n  public org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB(org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB);\n  public void close() throws java.io.IOException;\n  public void refreshUserToGroupsMappings() throws java.io.IOException;\n  public void refreshSuperUserGroupsConfiguration() throws java.io.IOException;\n  public boolean isMethodSupported(java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/AtMostOnce.class": "Compiled from \"AtMostOnce.java\"\npublic interface org.apache.hadoop.io.retry.AtMostOnce extends java.lang.annotation.Annotation {\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcRequestHeaderProto$OperationProto.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocketWatcher.class": "Compiled from \"DomainSocketWatcher.java\"\npublic final class org.apache.hadoop.net.unix.DomainSocketWatcher implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final java.lang.Thread watcherThread;\n  static final boolean $assertionsDisabled;\n  public static java.lang.String getLoadingFailureReason();\n  public org.apache.hadoop.net.unix.DomainSocketWatcher(int, java.lang.String) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void add(org.apache.hadoop.net.unix.DomainSocket, org.apache.hadoop.net.unix.DomainSocketWatcher$Handler);\n  public void remove(org.apache.hadoop.net.unix.DomainSocket);\n  public java.lang.String toString();\n  static java.util.concurrent.locks.ReentrantLock access$000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$102(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static boolean access$202(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static int access$300(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static void access$400(org.apache.hadoop.net.unix.DomainSocketWatcher, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet);\n  static void access$500(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static java.util.LinkedList access$600(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.TreeMap access$700(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.concurrent.locks.Condition access$800(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$200(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static int access$900(int, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet) throws java.io.IOException;\n  static void access$1000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$1100(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$Listener$Reader.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceStateProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIOException.class": "Compiled from \"NativeIOException.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIOException extends java.io.IOException {\n  public org.apache.hadoop.io.nativeio.NativeIOException(java.lang.String, org.apache.hadoop.io.nativeio.Errno);\n  public org.apache.hadoop.io.nativeio.NativeIOException(java.lang.String, int);\n  public long getErrorCode();\n  public org.apache.hadoop.io.nativeio.Errno getErrno();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/metrics2/util/SampleQuantiles$SampleItem.class": "Compiled from \"SampleQuantiles.java\"\npublic class org.apache.hadoop.metrics2.util.SampleQuantiles {\n  public org.apache.hadoop.metrics2.util.SampleQuantiles(org.apache.hadoop.metrics2.util.Quantile[]);\n  public synchronized void insert(long);\n  public synchronized java.util.Map<org.apache.hadoop.metrics2.util.Quantile, java.lang.Long> snapshot();\n  public synchronized long getCount();\n  public synchronized int getSampleCount();\n  public synchronized void clear();\n  public synchronized java.lang.String toString();\n}\n", 
  "org/apache/hadoop/util/MachineList$InetAddressFactory.class": "Compiled from \"MachineList.java\"\npublic class org.apache.hadoop.util.MachineList {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String WILDCARD_VALUE;\n  public org.apache.hadoop.util.MachineList(java.lang.String);\n  public org.apache.hadoop.util.MachineList(java.util.Collection<java.lang.String>);\n  public org.apache.hadoop.util.MachineList(java.util.Collection<java.lang.String>, org.apache.hadoop.util.MachineList$InetAddressFactory);\n  public boolean includes(java.lang.String);\n  public java.util.Collection<java.lang.String> getCollection();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/CachingKeyProvider$CacheExtension$1.class": "Compiled from \"CachingKeyProvider.java\"\npublic class org.apache.hadoop.crypto.key.CachingKeyProvider extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension> {\n  public org.apache.hadoop.crypto.key.CachingKeyProvider(org.apache.hadoop.crypto.key.KeyProvider, long, long);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocket$DomainChannel.class": "Compiled from \"DomainSocket.java\"\npublic class org.apache.hadoop.net.unix.DomainSocket implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final org.apache.hadoop.util.CloseableReferenceCount refCount;\n  final int fd;\n  public static final int SEND_BUFFER_SIZE;\n  public static final int RECEIVE_BUFFER_SIZE;\n  public static final int SEND_TIMEOUT;\n  public static final int RECEIVE_TIMEOUT;\n  static native void validateSocketPathSecurity0(java.lang.String, int) throws java.io.IOException;\n  public static java.lang.String getLoadingFailureReason();\n  public static void disableBindPathValidation();\n  public static java.lang.String getEffectivePath(java.lang.String, int);\n  public static org.apache.hadoop.net.unix.DomainSocket bindAndListen(java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.net.unix.DomainSocket[] socketpair() throws java.io.IOException;\n  public org.apache.hadoop.net.unix.DomainSocket accept() throws java.io.IOException;\n  public static org.apache.hadoop.net.unix.DomainSocket connect(java.lang.String) throws java.io.IOException;\n  public boolean isOpen();\n  public java.lang.String getPath();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainInputStream getInputStream();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream getOutputStream();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainChannel getChannel();\n  public void setAttribute(int, int) throws java.io.IOException;\n  public int getAttribute(int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void shutdown() throws java.io.IOException;\n  public void sendFileDescriptors(java.io.FileDescriptor[], byte[], int, int) throws java.io.IOException;\n  public int receiveFileDescriptors(java.io.FileDescriptor[], byte[], int, int) throws java.io.IOException;\n  public int recvFileInputStreams(java.io.FileInputStream[], byte[], int, int) throws java.io.IOException;\n  public java.lang.String toString();\n  static int access$000(int, byte[], int, int) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.net.unix.DomainSocket, boolean) throws java.nio.channels.ClosedChannelException;\n  static int access$200(int) throws java.io.IOException;\n  static void access$300(int, byte[], int, int) throws java.io.IOException;\n  static int access$400(int, java.nio.ByteBuffer, int, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Delete$Expunge.class": "Compiled from \"Delete.java\"\nclass org.apache.hadoop.fs.shell.Delete {\n  org.apache.hadoop.fs.shell.Delete();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserMappingsProtocolService$BlockingStub.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/DelegationTokenRenewer$Renewable.class": "Compiled from \"DelegationTokenRenewer.java\"\npublic class org.apache.hadoop.fs.DelegationTokenRenewer extends java.lang.Thread {\n  public static long renewCycle;\n  protected int getRenewQueueLength();\n  public static synchronized org.apache.hadoop.fs.DelegationTokenRenewer getInstance();\n  static synchronized void reset();\n  public <T extends org/apache/hadoop/fs/FileSystem & org/apache/hadoop/fs/DelegationTokenRenewer$Renewable> org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction<T> addRenewAction(T);\n  public <T extends org/apache/hadoop/fs/FileSystem & org/apache/hadoop/fs/DelegationTokenRenewer$Renewable> void removeRenewAction(T) throws java.io.IOException;\n  public void run();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcResponseHeaderProtoOrBuilder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/CsvRecordInput$CsvIndex.class": "Compiled from \"CsvRecordInput.java\"\npublic class org.apache.hadoop.record.CsvRecordInput implements org.apache.hadoop.record.RecordInput {\n  public org.apache.hadoop.record.CsvRecordInput(java.io.InputStream);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  static java.io.PushbackReader access$000(org.apache.hadoop.record.CsvRecordInput);\n}\n", 
  "org/apache/hadoop/security/SaslRpcServer$FastSaslServerFactory.class": "Compiled from \"SaslRpcServer.java\"\npublic class org.apache.hadoop.security.SaslRpcServer {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String SASL_DEFAULT_REALM;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod authMethod;\n  public java.lang.String mechanism;\n  public java.lang.String protocol;\n  public java.lang.String serverId;\n  public org.apache.hadoop.security.SaslRpcServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod) throws java.io.IOException;\n  public javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection, java.util.Map<java.lang.String, ?>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void init(org.apache.hadoop.conf.Configuration);\n  static java.lang.String encodeIdentifier(byte[]);\n  static byte[] decodeIdentifier(java.lang.String);\n  public static <T extends org/apache/hadoop/security/token/TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  static char[] encodePassword(byte[]);\n  public static java.lang.String[] splitKerberosName(java.lang.String);\n  static javax.security.sasl.SaslServerFactory access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/JavaKeyStoreProvider.class": "Compiled from \"JavaKeyStoreProvider.java\"\npublic class org.apache.hadoop.crypto.key.JavaKeyStoreProvider extends org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String KEYSTORE_PASSWORD_FILE_KEY;\n  public static final java.lang.String KEYSTORE_PASSWORD_ENV_VAR;\n  public static final char[] KEYSTORE_PASSWORD_DEFAULT;\n  org.apache.hadoop.crypto.key.JavaKeyStoreProvider(org.apache.hadoop.crypto.key.JavaKeyStoreProvider);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.crypto.key.KeyProvider$KeyVersion innerSetKeyVersion(java.lang.String, java.lang.String, byte[], java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  protected void writeToNew(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean backupToOld(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.lang.String toString();\n  org.apache.hadoop.crypto.key.JavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.crypto.key.JavaKeyStoreProvider$1) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.class": "Compiled from \"MetricsCollectorImpl.java\"\npublic class org.apache.hadoop.metrics2.impl.MetricsCollectorImpl implements org.apache.hadoop.metrics2.MetricsCollector, java.lang.Iterable<org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl> {\n  public org.apache.hadoop.metrics2.impl.MetricsCollectorImpl();\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl addRecord(org.apache.hadoop.metrics2.MetricsInfo);\n  public org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl addRecord(java.lang.String);\n  public java.util.List<org.apache.hadoop.metrics2.impl.MetricsRecordImpl> getRecords();\n  public java.util.Iterator<org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl> iterator();\n  public void clear();\n  org.apache.hadoop.metrics2.impl.MetricsCollectorImpl setRecordFilter(org.apache.hadoop.metrics2.MetricsFilter);\n  org.apache.hadoop.metrics2.impl.MetricsCollectorImpl setMetricFilter(org.apache.hadoop.metrics2.MetricsFilter);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder addRecord(org.apache.hadoop.metrics2.MetricsInfo);\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder addRecord(java.lang.String);\n}\n", 
  "org/apache/hadoop/conf/ReconfigurationUtil$PropertyChange.class": "Compiled from \"ReconfigurationUtil.java\"\npublic class org.apache.hadoop.conf.ReconfigurationUtil {\n  public org.apache.hadoop.conf.ReconfigurationUtil();\n  public static java.util.Collection<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange> getChangedProperties(org.apache.hadoop.conf.Configuration, org.apache.hadoop.conf.Configuration);\n  public java.util.Collection<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange> parseChangedProperties(org.apache.hadoop.conf.Configuration, org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/security/SaslRpcServer$1.class": "Compiled from \"SaslRpcServer.java\"\npublic class org.apache.hadoop.security.SaslRpcServer {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String SASL_DEFAULT_REALM;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod authMethod;\n  public java.lang.String mechanism;\n  public java.lang.String protocol;\n  public java.lang.String serverId;\n  public org.apache.hadoop.security.SaslRpcServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod) throws java.io.IOException;\n  public javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection, java.util.Map<java.lang.String, ?>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void init(org.apache.hadoop.conf.Configuration);\n  static java.lang.String encodeIdentifier(byte[]);\n  static byte[] decodeIdentifier(java.lang.String);\n  public static <T extends org/apache/hadoop/security/token/TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  static char[] encodePassword(byte[]);\n  public static java.lang.String[] splitKerberosName(java.lang.String);\n  static javax.security.sasl.SaslServerFactory access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$1.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshProtocolService$1.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ListSpanReceiversResponseProtoOrBuilder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$1.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$ZKFCProtocolService$BlockingInterface.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Client.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolServerSideTranslatorPB.class": "Compiled from \"RefreshAuthorizationPolicyProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB implements org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB {\n  public org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB(org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol);\n  public org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto refreshServiceAcl(com.google.protobuf.RpcController, org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto) throws com.google.protobuf.ServiceException;\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpRequestLogAppender.class": "Compiled from \"HttpRequestLogAppender.java\"\npublic class org.apache.hadoop.http.HttpRequestLogAppender extends org.apache.log4j.AppenderSkeleton {\n  public org.apache.hadoop.http.HttpRequestLogAppender();\n  public void setRetainDays(int);\n  public int getRetainDays();\n  public void setFilename(java.lang.String);\n  public java.lang.String getFilename();\n  public void append(org.apache.log4j.spi.LoggingEvent);\n  public void close();\n  public boolean requiresLayout();\n}\n", 
  "org/apache/hadoop/record/CsvRecordInput$1.class": "Compiled from \"CsvRecordInput.java\"\npublic class org.apache.hadoop.record.CsvRecordInput implements org.apache.hadoop.record.RecordInput {\n  public org.apache.hadoop.record.CsvRecordInput(java.io.InputStream);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  static java.io.PushbackReader access$000(org.apache.hadoop.record.CsvRecordInput);\n}\n", 
  "org/apache/hadoop/io/BloomMapFile$Writer.class": "Compiled from \"BloomMapFile.java\"\npublic class org.apache.hadoop.io.BloomMapFile {\n  public static final java.lang.String BLOOM_FILE_NAME;\n  public static final int HASH_COUNT;\n  public org.apache.hadoop.io.BloomMapFile();\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  static byte[] access$000(org.apache.hadoop.io.DataOutputBuffer);\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MethodMetric$2.class": "Compiled from \"MethodMetric.java\"\nclass org.apache.hadoop.metrics2.lib.MethodMetric extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  org.apache.hadoop.metrics2.lib.MethodMetric(java.lang.Object, java.lang.reflect.Method, org.apache.hadoop.metrics2.MetricsInfo, org.apache.hadoop.metrics2.annotation.Metric$Type);\n  org.apache.hadoop.metrics2.lib.MutableMetric newCounter(java.lang.Class<?>);\n  static boolean isInt(java.lang.Class<?>);\n  static boolean isLong(java.lang.Class<?>);\n  static boolean isFloat(java.lang.Class<?>);\n  static boolean isDouble(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newGauge(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newTag(java.lang.Class<?>);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  static org.apache.hadoop.metrics2.MetricsInfo metricInfo(java.lang.reflect.Method);\n  static java.lang.String nameFrom(java.lang.reflect.Method);\n  static java.lang.Object access$000(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static java.lang.reflect.Method access$100(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.hadoop.metrics2.MetricsInfo access$200(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.commons.logging.Log access$300();\n  static {};\n}\n", 
  "org/apache/hadoop/conf/ReconfigurationException.class": "Compiled from \"ReconfigurationException.java\"\npublic class org.apache.hadoop.conf.ReconfigurationException extends java.lang.Exception {\n  public org.apache.hadoop.conf.ReconfigurationException();\n  public org.apache.hadoop.conf.ReconfigurationException(java.lang.String, java.lang.String, java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.conf.ReconfigurationException(java.lang.String, java.lang.String, java.lang.String);\n  public java.lang.String getProperty();\n  public java.lang.String getNewValue();\n  public java.lang.String getOldValue();\n}\n", 
  "org/apache/hadoop/metrics/ganglia/GangliaContext.class": "Compiled from \"GangliaContext.java\"\npublic class org.apache.hadoop.metrics.ganglia.GangliaContext extends org.apache.hadoop.metrics.spi.AbstractMetricsContext {\n  protected byte[] buffer;\n  protected int offset;\n  protected java.util.List<? extends java.net.SocketAddress> metricsServers;\n  protected java.net.DatagramSocket datagramSocket;\n  public org.apache.hadoop.metrics.ganglia.GangliaContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  public void close();\n  public void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord) throws java.io.IOException;\n  protected void emitMetric(java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  protected java.lang.String getUnits(java.lang.String);\n  protected int getSlope(java.lang.String);\n  protected int getTmax(java.lang.String);\n  protected int getDmax(java.lang.String);\n  protected void xdr_string(java.lang.String);\n  protected void xdr_int(int);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto$Builder.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/metrics/RetryCacheMetrics.class": "Compiled from \"RetryCacheMetrics.java\"\npublic class org.apache.hadoop.ipc.metrics.RetryCacheMetrics {\n  static final org.apache.commons.logging.Log LOG;\n  final org.apache.hadoop.metrics2.lib.MetricsRegistry registry;\n  final java.lang.String name;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong cacheHit;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong cacheCleared;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong cacheUpdated;\n  org.apache.hadoop.ipc.metrics.RetryCacheMetrics(org.apache.hadoop.ipc.RetryCache);\n  public java.lang.String getName();\n  public static org.apache.hadoop.ipc.metrics.RetryCacheMetrics create(org.apache.hadoop.ipc.RetryCache);\n  public void incrCacheHit();\n  public void incrCacheCleared();\n  public void incrCacheUpdated();\n  public long getCacheHit();\n  public long getCacheCleared();\n  public long getCacheUpdated();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RefreshResponse.class": "Compiled from \"RefreshResponse.java\"\npublic class org.apache.hadoop.ipc.RefreshResponse {\n  public static org.apache.hadoop.ipc.RefreshResponse successResponse();\n  public org.apache.hadoop.ipc.RefreshResponse(int, java.lang.String);\n  public void setSenderName(java.lang.String);\n  public java.lang.String getSenderName();\n  public int getReturnCode();\n  public void setReturnCode(int);\n  public void setMessage(java.lang.String);\n  public java.lang.String getMessage();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/fs/shell/CommandFormat$UnknownOptionException.class": "Compiled from \"CommandFormat.java\"\npublic class org.apache.hadoop.fs.shell.CommandFormat {\n  final int minPar;\n  final int maxPar;\n  final java.util.Map<java.lang.String, java.lang.Boolean> options;\n  boolean ignoreUnknownOpts;\n  public org.apache.hadoop.fs.shell.CommandFormat(java.lang.String, int, int, java.lang.String...);\n  public org.apache.hadoop.fs.shell.CommandFormat(int, int, java.lang.String...);\n  public java.util.List<java.lang.String> parse(java.lang.String[], int);\n  public void parse(java.util.List<java.lang.String>);\n  public boolean getOpt(java.lang.String);\n  public java.util.Set<java.lang.String> getOpts();\n}\n", 
  "org/apache/hadoop/conf/ReconfigurableBase.class": "Compiled from \"ReconfigurableBase.java\"\npublic abstract class org.apache.hadoop.conf.ReconfigurableBase extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.conf.Reconfigurable {\n  public org.apache.hadoop.conf.ReconfigurableBase();\n  public org.apache.hadoop.conf.ReconfigurableBase(org.apache.hadoop.conf.Configuration);\n  public void setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil);\n  public java.util.Collection<org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange> getChangedProperties(org.apache.hadoop.conf.Configuration, org.apache.hadoop.conf.Configuration);\n  public void startReconfigurationTask() throws java.io.IOException;\n  public org.apache.hadoop.conf.ReconfigurationTaskStatus getReconfigurationTaskStatus();\n  public void shutdownReconfigurationTask();\n  public final java.lang.String reconfigureProperty(java.lang.String, java.lang.String) throws org.apache.hadoop.conf.ReconfigurationException;\n  public abstract java.util.Collection<java.lang.String> getReconfigurableProperties();\n  public boolean isPropertyReconfigurable(java.lang.String);\n  protected abstract void reconfigurePropertyImpl(java.lang.String, java.lang.String) throws org.apache.hadoop.conf.ReconfigurationException;\n  static org.apache.commons.logging.Log access$000();\n  static java.lang.Object access$100(org.apache.hadoop.conf.ReconfigurableBase);\n  static long access$202(org.apache.hadoop.conf.ReconfigurableBase, long);\n  static java.util.Map access$302(org.apache.hadoop.conf.ReconfigurableBase, java.util.Map);\n  static java.lang.Thread access$402(org.apache.hadoop.conf.ReconfigurableBase, java.lang.Thread);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/PathData$FileTypeRequirement.class": "Compiled from \"PathData.java\"\npublic class org.apache.hadoop.fs.shell.PathData implements java.lang.Comparable<org.apache.hadoop.fs.shell.PathData> {\n  protected final java.net.URI uri;\n  public final org.apache.hadoop.fs.FileSystem fs;\n  public final org.apache.hadoop.fs.Path path;\n  public org.apache.hadoop.fs.FileStatus stat;\n  public boolean exists;\n  public org.apache.hadoop.fs.shell.PathData(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus refreshStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData suffix(java.lang.String) throws java.io.IOException;\n  public boolean parentExists() throws java.io.IOException;\n  public boolean representsDirectory();\n  public org.apache.hadoop.fs.shell.PathData[] getDirectoryContents() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData getPathDataForChild(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  public static org.apache.hadoop.fs.shell.PathData[] expandAsGlob(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String toString();\n  public java.io.File toFile();\n  public int compareTo(org.apache.hadoop.fs.shell.PathData);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/CopyCommands$Merge.class": "Compiled from \"CopyCommands.java\"\nclass org.apache.hadoop.fs.shell.CopyCommands {\n  org.apache.hadoop.fs.shell.CopyCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcRequestHeaderProto$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/FsAction.class": "Compiled from \"FsAction.java\"\npublic final class org.apache.hadoop.fs.permission.FsAction extends java.lang.Enum<org.apache.hadoop.fs.permission.FsAction> {\n  public static final org.apache.hadoop.fs.permission.FsAction NONE;\n  public static final org.apache.hadoop.fs.permission.FsAction EXECUTE;\n  public static final org.apache.hadoop.fs.permission.FsAction WRITE;\n  public static final org.apache.hadoop.fs.permission.FsAction WRITE_EXECUTE;\n  public static final org.apache.hadoop.fs.permission.FsAction READ;\n  public static final org.apache.hadoop.fs.permission.FsAction READ_EXECUTE;\n  public static final org.apache.hadoop.fs.permission.FsAction READ_WRITE;\n  public static final org.apache.hadoop.fs.permission.FsAction ALL;\n  public final java.lang.String SYMBOL;\n  public static org.apache.hadoop.fs.permission.FsAction[] values();\n  public static org.apache.hadoop.fs.permission.FsAction valueOf(java.lang.String);\n  public boolean implies(org.apache.hadoop.fs.permission.FsAction);\n  public org.apache.hadoop.fs.permission.FsAction and(org.apache.hadoop.fs.permission.FsAction);\n  public org.apache.hadoop.fs.permission.FsAction or(org.apache.hadoop.fs.permission.FsAction);\n  public org.apache.hadoop.fs.permission.FsAction not();\n  public static org.apache.hadoop.fs.permission.FsAction getFsAction(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/util/PureJavaCrc32C.class": "Compiled from \"PureJavaCrc32C.java\"\npublic class org.apache.hadoop.util.PureJavaCrc32C implements java.util.zip.Checksum {\n  public org.apache.hadoop.util.PureJavaCrc32C();\n  public long getValue();\n  public void reset();\n  public void update(byte[], int, int);\n  public final void update(int);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Chunk$SingleChunkEncoder.class": "Compiled from \"Chunk.java\"\nfinal class org.apache.hadoop.io.file.tfile.Chunk {\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$Reader$Scanner$Entry.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$RetryUpToMaximumTimeWithFixedSleep.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$ValueBytes.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/GzipCodec$GzipZlibDecompressor.class": "Compiled from \"GzipCodec.java\"\npublic class org.apache.hadoop.io.compress.GzipCodec extends org.apache.hadoop.io.compress.DefaultCodec {\n  public org.apache.hadoop.io.compress.GzipCodec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.DirectDecompressor createDirectDecompressor();\n  public java.lang.String getDefaultExtension();\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Shell$ExitCodeException.class": "Compiled from \"Shell.java\"\npublic abstract class org.apache.hadoop.util.Shell {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int WINDOWS_MAX_SHELL_LENGHT;\n  public static final java.lang.String USER_NAME_COMMAND;\n  public static final java.lang.Object WindowsProcessLaunchLock;\n  public static final org.apache.hadoop.util.Shell$OSType osType;\n  public static final boolean WINDOWS;\n  public static final boolean SOLARIS;\n  public static final boolean MAC;\n  public static final boolean FREEBSD;\n  public static final boolean LINUX;\n  public static final boolean OTHER;\n  public static final boolean PPC_64;\n  public static final java.lang.String SET_PERMISSION_COMMAND;\n  public static final java.lang.String SET_OWNER_COMMAND;\n  public static final java.lang.String SET_GROUP_COMMAND;\n  public static final java.lang.String LINK_COMMAND;\n  public static final java.lang.String READ_LINK_COMMAND;\n  protected long timeOutInterval;\n  public static final java.lang.String WINUTILS;\n  public static final boolean isSetsidAvailable;\n  public static final java.lang.String TOKEN_SEPARATOR_REGEX;\n  public static boolean isJava7OrAbove();\n  public static void checkWindowsCommandLineLength(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String[] getGroupsCommand();\n  public static java.lang.String[] getGroupsForUserCommand(java.lang.String);\n  public static java.lang.String[] getUsersForNetgroupCommand(java.lang.String);\n  public static java.lang.String[] getGetPermissionCommand();\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean);\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean, java.lang.String);\n  public static java.lang.String[] getSetOwnerCommand(java.lang.String);\n  public static java.lang.String[] getSymlinkCommand(java.lang.String, java.lang.String);\n  public static java.lang.String[] getReadlinkCommand(java.lang.String);\n  public static java.lang.String[] getCheckProcessIsAliveCommand(java.lang.String);\n  public static java.lang.String[] getSignalKillCommand(int, java.lang.String);\n  public static java.lang.String getEnvironmentVariableRegex();\n  public static java.io.File appendScriptExtension(java.io.File, java.lang.String);\n  public static java.lang.String appendScriptExtension(java.lang.String);\n  public static java.lang.String[] getRunScriptCommand(java.io.File);\n  public static final java.lang.String getHadoopHome() throws java.io.IOException;\n  public static final java.lang.String getQualifiedBinPath(java.lang.String) throws java.io.IOException;\n  public static final java.lang.String getWinUtilsPath();\n  public org.apache.hadoop.util.Shell();\n  public org.apache.hadoop.util.Shell(long);\n  public org.apache.hadoop.util.Shell(long, boolean);\n  protected void setEnvironment(java.util.Map<java.lang.String, java.lang.String>);\n  protected void setWorkingDirectory(java.io.File);\n  protected void run() throws java.io.IOException;\n  protected abstract java.lang.String[] getExecString();\n  protected abstract void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public java.lang.String getEnvironment(java.lang.String);\n  public java.lang.Process getProcess();\n  public int getExitCode();\n  public boolean isTimedOut();\n  public static java.lang.String execCommand(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String[], long) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String...) throws java.io.IOException;\n  static java.util.concurrent.atomic.AtomicBoolean access$000(org.apache.hadoop.util.Shell);\n  static void access$100(org.apache.hadoop.util.Shell);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer2$QuotingInputFilter$RequestQuoter.class": "Compiled from \"HttpServer2.java\"\npublic final class org.apache.hadoop.http.HttpServer2 implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  public static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean);\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public java.net.InetSocketAddress getConnectorAddress(int);\n  public void setThreads(int, int);\n  public void start() throws java.io.IOException;\n  void openListeners() throws java.lang.Exception;\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  org.apache.hadoop.http.HttpServer2(org.apache.hadoop.http.HttpServer2$Builder, org.apache.hadoop.http.HttpServer2$1) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.http.HttpServer2, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.http.HttpServer2, org.mortbay.jetty.Connector);\n  static void access$300(org.apache.hadoop.http.HttpServer2);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileAlreadyExistsException.class": "Compiled from \"FileAlreadyExistsException.java\"\npublic class org.apache.hadoop.fs.FileAlreadyExistsException extends java.io.IOException {\n  public org.apache.hadoop.fs.FileAlreadyExistsException();\n  public org.apache.hadoop.fs.FileAlreadyExistsException(java.lang.String);\n}\n", 
  "org/apache/hadoop/metrics/jvm/EventCounter.class": "Compiled from \"EventCounter.java\"\npublic class org.apache.hadoop.metrics.jvm.EventCounter extends org.apache.hadoop.log.metrics.EventCounter {\n  public org.apache.hadoop.metrics.jvm.EventCounter();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Display$Text.class": "Compiled from \"Display.java\"\nclass org.apache.hadoop.fs.shell.Display extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.Display();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/record/compiler/JString$CppString.class": "Compiled from \"JString.java\"\npublic class org.apache.hadoop.record.compiler.JString extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JString();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/conf/Configuration$ParsedTimeDuration$2.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/IntWritable$Comparator.class": "Compiled from \"IntWritable.java\"\npublic class org.apache.hadoop.io.IntWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.IntWritable> {\n  public org.apache.hadoop.io.IntWritable();\n  public org.apache.hadoop.io.IntWritable(int);\n  public void set(int);\n  public int get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.IntWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/MoveCommands.class": "Compiled from \"MoveCommands.java\"\nclass org.apache.hadoop.fs.shell.MoveCommands {\n  org.apache.hadoop.fs.shell.MoveCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/util/bloom/RetouchedBloomFilter.class": "Compiled from \"RetouchedBloomFilter.java\"\npublic final class org.apache.hadoop.util.bloom.RetouchedBloomFilter extends org.apache.hadoop.util.bloom.BloomFilter implements org.apache.hadoop.util.bloom.RemoveScheme {\n  java.util.List<org.apache.hadoop.util.bloom.Key>[] fpVector;\n  java.util.List<org.apache.hadoop.util.bloom.Key>[] keyVector;\n  double[] ratio;\n  public org.apache.hadoop.util.bloom.RetouchedBloomFilter();\n  public org.apache.hadoop.util.bloom.RetouchedBloomFilter(int, int, int);\n  public void add(org.apache.hadoop.util.bloom.Key);\n  public void addFalsePositive(org.apache.hadoop.util.bloom.Key);\n  public void addFalsePositive(java.util.Collection<org.apache.hadoop.util.bloom.Key>);\n  public void addFalsePositive(java.util.List<org.apache.hadoop.util.bloom.Key>);\n  public void addFalsePositive(org.apache.hadoop.util.bloom.Key[]);\n  public void selectiveClearing(org.apache.hadoop.util.bloom.Key, short);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/authorize/ProxyUsers.class": "Compiled from \"ProxyUsers.java\"\npublic class org.apache.hadoop.security.authorize.ProxyUsers {\n  public static final java.lang.String CONF_HADOOP_PROXYUSER;\n  public org.apache.hadoop.security.authorize.ProxyUsers();\n  public static void refreshSuperUserGroupsConfiguration();\n  public static void refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public static void refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration);\n  public static void authorize(org.apache.hadoop.security.UserGroupInformation, java.lang.String) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  public static void authorize(org.apache.hadoop.security.UserGroupInformation, java.lang.String, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  public static org.apache.hadoop.security.authorize.DefaultImpersonationProvider getDefaultImpersonationProvider();\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ConfigPair$Builder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/snappy/SnappyDecompressor.class": "Compiled from \"SnappyDecompressor.java\"\npublic class org.apache.hadoop.io.compress.snappy.SnappyDecompressor implements org.apache.hadoop.io.compress.Decompressor {\n  static final boolean $assertionsDisabled;\n  public static boolean isNativeCodeLoaded();\n  public org.apache.hadoop.io.compress.snappy.SnappyDecompressor(int);\n  public org.apache.hadoop.io.compress.snappy.SnappyDecompressor();\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public boolean needsDictionary();\n  public boolean finished();\n  public int decompress(byte[], int, int) throws java.io.IOException;\n  public int getRemaining();\n  public void reset();\n  public void end();\n  int decompressDirect(java.nio.ByteBuffer, java.nio.ByteBuffer) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/MapFile$Writer$KeyClassOption.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/security/Groups$TimerToTickerAdapter.class": "Compiled from \"Groups.java\"\npublic class org.apache.hadoop.security.Groups {\n  public org.apache.hadoop.security.Groups(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.security.Groups(org.apache.hadoop.conf.Configuration, org.apache.hadoop.util.Timer);\n  java.util.Set<java.lang.String> getNegativeCache();\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void refresh();\n  public void cacheGroupsAdd(java.util.List<java.lang.String>);\n  public static org.apache.hadoop.security.Groups getUserToGroupsMappingService();\n  public static synchronized org.apache.hadoop.security.Groups getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration);\n  public static synchronized org.apache.hadoop.security.Groups getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration);\n  static boolean access$100(org.apache.hadoop.security.Groups);\n  static java.util.Set access$200(org.apache.hadoop.security.Groups);\n  static java.io.IOException access$300(org.apache.hadoop.security.Groups, java.lang.String);\n  static org.apache.hadoop.util.Timer access$400(org.apache.hadoop.security.Groups);\n  static org.apache.hadoop.security.GroupMappingServiceProvider access$500(org.apache.hadoop.security.Groups);\n  static long access$600(org.apache.hadoop.security.Groups);\n  static org.apache.commons.logging.Log access$700();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyShell$RollCommand.class": "Compiled from \"KeyShell.java\"\npublic class org.apache.hadoop.crypto.key.KeyShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.crypto.key.KeyShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.crypto.key.KeyShell);\n  static boolean access$300(org.apache.hadoop.crypto.key.KeyShell);\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenManager$DelegationTokenSecretManager.class": "Compiled from \"DelegationTokenManager.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenManager {\n  public static final java.lang.String ENABLE_ZK_KEY;\n  public static final java.lang.String PREFIX;\n  public static final java.lang.String UPDATE_INTERVAL;\n  public static final long UPDATE_INTERVAL_DEFAULT;\n  public static final java.lang.String MAX_LIFETIME;\n  public static final long MAX_LIFETIME_DEFAULT;\n  public static final java.lang.String RENEW_INTERVAL;\n  public static final long RENEW_INTERVAL_DEFAULT;\n  public static final java.lang.String REMOVAL_SCAN_INTERVAL;\n  public static final long REMOVAL_SCAN_INTERVAL_DEFAULT;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.Text);\n  public void setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  public void init();\n  public void destroy();\n  public org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> createToken(org.apache.hadoop.security.UserGroupInformation, java.lang.String);\n  public long renewToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public void cancelToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.UserGroupInformation verifyToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>) throws java.io.IOException;\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager getDelegationTokenSecretManager();\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier access$000(org.apache.hadoop.security.token.Token, org.apache.hadoop.io.Text) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$AddSpanReceiverRequestProtoOrBuilder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Progressable.class": "Compiled from \"Progressable.java\"\npublic interface org.apache.hadoop.util.Progressable {\n  public abstract void progress();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSystemImpl$2.class": "Compiled from \"MetricsSystemImpl.java\"\npublic class org.apache.hadoop.metrics2.impl.MetricsSystemImpl extends org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSource {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String MS_NAME;\n  static final java.lang.String MS_STATS_NAME;\n  static final java.lang.String MS_STATS_DESC;\n  static final java.lang.String MS_CONTROL_NAME;\n  static final java.lang.String MS_INIT_MODE_KEY;\n  org.apache.hadoop.metrics2.lib.MutableStat snapshotStat;\n  org.apache.hadoop.metrics2.lib.MutableStat publishStat;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong droppedPubAll;\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl(java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl();\n  public synchronized org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public synchronized void unregisterSource(java.lang.String);\n  synchronized void registerSource(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource);\n  public synchronized <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  synchronized void registerSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink);\n  public synchronized void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public synchronized void startMetricsMBeans();\n  public synchronized void stopMetricsMBeans();\n  public synchronized java.lang.String currentConfig();\n  synchronized void onTimerEvent();\n  public synchronized void publishMetricsNow();\n  synchronized org.apache.hadoop.metrics2.impl.MetricsBuffer sampleMetrics();\n  synchronized void publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, boolean);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static java.lang.String getHostname();\n  public synchronized void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n  public synchronized boolean shutdown();\n  public org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter getSourceAdapter(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/authorize/PolicyProvider$1.class": "Compiled from \"PolicyProvider.java\"\npublic abstract class org.apache.hadoop.security.authorize.PolicyProvider {\n  public static final java.lang.String POLICY_PROVIDER_CONFIG;\n  public static final org.apache.hadoop.security.authorize.PolicyProvider DEFAULT_POLICY_PROVIDER;\n  public org.apache.hadoop.security.authorize.PolicyProvider();\n  public abstract org.apache.hadoop.security.authorize.Service[] getServices();\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/JavaSerialization.class": "Compiled from \"JavaSerialization.java\"\npublic class org.apache.hadoop.io.serializer.JavaSerialization implements org.apache.hadoop.io.serializer.Serialization<java.io.Serializable> {\n  public org.apache.hadoop.io.serializer.JavaSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.hadoop.io.serializer.Deserializer<java.io.Serializable> getDeserializer(java.lang.Class<java.io.Serializable>);\n  public org.apache.hadoop.io.serializer.Serializer<java.io.Serializable> getSerializer(java.lang.Class<java.io.Serializable>);\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto$Builder.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolVersionsRequestProto$1.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto$Builder.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HAAdmin.class": "Compiled from \"HAAdmin.java\"\npublic abstract class org.apache.hadoop.ha.HAAdmin extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  protected static final java.util.Map<java.lang.String, org.apache.hadoop.ha.HAAdmin$UsageInfo> USAGE;\n  protected java.io.PrintStream errOut;\n  protected java.io.PrintStream out;\n  protected org.apache.hadoop.ha.HAAdmin();\n  protected org.apache.hadoop.ha.HAAdmin(org.apache.hadoop.conf.Configuration);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget resolveTarget(java.lang.String);\n  protected java.util.Collection<java.lang.String> getTargetIds(java.lang.String);\n  protected java.lang.String getUsageString();\n  protected void printUsage(java.io.PrintStream);\n  protected java.lang.String getServiceAddr(java.lang.String);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int runCmd(java.lang.String[]) throws java.lang.Exception;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.class": "Compiled from \"CryptoFSDataOutputStream.java\"\npublic class org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream extends org.apache.hadoop.fs.FSDataOutputStream {\n  public org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream(org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.crypto.CryptoCodec, int, byte[], byte[]) throws java.io.IOException;\n  public org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream(org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.crypto.CryptoCodec, byte[], byte[]) throws java.io.IOException;\n  public long getPos() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/FsConstants.class": "Compiled from \"FsConstants.java\"\npublic interface org.apache.hadoop.fs.FsConstants {\n  public static final java.net.URI LOCAL_FS_URI;\n  public static final java.lang.String FTP_SCHEME;\n  public static final int MAX_PATH_LINKS;\n  public static final java.net.URI VIEWFS_URI;\n  public static final java.lang.String VIEWFS_SCHEME;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/FsPermission$ImmutableFsPermission.class": "Compiled from \"FsPermission.java\"\npublic class org.apache.hadoop.fs.permission.FsPermission implements org.apache.hadoop.io.Writable {\n  static final org.apache.hadoop.io.WritableFactory FACTORY;\n  public static final int MAX_PERMISSION_LENGTH;\n  public static final java.lang.String DEPRECATED_UMASK_LABEL;\n  public static final java.lang.String UMASK_LABEL;\n  public static final int DEFAULT_UMASK;\n  public static org.apache.hadoop.fs.permission.FsPermission createImmutable(short);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, boolean);\n  public org.apache.hadoop.fs.permission.FsPermission(short);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.permission.FsPermission(java.lang.String);\n  public org.apache.hadoop.fs.permission.FsAction getUserAction();\n  public org.apache.hadoop.fs.permission.FsAction getGroupAction();\n  public org.apache.hadoop.fs.permission.FsAction getOtherAction();\n  public void fromShort(short);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static org.apache.hadoop.fs.permission.FsPermission read(java.io.DataInput) throws java.io.IOException;\n  public short toShort();\n  public short toExtendedShort();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.permission.FsPermission applyUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public static org.apache.hadoop.fs.permission.FsPermission getUMask(org.apache.hadoop.conf.Configuration);\n  public boolean getStickyBit();\n  public boolean getAclBit();\n  public boolean getEncryptedBit();\n  public static void setUMask(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.permission.FsPermission);\n  public static org.apache.hadoop.fs.permission.FsPermission getDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getDirDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getFileDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getCachePoolDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission valueOf(java.lang.String);\n  org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsPermission$1);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HtmlQuoting.class": "Compiled from \"HtmlQuoting.java\"\npublic class org.apache.hadoop.http.HtmlQuoting {\n  public org.apache.hadoop.http.HtmlQuoting();\n  public static boolean needsQuoting(byte[], int, int);\n  public static boolean needsQuoting(java.lang.String);\n  public static void quoteHtmlChars(java.io.OutputStream, byte[], int, int) throws java.io.IOException;\n  public static java.lang.String quoteHtmlChars(java.lang.String);\n  public static java.io.OutputStream quoteOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public static java.lang.String unquoteHtmlChars(java.lang.String);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static {};\n}\n", 
  "org/apache/hadoop/io/LongWritable$Comparator.class": "Compiled from \"LongWritable.java\"\npublic class org.apache.hadoop.io.LongWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.LongWritable> {\n  public org.apache.hadoop.io.LongWritable();\n  public org.apache.hadoop.io.LongWritable(long);\n  public void set(long);\n  public long get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.LongWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserMappingsProtocolService$Stub.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyShell$ListCommand.class": "Compiled from \"KeyShell.java\"\npublic class org.apache.hadoop.crypto.key.KeyShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.crypto.key.KeyShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.crypto.key.KeyShell);\n  static boolean access$300(org.apache.hadoop.crypto.key.KeyShell);\n}\n", 
  "org/apache/hadoop/net/SocketOutputStream$Writer.class": "Compiled from \"SocketOutputStream.java\"\npublic class org.apache.hadoop.net.SocketOutputStream extends java.io.OutputStream implements java.nio.channels.WritableByteChannel {\n  public org.apache.hadoop.net.SocketOutputStream(java.nio.channels.WritableByteChannel, long) throws java.io.IOException;\n  public org.apache.hadoop.net.SocketOutputStream(java.net.Socket, long) throws java.io.IOException;\n  public void write(int) throws java.io.IOException;\n  public void write(byte[], int, int) throws java.io.IOException;\n  public synchronized void close() throws java.io.IOException;\n  public java.nio.channels.WritableByteChannel getChannel();\n  public boolean isOpen();\n  public int write(java.nio.ByteBuffer) throws java.io.IOException;\n  public void waitForWritable() throws java.io.IOException;\n  public void transferToFully(java.nio.channels.FileChannel, long, int, org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.LongWritable) throws java.io.IOException;\n  public void transferToFully(java.nio.channels.FileChannel, long, int) throws java.io.IOException;\n  public void setTimeout(int);\n}\n", 
  "org/apache/hadoop/security/SecurityInfo.class": "Compiled from \"SecurityInfo.java\"\npublic abstract class org.apache.hadoop.security.SecurityInfo {\n  public org.apache.hadoop.security.SecurityInfo();\n  public abstract org.apache.hadoop.security.KerberosInfo getKerberosInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public abstract org.apache.hadoop.security.token.TokenInfo getTokenInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/service/ServiceStateException.class": "Compiled from \"ServiceStateException.java\"\npublic class org.apache.hadoop.service.ServiceStateException extends java.lang.RuntimeException {\n  public org.apache.hadoop.service.ServiceStateException(java.lang.String);\n  public org.apache.hadoop.service.ServiceStateException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.service.ServiceStateException(java.lang.Throwable);\n  public static java.lang.RuntimeException convert(java.lang.Throwable);\n  public static java.lang.RuntimeException convert(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/fs/RawLocalFileSystem$DeprecatedRawLocalFileStatus.class": "Compiled from \"RawLocalFileSystem.java\"\npublic class org.apache.hadoop.fs.RawLocalFileSystem extends org.apache.hadoop.fs.FileSystem {\n  static final java.net.URI NAME;\n  public static void useStatIfAvailable();\n  public org.apache.hadoop.fs.RawLocalFileSystem();\n  public java.io.File pathToFile(org.apache.hadoop.fs.Path);\n  public java.net.URI getUri();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStream(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStreamWithMode(org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean mkOneDir(java.io.File) throws java.io.IOException;\n  protected boolean mkOneDirWithMode(org.apache.hadoop.fs.Path, java.io.File, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProvider.class": "Compiled from \"KeyProvider.java\"\npublic abstract class org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String DEFAULT_CIPHER_NAME;\n  public static final java.lang.String DEFAULT_CIPHER;\n  public static final java.lang.String DEFAULT_BITLENGTH_NAME;\n  public static final int DEFAULT_BITLENGTH;\n  public org.apache.hadoop.crypto.key.KeyProvider(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public static org.apache.hadoop.crypto.key.KeyProvider$Options options(org.apache.hadoop.conf.Configuration);\n  public boolean isTransient();\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  protected byte[] generateKey(int, java.lang.String) throws java.security.NoSuchAlgorithmException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public abstract void deleteKey(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public abstract void flush() throws java.io.IOException;\n  public static java.lang.String getBaseName(java.lang.String) throws java.io.IOException;\n  protected static java.lang.String buildVersionName(java.lang.String, int);\n  public static org.apache.hadoop.crypto.key.KeyProvider findProvider(java.util.List<org.apache.hadoop.crypto.key.KeyProvider>, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/ProtocolTranslator.class": "Compiled from \"ProtocolTranslator.java\"\npublic interface org.apache.hadoop.ipc.ProtocolTranslator {\n  public abstract java.lang.Object getUnderlyingProxyObject();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/GenericOptionsParser.class": "Compiled from \"GenericOptionsParser.java\"\npublic class org.apache.hadoop.util.GenericOptionsParser {\n  public org.apache.hadoop.util.GenericOptionsParser(org.apache.commons.cli.Options, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.util.GenericOptionsParser(java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.util.GenericOptionsParser(org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.util.GenericOptionsParser(org.apache.hadoop.conf.Configuration, org.apache.commons.cli.Options, java.lang.String[]) throws java.io.IOException;\n  public java.lang.String[] getRemainingArgs();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.commons.cli.CommandLine getCommandLine();\n  public static java.net.URL[] getLibJars(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void printGenericCommandUsage(java.io.PrintStream);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$GetDelegationTokenRequestProto.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Writer$WBlockState.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.fs.shell.package-info {\n}\n", 
  "org/apache/hadoop/record/compiler/JDouble$JavaDouble.class": "Compiled from \"JDouble.java\"\npublic class org.apache.hadoop.record.compiler.JDouble extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JDouble();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/viewfs/InodeTree$INodeDir.class": "Compiled from \"InodeTree.java\"\nabstract class org.apache.hadoop.fs.viewfs.InodeTree<T> {\n  static final org.apache.hadoop.fs.Path SlashPath;\n  final org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T> root;\n  final java.lang.String homedirPrefix;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> mountPoints;\n  static final boolean $assertionsDisabled;\n  static java.lang.String[] breakIntoPathComponents(java.lang.String);\n  protected abstract T getTargetFileSystem(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, java.io.IOException;\n  protected abstract T getTargetFileSystem(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T>) throws java.net.URISyntaxException;\n  protected abstract T getTargetFileSystem(java.net.URI[]) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException;\n  protected org.apache.hadoop.fs.viewfs.InodeTree(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.IOException;\n  org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult<T> resolve(java.lang.String, boolean) throws java.io.FileNotFoundException;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> getMountPoints();\n  java.lang.String getHomeDirPrefixValue();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/AclStatus$Builder.class": "Compiled from \"AclStatus.java\"\npublic class org.apache.hadoop.fs.permission.AclStatus {\n  public java.lang.String getOwner();\n  public java.lang.String getGroup();\n  public boolean isStickyBit();\n  public java.util.List<org.apache.hadoop.fs.permission.AclEntry> getEntries();\n  public org.apache.hadoop.fs.permission.FsPermission getPermission();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.permission.FsAction getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry);\n  public org.apache.hadoop.fs.permission.FsAction getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry, org.apache.hadoop.fs.permission.FsPermission) throws java.lang.IllegalArgumentException;\n  org.apache.hadoop.fs.permission.AclStatus(java.lang.String, java.lang.String, boolean, java.lang.Iterable, org.apache.hadoop.fs.permission.FsPermission, org.apache.hadoop.fs.permission.AclStatus$1);\n}\n", 
  "org/apache/hadoop/ha/HAAdmin$UsageInfo.class": "Compiled from \"HAAdmin.java\"\npublic abstract class org.apache.hadoop.ha.HAAdmin extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  protected static final java.util.Map<java.lang.String, org.apache.hadoop.ha.HAAdmin$UsageInfo> USAGE;\n  protected java.io.PrintStream errOut;\n  protected java.io.PrintStream out;\n  protected org.apache.hadoop.ha.HAAdmin();\n  protected org.apache.hadoop.ha.HAAdmin(org.apache.hadoop.conf.Configuration);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget resolveTarget(java.lang.String);\n  protected java.util.Collection<java.lang.String> getTargetIds(java.lang.String);\n  protected java.lang.String getUsageString();\n  protected void printUsage(java.io.PrintStream);\n  protected java.lang.String getServiceAddr(java.lang.String);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int runCmd(java.lang.String[]) throws java.lang.Exception;\n  static {};\n}\n", 
  "org/apache/hadoop/util/IPList.class": "Compiled from \"IPList.java\"\npublic interface org.apache.hadoop.util.IPList {\n  public abstract boolean isIn(java.lang.String);\n}\n", 
  "org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider$1.class": "Compiled from \"LocalJavaKeyStoreProvider.java\"\npublic final class org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider extends org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider {\n  public static final java.lang.String SCHEME_NAME;\n  protected java.lang.String getSchemeName();\n  protected java.io.OutputStream getOutputStreamForKeystore() throws java.io.IOException;\n  protected boolean keystoreExists() throws java.io.IOException;\n  protected java.io.InputStream getInputStreamForFile() throws java.io.IOException;\n  protected void createPermissions(java.lang.String) throws java.io.IOException;\n  protected void stashOriginalFilePermissions() throws java.io.IOException;\n  protected void initFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.class": "Compiled from \"SimpleBufferedOutputStream.java\"\nclass org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream extends java.io.FilterOutputStream {\n  protected byte[] buf;\n  protected int count;\n  public org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream(java.io.OutputStream, byte[]);\n  public void write(int) throws java.io.IOException;\n  public void write(byte[], int, int) throws java.io.IOException;\n  public synchronized void flush() throws java.io.IOException;\n  public int size();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolSignatureProto$1.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider$EncryptedQueueRefiller.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/metrics2/util/MBeans.class": "Compiled from \"MBeans.java\"\npublic class org.apache.hadoop.metrics2.util.MBeans {\n  public org.apache.hadoop.metrics2.util.MBeans();\n  public static javax.management.ObjectName register(java.lang.String, java.lang.String, java.lang.Object);\n  public static void unregister(javax.management.ObjectName);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension$1.class": "Compiled from \"KeyProviderDelegationTokenExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension> {\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public static org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  static {};\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation$TestingGroups.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$TokenProto$Builder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$RpcResponseWrapper.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceUtils$1.class": "Compiled from \"TraceUtils.java\"\npublic class org.apache.hadoop.tracing.TraceUtils {\n  public org.apache.hadoop.tracing.TraceUtils();\n  public static org.apache.htrace.HTraceConfiguration wrapHadoopConf(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public static org.apache.htrace.HTraceConfiguration wrapHadoopConf(java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.tracing.SpanReceiverInfo$ConfigurationPair>);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto$1.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Sorter$LinkedSegmentsDescriptor.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/util/ZKUtil$ZKAuthInfo.class": "Compiled from \"ZKUtil.java\"\npublic class org.apache.hadoop.util.ZKUtil {\n  public org.apache.hadoop.util.ZKUtil();\n  public static int removeSpecificPerms(int, int);\n  public static java.util.List<org.apache.zookeeper.data.ACL> parseACLs(java.lang.String) throws org.apache.hadoop.util.ZKUtil$BadAclFormatException;\n  public static java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo> parseAuth(java.lang.String) throws org.apache.hadoop.util.ZKUtil$BadAuthFormatException;\n  public static java.lang.String resolveConfIndirection(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/FsShellPermissions.class": "Compiled from \"FsShellPermissions.java\"\npublic class org.apache.hadoop.fs.FsShellPermissions extends org.apache.hadoop.fs.shell.FsCommand {\n  static org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.fs.FsShellPermissions();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  static java.lang.String access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticator$1.class": "Compiled from \"PseudoDelegationTokenAuthenticator.java\"\npublic class org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator {\n  public org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator();\n}\n", 
  "org/apache/hadoop/io/CompressedWritable.class": "Compiled from \"CompressedWritable.java\"\npublic abstract class org.apache.hadoop.io.CompressedWritable implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.io.CompressedWritable();\n  public final void readFields(java.io.DataInput) throws java.io.IOException;\n  protected void ensureInflated();\n  protected abstract void readFieldsCompressed(java.io.DataInput) throws java.io.IOException;\n  public final void write(java.io.DataOutput) throws java.io.IOException;\n  protected abstract void writeCompressed(java.io.DataOutput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/ReflectionUtils.class": "Compiled from \"ReflectionUtils.java\"\npublic class org.apache.hadoop.util.ReflectionUtils {\n  public org.apache.hadoop.util.ReflectionUtils();\n  public static void setConf(java.lang.Object, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T newInstance(java.lang.Class<T>, org.apache.hadoop.conf.Configuration);\n  public static void setContentionTracing(boolean);\n  public static synchronized void printThreadInfo(java.io.PrintStream, java.lang.String);\n  public static void logThreadInfo(org.apache.commons.logging.Log, java.lang.String, long);\n  public static <T extends java/lang/Object> java.lang.Class<T> getClass(T);\n  static void clearCache();\n  static int getCacheSize();\n  public static <T extends java/lang/Object> T copy(org.apache.hadoop.conf.Configuration, T, T) throws java.io.IOException;\n  public static void cloneWritableInto(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable) throws java.io.IOException;\n  public static java.util.List<java.lang.reflect.Field> getDeclaredFieldsIncludingInherited(java.lang.Class<?>);\n  public static java.util.List<java.lang.reflect.Method> getDeclaredMethodsIncludingInherited(java.lang.Class<?>);\n  static {};\n}\n", 
  "org/apache/hadoop/util/StringUtils$TraditionalBinaryPrefix.class": "Compiled from \"StringUtils.java\"\npublic class org.apache.hadoop.util.StringUtils {\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  public static final java.util.regex.Pattern SHELL_ENV_VAR_PATTERN;\n  public static final java.util.regex.Pattern WIN_ENV_VAR_PATTERN;\n  public static final java.util.regex.Pattern ENV_VAR_PATTERN;\n  public static final java.lang.String[] emptyStringArray;\n  public static final char COMMA;\n  public static final java.lang.String COMMA_STR;\n  public static final char ESCAPE_CHAR;\n  public org.apache.hadoop.util.StringUtils();\n  public static java.lang.String stringifyException(java.lang.Throwable);\n  public static java.lang.String simpleHostname(java.lang.String);\n  public static java.lang.String humanReadableInt(long);\n  public static java.lang.String format(java.lang.String, java.lang.Object...);\n  public static java.lang.String formatPercent(double, int);\n  public static java.lang.String arrayToString(java.lang.String[]);\n  public static java.lang.String byteToHexString(byte[], int, int);\n  public static java.lang.String byteToHexString(byte[]);\n  public static byte[] hexStringToByte(java.lang.String);\n  public static java.lang.String uriToString(java.net.URI[]);\n  public static java.net.URI[] stringToURI(java.lang.String[]);\n  public static org.apache.hadoop.fs.Path[] stringToPath(java.lang.String[]);\n  public static java.lang.String formatTimeDiff(long, long);\n  public static java.lang.String formatTime(long);\n  public static java.lang.String getFormattedTimeWithDiff(java.text.DateFormat, long, long);\n  public static java.lang.String[] getStrings(java.lang.String);\n  public static java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public static java.util.Collection<java.lang.String> getStringCollection(java.lang.String, java.lang.String);\n  public static java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public static java.lang.String[] getTrimmedStrings(java.lang.String);\n  public static java.util.Set<java.lang.String> getTrimmedStrings(java.util.Collection<java.lang.String>);\n  public static java.lang.String[] split(java.lang.String);\n  public static java.lang.String[] split(java.lang.String, char, char);\n  public static java.lang.String[] split(java.lang.String, char);\n  public static int findNext(java.lang.String, char, char, int, java.lang.StringBuilder);\n  public static java.lang.String escapeString(java.lang.String);\n  public static java.lang.String escapeString(java.lang.String, char, char);\n  public static java.lang.String escapeString(java.lang.String, char, char[]);\n  public static java.lang.String unEscapeString(java.lang.String);\n  public static java.lang.String unEscapeString(java.lang.String, char, char);\n  public static java.lang.String unEscapeString(java.lang.String, char, char[]);\n  public static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.apache.commons.logging.Log);\n  public static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.slf4j.Logger);\n  static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.apache.hadoop.util.LogAdapter);\n  public static java.lang.String escapeHTML(java.lang.String);\n  public static java.lang.String byteDesc(long);\n  public static java.lang.String limitDecimalTo2(double);\n  public static java.lang.String join(java.lang.CharSequence, java.lang.Iterable<?>);\n  public static java.lang.String join(java.lang.CharSequence, java.lang.String[]);\n  public static java.lang.String camelize(java.lang.String);\n  public static java.lang.String replaceTokens(java.lang.String, java.util.regex.Pattern, java.util.Map<java.lang.String, java.lang.String>);\n  public static java.lang.String getStackTrace(java.lang.Thread);\n  public static java.lang.String popOptionWithArgument(java.lang.String, java.util.List<java.lang.String>) throws java.lang.IllegalArgumentException;\n  public static boolean popOption(java.lang.String, java.util.List<java.lang.String>);\n  public static java.lang.String popFirstNonOption(java.util.List<java.lang.String>);\n  public static java.lang.String toLowerCase(java.lang.String);\n  public static java.lang.String toUpperCase(java.lang.String);\n  public static boolean equalsIgnoreCase(java.lang.String, java.lang.String);\n  static java.lang.String access$000(java.lang.String, java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/record/XmlRecordInput$1.class": "Compiled from \"XmlRecordInput.java\"\npublic class org.apache.hadoop.record.XmlRecordInput implements org.apache.hadoop.record.RecordInput {\n  public org.apache.hadoop.record.XmlRecordInput(java.io.InputStream);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  static int access$000(org.apache.hadoop.record.XmlRecordInput);\n  static java.util.ArrayList access$100(org.apache.hadoop.record.XmlRecordInput);\n  static int access$008(org.apache.hadoop.record.XmlRecordInput);\n}\n", 
  "org/apache/hadoop/crypto/key/kms/ValueQueue.class": "Compiled from \"ValueQueue.java\"\npublic class org.apache.hadoop.crypto.key.kms.ValueQueue<E> {\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public void initializeQueuesForKeys(java.lang.String...) throws java.util.concurrent.ExecutionException;\n  public E getNext(java.lang.String) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void drain(java.lang.String);\n  public int getSize(java.lang.String) throws java.util.concurrent.ExecutionException;\n  public java.util.List<E> getAtMost(java.lang.String, int) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void shutdown();\n  static int access$200(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static float access$300(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller access$400(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Reader$Option.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$NegativeCacheSentinel.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/CBZip2InputStream$1.class": "Compiled from \"CBZip2InputStream.java\"\npublic class org.apache.hadoop.io.compress.bzip2.CBZip2InputStream extends java.io.InputStream implements org.apache.hadoop.io.compress.bzip2.BZip2Constants {\n  public static final long BLOCK_DELIMITER;\n  public static final long EOS_DELIMITER;\n  org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE readMode;\n  public long getProcessedByteCount();\n  protected void updateProcessedByteCount(int);\n  public void updateReportedByteCount(int);\n  public boolean skipToNextMarker(long, int) throws java.io.IOException, java.lang.IllegalArgumentException;\n  protected void reportCRCError() throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2InputStream(java.io.InputStream, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n  public static long numberOfBytesTillNextMarker(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2InputStream(java.io.InputStream) throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/CanSetReadahead.class": "Compiled from \"CanSetReadahead.java\"\npublic interface org.apache.hadoop.fs.CanSetReadahead {\n  public abstract void setReadahead(java.lang.Long) throws java.io.IOException, java.lang.UnsupportedOperationException;\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Sorter$SortPass.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/service/ServiceOperations.class": "Compiled from \"ServiceOperations.java\"\npublic final class org.apache.hadoop.service.ServiceOperations {\n  public static void stop(org.apache.hadoop.service.Service);\n  public static java.lang.Exception stopQuietly(org.apache.hadoop.service.Service);\n  public static java.lang.Exception stopQuietly(org.apache.commons.logging.Log, org.apache.hadoop.service.Service);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslPropertiesResolver.class": "Compiled from \"SaslPropertiesResolver.java\"\npublic class org.apache.hadoop.security.SaslPropertiesResolver implements org.apache.hadoop.conf.Configurable {\n  org.apache.hadoop.conf.Configuration conf;\n  public org.apache.hadoop.security.SaslPropertiesResolver();\n  public static org.apache.hadoop.security.SaslPropertiesResolver getInstance(org.apache.hadoop.conf.Configuration);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.util.Map<java.lang.String, java.lang.String> getDefaultProperties();\n  public java.util.Map<java.lang.String, java.lang.String> getServerProperties(java.net.InetAddress);\n  public java.util.Map<java.lang.String, java.lang.String> getClientProperties(java.net.InetAddress);\n}\n", 
  "org/apache/hadoop/record/meta/Utils.class": "Compiled from \"Utils.java\"\npublic class org.apache.hadoop.record.meta.Utils {\n  public static void skip(org.apache.hadoop.record.RecordInput, java.lang.String, org.apache.hadoop.record.meta.TypeID) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProtoOrBuilder.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/Record.class": "Compiled from \"Record.java\"\npublic abstract class org.apache.hadoop.record.Record implements org.apache.hadoop.io.WritableComparable,java.lang.Cloneable {\n  public org.apache.hadoop.record.Record();\n  public abstract void serialize(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  public abstract void deserialize(org.apache.hadoop.record.RecordInput, java.lang.String) throws java.io.IOException;\n  public abstract int compareTo(java.lang.Object) throws java.lang.ClassCastException;\n  public void serialize(org.apache.hadoop.record.RecordOutput) throws java.io.IOException;\n  public void deserialize(org.apache.hadoop.record.RecordInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/fs/CreateFlag.class": "Compiled from \"CreateFlag.java\"\npublic final class org.apache.hadoop.fs.CreateFlag extends java.lang.Enum<org.apache.hadoop.fs.CreateFlag> {\n  public static final org.apache.hadoop.fs.CreateFlag CREATE;\n  public static final org.apache.hadoop.fs.CreateFlag OVERWRITE;\n  public static final org.apache.hadoop.fs.CreateFlag APPEND;\n  public static final org.apache.hadoop.fs.CreateFlag SYNC_BLOCK;\n  public static final org.apache.hadoop.fs.CreateFlag LAZY_PERSIST;\n  public static final org.apache.hadoop.fs.CreateFlag NEW_BLOCK;\n  public static org.apache.hadoop.fs.CreateFlag[] values();\n  public static org.apache.hadoop.fs.CreateFlag valueOf(java.lang.String);\n  short getMode();\n  public static void validate(java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>);\n  public static void validate(java.lang.Object, boolean, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>) throws java.io.IOException;\n  public static void validateForAppend(java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/jvm/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics.jvm.package-info {\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticator.class": "Compiled from \"PseudoDelegationTokenAuthenticator.java\"\npublic class org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator {\n  public org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator();\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocketWatcher$Entry.class": "Compiled from \"DomainSocketWatcher.java\"\npublic final class org.apache.hadoop.net.unix.DomainSocketWatcher implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final java.lang.Thread watcherThread;\n  static final boolean $assertionsDisabled;\n  public static java.lang.String getLoadingFailureReason();\n  public org.apache.hadoop.net.unix.DomainSocketWatcher(int, java.lang.String) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void add(org.apache.hadoop.net.unix.DomainSocket, org.apache.hadoop.net.unix.DomainSocketWatcher$Handler);\n  public void remove(org.apache.hadoop.net.unix.DomainSocket);\n  public java.lang.String toString();\n  static java.util.concurrent.locks.ReentrantLock access$000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$102(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static boolean access$202(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static int access$300(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static void access$400(org.apache.hadoop.net.unix.DomainSocketWatcher, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet);\n  static void access$500(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static java.util.LinkedList access$600(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.TreeMap access$700(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.concurrent.locks.Condition access$800(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$200(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static int access$900(int, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet) throws java.io.IOException;\n  static void access$1000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$1100(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static {};\n}\n", 
  "org/apache/hadoop/tools/TableListing$Column.class": "Compiled from \"TableListing.java\"\npublic class org.apache.hadoop.tools.TableListing {\n  org.apache.hadoop.tools.TableListing(org.apache.hadoop.tools.TableListing$Column[], boolean, int);\n  public void addRow(java.lang.String...);\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/io/retry/FailoverProxyProvider$ProxyInfo.class": "Compiled from \"FailoverProxyProvider.java\"\npublic interface org.apache.hadoop.io.retry.FailoverProxyProvider<T> extends java.io.Closeable {\n  public abstract org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo<T> getProxy();\n  public abstract void performFailover(T);\n  public abstract java.lang.Class<T> getInterface();\n}\n", 
  "org/apache/hadoop/security/ssl/SSLHostnameVerifier.class": "Compiled from \"SSLHostnameVerifier.java\"\npublic interface org.apache.hadoop.security.ssl.SSLHostnameVerifier extends javax.net.ssl.HostnameVerifier {\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT_AND_LOCALHOST;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT_IE6;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier ALLOW_ALL;\n  public abstract boolean verify(java.lang.String, javax.net.ssl.SSLSession);\n  public abstract void check(java.lang.String, javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String, java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String, java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String[], java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/FairCallQueueMXBean.class": "Compiled from \"FairCallQueueMXBean.java\"\npublic interface org.apache.hadoop.ipc.FairCallQueueMXBean {\n  public abstract int[] getQueueSizes();\n  public abstract long[] getOverflowedCalls();\n  public abstract int getRevision();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$IpcConnectionContextProto$Builder.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$24.class": "", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$5.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/PrintJarMainClass.class": "Compiled from \"PrintJarMainClass.java\"\npublic class org.apache.hadoop.util.PrintJarMainClass {\n  public org.apache.hadoop.util.PrintJarMainClass();\n  public static void main(java.lang.String[]);\n}\n", 
  "org/apache/hadoop/io/compress/DirectDecompressor.class": "Compiled from \"DirectDecompressor.java\"\npublic interface org.apache.hadoop.io.compress.DirectDecompressor {\n  public abstract void decompress(java.nio.ByteBuffer, java.nio.ByteBuffer) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/token/TokenInfo.class": "Compiled from \"TokenInfo.java\"\npublic interface org.apache.hadoop.security.token.TokenInfo extends java.lang.annotation.Annotation {\n  public abstract java.lang.Class<? extends org.apache.hadoop.security.token.TokenSelector<? extends org.apache.hadoop.security.token.TokenIdentifier>> value();\n}\n", 
  "org/apache/hadoop/io/UTF8$Comparator.class": "Compiled from \"UTF8.java\"\npublic class org.apache.hadoop.io.UTF8 implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.UTF8> {\n  public org.apache.hadoop.io.UTF8();\n  public org.apache.hadoop.io.UTF8(java.lang.String);\n  public org.apache.hadoop.io.UTF8(org.apache.hadoop.io.UTF8);\n  public byte[] getBytes();\n  public int getLength();\n  public void set(java.lang.String);\n  public void set(org.apache.hadoop.io.UTF8);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static void skip(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public int compareTo(org.apache.hadoop.io.UTF8);\n  public java.lang.String toString();\n  public java.lang.String toStringChecked() throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static byte[] getBytes(java.lang.String);\n  public static java.lang.String fromBytes(byte[]) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$AddSpanReceiverResponseProto.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/ServletUtils.class": "Compiled from \"ServletUtils.java\"\nclass org.apache.hadoop.security.token.delegation.web.ServletUtils {\n  org.apache.hadoop.security.token.delegation.web.ServletUtils();\n  public static java.lang.String getParameter(javax.servlet.http.HttpServletRequest, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$GetServiceStatusRequestProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager$ExpiredTokenRemover.class": "Compiled from \"AbstractDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.SecretManager<TokenIdent> {\n  protected final java.util.Map<TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation> currentTokens;\n  protected int delegationTokenSequenceNumber;\n  protected final java.util.Map<java.lang.Integer, org.apache.hadoop.security.token.delegation.DelegationKey> allKeys;\n  protected int currentId;\n  protected boolean storeTokenTrackingId;\n  protected volatile boolean running;\n  protected java.lang.Object noInterruptsLock;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager(long, long, long, long);\n  public void startThreads() throws java.io.IOException;\n  public synchronized void reset();\n  public synchronized void addKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  public synchronized org.apache.hadoop.security.token.delegation.DelegationKey[] getAllKeys();\n  protected void logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void logExpireToken(TokenIdent) throws java.io.IOException;\n  protected void storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeNewToken(TokenIdent, long) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  protected void updateStoredToken(TokenIdent, long) throws java.io.IOException;\n  protected synchronized int getCurrentKeyId();\n  protected synchronized int incrementCurrentKeyId();\n  protected synchronized void setCurrentKeyId(int);\n  protected synchronized int getDelegationTokenSeqNum();\n  protected synchronized int incrementDelegationTokenSeqNum();\n  protected synchronized void setDelegationTokenSeqNum(int);\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  public synchronized void addPersistedDelegationToken(TokenIdent, long) throws java.io.IOException;\n  void rollMasterKey() throws java.io.IOException;\n  protected synchronized byte[] createPassword(TokenIdent);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation checkToken(TokenIdent) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public synchronized byte[] retrievePassword(TokenIdent) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  protected java.lang.String getTrackingIdIfEnabled(TokenIdent);\n  public synchronized java.lang.String getTokenTrackingId(TokenIdent);\n  public synchronized void verifyToken(TokenIdent, byte[]) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public synchronized long renewToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  public static javax.crypto.SecretKey createSecretKey(byte[]);\n  public void stopThreads();\n  public synchronized boolean isRunning();\n  public TokenIdent decodeTokenIdentifier(org.apache.hadoop.security.token.Token<TokenIdent>) throws java.io.IOException;\n  public byte[] retrievePassword(org.apache.hadoop.security.token.TokenIdentifier) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  protected byte[] createPassword(org.apache.hadoop.security.token.TokenIdentifier);\n  static long access$100(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  static org.apache.commons.logging.Log access$200();\n  static long access$300(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  static void access$400(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$33.class": "", 
  "org/apache/hadoop/metrics2/util/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics2.util.package-info {\n}\n", 
  "org/apache/hadoop/metrics/spi/AbstractMetricsContext.class": "Compiled from \"AbstractMetricsContext.java\"\npublic abstract class org.apache.hadoop.metrics.spi.AbstractMetricsContext implements org.apache.hadoop.metrics.MetricsContext {\n  protected org.apache.hadoop.metrics.spi.AbstractMetricsContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  protected java.lang.String getAttribute(java.lang.String);\n  protected java.util.Map<java.lang.String, java.lang.String> getAttributeTable(java.lang.String);\n  public java.lang.String getContextName();\n  public org.apache.hadoop.metrics.ContextFactory getContextFactory();\n  public synchronized void startMonitoring() throws java.io.IOException;\n  public synchronized void stopMonitoring();\n  public boolean isMonitoring();\n  public synchronized void close();\n  public final synchronized org.apache.hadoop.metrics.MetricsRecord createRecord(java.lang.String);\n  protected org.apache.hadoop.metrics.MetricsRecord newRecord(java.lang.String);\n  public synchronized void registerUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized void unregisterUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized java.util.Map<java.lang.String, java.util.Collection<org.apache.hadoop.metrics.spi.OutputRecord>> getAllRecords();\n  protected abstract void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord) throws java.io.IOException;\n  protected void flush() throws java.io.IOException;\n  protected void update(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  protected void remove(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  public int getPeriod();\n  protected void setPeriod(int);\n  protected void parseAndSetPeriod(java.lang.String);\n  static void access$000(org.apache.hadoop.metrics.spi.AbstractMetricsContext) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFileDumper.class": "Compiled from \"TFileDumper.java\"\nclass org.apache.hadoop.io.file.tfile.TFileDumper {\n  static final org.apache.commons.logging.Log LOG;\n  public static void dumpInfo(java.lang.String, java.io.PrintStream, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/net/NetworkTopology$InnerNode.class": "Compiled from \"NetworkTopology.java\"\npublic class org.apache.hadoop.net.NetworkTopology {\n  public static final java.lang.String DEFAULT_RACK;\n  public static final int DEFAULT_HOST_LEVEL;\n  public static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.net.NetworkTopology$InnerNode clusterMap;\n  protected int numOfRacks;\n  protected java.util.concurrent.locks.ReadWriteLock netlock;\n  public static org.apache.hadoop.net.NetworkTopology getInstance(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.net.NetworkTopology();\n  public void add(org.apache.hadoop.net.Node);\n  protected org.apache.hadoop.net.Node getNodeForNetworkLocation(org.apache.hadoop.net.Node);\n  public java.util.List<org.apache.hadoop.net.Node> getDatanodesInRack(java.lang.String);\n  public void remove(org.apache.hadoop.net.Node);\n  public boolean contains(org.apache.hadoop.net.Node);\n  public org.apache.hadoop.net.Node getNode(java.lang.String);\n  public java.lang.String getRack(java.lang.String);\n  public int getNumOfRacks();\n  public int getNumOfLeaves();\n  public int getDistance(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isOnSameRack(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isNodeGroupAware();\n  public boolean isOnSameNodeGroup(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  protected boolean isSameParents(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  void setRandomSeed(long);\n  public org.apache.hadoop.net.Node chooseRandom(java.lang.String);\n  public java.util.List<org.apache.hadoop.net.Node> getLeaves(java.lang.String);\n  public int countNumOfAvailableNodes(java.lang.String, java.util.Collection<org.apache.hadoop.net.Node>);\n  public java.lang.String toString();\n  public static java.lang.String getFirstHalf(java.lang.String);\n  public static java.lang.String getLastHalf(java.lang.String);\n  protected int getWeight(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public void sortByDistance(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node[], int);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.class": "Compiled from \"DelegationTokenAuthenticatedURL.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL extends org.apache.hadoop.security.authentication.client.AuthenticatedURL {\n  static final java.lang.String DO_AS;\n  public static void setDefaultDelegationTokenAuthenticator(java.lang.Class<? extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator>);\n  public static java.lang.Class<? extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator> getDefaultDelegationTokenAuthenticator();\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL();\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator);\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL(org.apache.hadoop.security.authentication.client.ConnectionConfigurator);\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator, org.apache.hadoop.security.authentication.client.ConnectionConfigurator);\n  protected void setUseQueryStringForDelegationToken(boolean);\n  public boolean useQueryStringForDelegationToken();\n  public java.net.HttpURLConnection openConnection(java.net.URL, org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public java.net.HttpURLConnection openConnection(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public java.net.HttpURLConnection openConnection(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> getDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> getDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public long renewDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public long renewDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public void cancelDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token) throws java.io.IOException;\n  public void cancelDelegationToken(java.net.URL, org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$Reader$Location.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/http/FilterInitializer.class": "Compiled from \"FilterInitializer.java\"\npublic abstract class org.apache.hadoop.http.FilterInitializer {\n  public org.apache.hadoop.http.FilterInitializer();\n  public abstract void initFilter(org.apache.hadoop.http.FilterContainer, org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/util/SignalLogger$Handler.class": "Compiled from \"SignalLogger.java\"\npublic final class org.apache.hadoop.util.SignalLogger extends java.lang.Enum<org.apache.hadoop.util.SignalLogger> {\n  public static final org.apache.hadoop.util.SignalLogger INSTANCE;\n  public static org.apache.hadoop.util.SignalLogger[] values();\n  public static org.apache.hadoop.util.SignalLogger valueOf(java.lang.String);\n  public void register(org.apache.commons.logging.Log);\n  void register(org.apache.hadoop.util.LogAdapter);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslPlainServer$SecurityProvider.class": "Compiled from \"SaslPlainServer.java\"\npublic class org.apache.hadoop.security.SaslPlainServer implements javax.security.sasl.SaslServer {\n  org.apache.hadoop.security.SaslPlainServer(javax.security.auth.callback.CallbackHandler);\n  public java.lang.String getMechanismName();\n  public byte[] evaluateResponse(byte[]) throws javax.security.sasl.SaslException;\n  public boolean isComplete();\n  public java.lang.String getAuthorizationID();\n  public java.lang.Object getNegotiatedProperty(java.lang.String);\n  public byte[] wrap(byte[], int, int) throws javax.security.sasl.SaslException;\n  public byte[] unwrap(byte[], int, int) throws javax.security.sasl.SaslException;\n  public void dispose() throws javax.security.sasl.SaslException;\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$GracefulFailoverResponseProtoOrBuilder.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ConfigPair.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/LocalDirAllocator$AllocatorPerContext$PathIterator.class": "Compiled from \"LocalDirAllocator.java\"\npublic class org.apache.hadoop.fs.LocalDirAllocator {\n  public static final int SIZE_UNKNOWN;\n  public org.apache.hadoop.fs.LocalDirAllocator(java.lang.String);\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathToRead(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.Iterable<org.apache.hadoop.fs.Path> getAllLocalPathsToRead(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.io.File createTmpFileForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean isContextValid(java.lang.String);\n  public static void removeContext(java.lang.String);\n  public boolean ifExists(java.lang.String, org.apache.hadoop.conf.Configuration);\n  int getCurrentDirectoryIndex();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Mkdir.class": "Compiled from \"Mkdir.java\"\nclass org.apache.hadoop.fs.shell.Mkdir extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  org.apache.hadoop.fs.shell.Mkdir();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected void processOptions(java.util.LinkedList<java.lang.String>);\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processNonexistentPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/GlobFilter.class": "Compiled from \"GlobFilter.java\"\npublic class org.apache.hadoop.fs.GlobFilter implements org.apache.hadoop.fs.PathFilter {\n  public org.apache.hadoop.fs.GlobFilter(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.fs.GlobFilter(java.lang.String, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  void init(java.lang.String, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  boolean hasPattern();\n  public boolean accept(org.apache.hadoop.fs.Path);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/DecayRpcScheduler$MetricsProxy.class": "Compiled from \"DecayRpcScheduler.java\"\npublic class org.apache.hadoop.ipc.DecayRpcScheduler implements org.apache.hadoop.ipc.RpcScheduler,org.apache.hadoop.ipc.DecayRpcSchedulerMXBean {\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_PERIOD_KEY;\n  public static final long IPC_CALLQUEUE_DECAYSCHEDULER_PERIOD_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_FACTOR_KEY;\n  public static final double IPC_CALLQUEUE_DECAYSCHEDULER_FACTOR_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_DECAYSCHEDULER_THRESHOLDS_KEY;\n  public static final java.lang.String DECAYSCHEDULER_UNKNOWN_IDENTITY;\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.DecayRpcScheduler(int, java.lang.String, org.apache.hadoop.conf.Configuration);\n  public int getPriorityLevel(org.apache.hadoop.ipc.Schedulable);\n  public double getDecayFactor();\n  public long getDecayPeriodMillis();\n  public double[] getThresholds();\n  public void forceDecay();\n  public java.util.Map<java.lang.Object, java.lang.Long> getCallCountSnapshot();\n  public long getTotalCallSnapshot();\n  public int getUniqueIdentityCount();\n  public long getTotalCallVolume();\n  public java.lang.String getSchedulingDecisionSummary();\n  public java.lang.String getCallVolumeSummary();\n  static void access$000(org.apache.hadoop.ipc.DecayRpcScheduler);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ChecksumFileSystem$FSDataBoundedInputStream.class": "Compiled from \"ChecksumFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.ChecksumFileSystem extends org.apache.hadoop.fs.FilterFileSystem {\n  public static double getApproxChkSumLength(long);\n  public org.apache.hadoop.fs.ChecksumFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem getRawFileSystem();\n  public org.apache.hadoop.fs.Path getChecksumFile(org.apache.hadoop.fs.Path);\n  public static boolean isChecksumFile(org.apache.hadoop.fs.Path);\n  public long getChecksumFileLength(org.apache.hadoop.fs.Path, long);\n  public int getBytesPerSum();\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public static long getChecksumLength(long, int);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  static int access$000(org.apache.hadoop.fs.ChecksumFileSystem, int, int);\n  static byte[] access$100();\n  static boolean access$200(org.apache.hadoop.fs.ChecksumFileSystem);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HardLink$HardLinkCGWin.class": "Compiled from \"HardLink.java\"\npublic class org.apache.hadoop.fs.HardLink {\n  public final org.apache.hadoop.fs.HardLink$LinkStats linkStats;\n  public org.apache.hadoop.fs.HardLink();\n  public static void createHardLink(java.io.File, java.io.File) throws java.io.IOException;\n  public static void createHardLinkMult(java.io.File, java.lang.String[], java.io.File) throws java.io.IOException;\n  public static int getLinkCount(java.io.File) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JLong.class": "Compiled from \"JLong.java\"\npublic class org.apache.hadoop.record.compiler.JLong extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JLong();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/io/MapFile.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/CBZip2InputStream$Data.class": "Compiled from \"CBZip2InputStream.java\"\npublic class org.apache.hadoop.io.compress.bzip2.CBZip2InputStream extends java.io.InputStream implements org.apache.hadoop.io.compress.bzip2.BZip2Constants {\n  public static final long BLOCK_DELIMITER;\n  public static final long EOS_DELIMITER;\n  org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE readMode;\n  public long getProcessedByteCount();\n  protected void updateProcessedByteCount(int);\n  public void updateReportedByteCount(int);\n  public boolean skipToNextMarker(long, int) throws java.io.IOException, java.lang.IllegalArgumentException;\n  protected void reportCRCError() throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2InputStream(java.io.InputStream, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n  public static long numberOfBytesTillNextMarker(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2InputStream(java.io.InputStream) throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB$1.class": "Compiled from \"HAServiceProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB implements org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB {\n  public org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB(org.apache.hadoop.ha.HAServiceProtocol);\n  public org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthResponseProto monitorHealth(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveResponseProto transitionToActive(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyResponseProto transitionToStandby(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto getServiceStatus(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto) throws com.google.protobuf.ServiceException;\n  public long getProtocolVersion(java.lang.String, long) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(java.lang.String, long, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/StreamPumper.class": "Compiled from \"StreamPumper.java\"\nclass org.apache.hadoop.ha.StreamPumper {\n  final java.lang.Thread thread;\n  final java.lang.String logPrefix;\n  final org.apache.hadoop.ha.StreamPumper$StreamType type;\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.StreamPumper(org.apache.commons.logging.Log, java.lang.String, java.io.InputStream, org.apache.hadoop.ha.StreamPumper$StreamType);\n  void join() throws java.lang.InterruptedException;\n  void start();\n  protected void pump() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JType$JavaType.class": "Compiled from \"JType.java\"\npublic abstract class org.apache.hadoop.record.compiler.JType {\n  org.apache.hadoop.record.compiler.JType$JavaType javaType;\n  org.apache.hadoop.record.compiler.JType$CppType cppType;\n  org.apache.hadoop.record.compiler.JType$CType cType;\n  public org.apache.hadoop.record.compiler.JType();\n  static java.lang.String toCamelCase(java.lang.String);\n  abstract java.lang.String getSignature();\n  void setJavaType(org.apache.hadoop.record.compiler.JType$JavaType);\n  org.apache.hadoop.record.compiler.JType$JavaType getJavaType();\n  void setCppType(org.apache.hadoop.record.compiler.JType$CppType);\n  org.apache.hadoop.record.compiler.JType$CppType getCppType();\n  void setCType(org.apache.hadoop.record.compiler.JType$CType);\n  org.apache.hadoop.record.compiler.JType$CType getCType();\n}\n", 
  "org/apache/hadoop/util/PerformanceAdvisory.class": "Compiled from \"PerformanceAdvisory.java\"\npublic class org.apache.hadoop.util.PerformanceAdvisory {\n  public static final org.slf4j.Logger LOG;\n  public org.apache.hadoop.util.PerformanceAdvisory();\n  static {};\n}\n", 
  "org/apache/hadoop/io/WritableName.class": "Compiled from \"WritableName.java\"\npublic class org.apache.hadoop.io.WritableName {\n  public static synchronized void setName(java.lang.Class<?>, java.lang.String);\n  public static synchronized void addName(java.lang.Class<?>, java.lang.String);\n  public static synchronized java.lang.String getName(java.lang.Class<?>);\n  public static synchronized java.lang.Class<?> getClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JBuffer.class": "Compiled from \"JBuffer.java\"\npublic class org.apache.hadoop.record.compiler.JBuffer extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JBuffer();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/DelegationTokenRenewer$RenewAction.class": "Compiled from \"DelegationTokenRenewer.java\"\npublic class org.apache.hadoop.fs.DelegationTokenRenewer extends java.lang.Thread {\n  public static long renewCycle;\n  protected int getRenewQueueLength();\n  public static synchronized org.apache.hadoop.fs.DelegationTokenRenewer getInstance();\n  static synchronized void reset();\n  public <T extends org/apache/hadoop/fs/FileSystem & org/apache/hadoop/fs/DelegationTokenRenewer$Renewable> org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction<T> addRenewAction(T);\n  public <T extends org/apache/hadoop/fs/FileSystem & org/apache/hadoop/fs/DelegationTokenRenewer$Renewable> void removeRenewAction(T) throws java.io.IOException;\n  public void run();\n  static {};\n}\n", 
  "org/apache/hadoop/io/BloomMapFile.class": "Compiled from \"BloomMapFile.java\"\npublic class org.apache.hadoop.io.BloomMapFile {\n  public static final java.lang.String BLOOM_FILE_NAME;\n  public static final int HASH_COUNT;\n  public org.apache.hadoop.io.BloomMapFile();\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  static byte[] access$000(org.apache.hadoop.io.DataOutputBuffer);\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/CopyCommands$Get.class": "Compiled from \"CopyCommands.java\"\nclass org.apache.hadoop.fs.shell.CopyCommands {\n  org.apache.hadoop.fs.shell.CopyCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/fs/Globber.class": "Compiled from \"Globber.java\"\nclass org.apache.hadoop.fs.Globber {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.fs.Globber(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter);\n  public org.apache.hadoop.fs.Globber(org.apache.hadoop.fs.FileContext, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter);\n  public org.apache.hadoop.fs.FileStatus[] glob() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/WritableSerialization.class": "Compiled from \"WritableSerialization.java\"\npublic class org.apache.hadoop.io.serializer.WritableSerialization extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.io.serializer.Serialization<org.apache.hadoop.io.Writable> {\n  public org.apache.hadoop.io.serializer.WritableSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.io.Writable> getSerializer(java.lang.Class<org.apache.hadoop.io.Writable>);\n  public org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.io.Writable> getDeserializer(java.lang.Class<org.apache.hadoop.io.Writable>);\n}\n", 
  "org/apache/hadoop/io/MapFile$Merger.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$GracefulFailoverRequestProto$Builder.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyShell$Command.class": "Compiled from \"KeyShell.java\"\npublic class org.apache.hadoop.crypto.key.KeyShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.crypto.key.KeyShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.crypto.key.KeyShell);\n  static boolean access$300(org.apache.hadoop.crypto.key.KeyShell);\n}\n", 
  "org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink$1.class": "Compiled from \"AbstractGangliaSink.java\"\npublic abstract class org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink implements org.apache.hadoop.metrics2.MetricsSink {\n  public final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String DEFAULT_UNITS;\n  public static final int DEFAULT_TMAX;\n  public static final int DEFAULT_DMAX;\n  public static final org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope DEFAULT_SLOPE;\n  public static final int DEFAULT_PORT;\n  public static final boolean DEFAULT_MULTICAST_ENABLED;\n  public static final int DEFAULT_MULTICAST_TTL;\n  public static final java.lang.String SERVERS_PROPERTY;\n  public static final java.lang.String MULTICAST_ENABLED_PROPERTY;\n  public static final java.lang.String MULTICAST_TTL_PROPERTY;\n  public static final int BUFFER_SIZE;\n  public static final java.lang.String SUPPORT_SPARSE_METRICS_PROPERTY;\n  public static final boolean SUPPORT_SPARSE_METRICS_DEFAULT;\n  public static final java.lang.String EQUAL;\n  protected final org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor gangliaMetricVisitor;\n  public org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  public void flush();\n  protected org.apache.hadoop.metrics2.sink.ganglia.GangliaConf getGangliaConfForMetric(java.lang.String);\n  protected java.lang.String getHostName();\n  protected void xdr_string(java.lang.String);\n  protected void xdr_int(int);\n  protected void emitToGangliaHosts() throws java.io.IOException;\n  void resetBuffer();\n  protected boolean isSupportSparseMetrics();\n  void setDatagramSocket(java.net.DatagramSocket);\n  java.net.DatagramSocket getDatagramSocket();\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Reader$InputStreamOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$GetServiceStatusResponseProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/avro/AvroSerialization$AvroSerializer.class": "Compiled from \"AvroSerialization.java\"\npublic abstract class org.apache.hadoop.io.serializer.avro.AvroSerialization<T> extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.io.serializer.Serialization<T> {\n  public static final java.lang.String AVRO_SCHEMA_KEY;\n  public org.apache.hadoop.io.serializer.avro.AvroSerialization();\n  public org.apache.hadoop.io.serializer.Deserializer<T> getDeserializer(java.lang.Class<T>);\n  public org.apache.hadoop.io.serializer.Serializer<T> getSerializer(java.lang.Class<T>);\n  public abstract org.apache.avro.Schema getSchema(T);\n  public abstract org.apache.avro.io.DatumWriter<T> getWriter(java.lang.Class<T>);\n  public abstract org.apache.avro.io.DatumReader<T> getReader(java.lang.Class<T>);\n}\n", 
  "org/apache/hadoop/fs/FileContext$12.class": "", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RPCTraceInfoProto.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/MoveCommands$Rename.class": "Compiled from \"MoveCommands.java\"\nclass org.apache.hadoop.fs.shell.MoveCommands {\n  org.apache.hadoop.fs.shell.MoveCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ha/protocolPB/ZKFCProtocolPB.class": "Compiled from \"ZKFCProtocolPB.java\"\npublic interface org.apache.hadoop.ha.protocolPB.ZKFCProtocolPB extends org.apache.hadoop.ha.proto.ZKFCProtocolProtos$ZKFCProtocolService$BlockingInterface,org.apache.hadoop.ipc.VersionedProtocol {\n}\n", 
  "org/apache/hadoop/net/ConnectTimeoutException.class": "Compiled from \"ConnectTimeoutException.java\"\npublic class org.apache.hadoop.net.ConnectTimeoutException extends java.net.SocketTimeoutException {\n  public org.apache.hadoop.net.ConnectTimeoutException(java.lang.String);\n}\n", 
  "org/apache/hadoop/record/compiler/JCompType$JavaCompType.class": "Compiled from \"JCompType.java\"\nabstract class org.apache.hadoop.record.compiler.JCompType extends org.apache.hadoop.record.compiler.JType {\n  org.apache.hadoop.record.compiler.JCompType();\n}\n", 
  "org/apache/hadoop/log/EventCounter.class": "Compiled from \"EventCounter.java\"\npublic class org.apache.hadoop.log.EventCounter extends org.apache.hadoop.log.metrics.EventCounter {\n  public org.apache.hadoop.log.EventCounter();\n  static {};\n}\n", 
  "org/apache/hadoop/record/meta/TypeID$RIOType.class": "Compiled from \"TypeID.java\"\npublic class org.apache.hadoop.record.meta.TypeID {\n  public static final org.apache.hadoop.record.meta.TypeID BoolTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID BufferTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID ByteTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID DoubleTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID FloatTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID IntTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID LongTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID StringTypeID;\n  protected byte typeVal;\n  org.apache.hadoop.record.meta.TypeID(byte);\n  public byte getTypeVal();\n  void write(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto$1.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$IntegerRanges$RangeNumberIterator.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$CancelDelegationTokenRequestProto$Builder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JRecord$JavaRecord.class": "Compiled from \"JRecord.java\"\npublic class org.apache.hadoop.record.compiler.JRecord extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JRecord(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType>>);\n  java.lang.String getSignature();\n  void genCppCode(java.io.FileWriter, java.io.FileWriter, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n  void genJavaCode(java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/OutputBuffer.class": "Compiled from \"OutputBuffer.java\"\npublic class org.apache.hadoop.io.OutputBuffer extends java.io.FilterOutputStream {\n  public org.apache.hadoop.io.OutputBuffer();\n  public byte[] getData();\n  public int getLength();\n  public org.apache.hadoop.io.OutputBuffer reset();\n  public void write(java.io.InputStream, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/net/TableMapping$1.class": "Compiled from \"TableMapping.java\"\npublic class org.apache.hadoop.net.TableMapping extends org.apache.hadoop.net.CachedDNSToSwitchMapping {\n  public org.apache.hadoop.net.TableMapping();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void reloadCachedMappings();\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolVersionsResponseProto.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ApplicationClassLoader.class": "Compiled from \"ApplicationClassLoader.java\"\npublic class org.apache.hadoop.util.ApplicationClassLoader extends java.net.URLClassLoader {\n  public static final java.lang.String SYSTEM_CLASSES_DEFAULT;\n  public org.apache.hadoop.util.ApplicationClassLoader(java.net.URL[], java.lang.ClassLoader, java.util.List<java.lang.String>);\n  public org.apache.hadoop.util.ApplicationClassLoader(java.lang.String, java.lang.ClassLoader, java.util.List<java.lang.String>) throws java.net.MalformedURLException;\n  static java.net.URL[] constructUrlsFromClasspath(java.lang.String) throws java.net.MalformedURLException;\n  public java.net.URL getResource(java.lang.String);\n  public java.lang.Class<?> loadClass(java.lang.String) throws java.lang.ClassNotFoundException;\n  protected synchronized java.lang.Class<?> loadClass(java.lang.String, boolean) throws java.lang.ClassNotFoundException;\n  public static boolean isSystemClass(java.lang.String, java.util.List<java.lang.String>);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueProtocolService$2.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ChunkedArrayList$1.class": "Compiled from \"ChunkedArrayList.java\"\npublic class org.apache.hadoop.util.ChunkedArrayList<T> extends java.util.AbstractList<T> {\n  public org.apache.hadoop.util.ChunkedArrayList();\n  public org.apache.hadoop.util.ChunkedArrayList(int, int);\n  public java.util.Iterator<T> iterator();\n  public boolean add(T);\n  public void clear();\n  public boolean isEmpty();\n  public int size();\n  int getNumChunks();\n  int getMaxChunkSize();\n  public T get(int);\n  static int access$010(org.apache.hadoop.util.ChunkedArrayList);\n}\n", 
  "org/apache/hadoop/ipc/Schedulable.class": "Compiled from \"Schedulable.java\"\npublic interface org.apache.hadoop.ipc.Schedulable {\n  public abstract org.apache.hadoop.security.UserGroupInformation getUserGroupInformation();\n}\n", 
  "org/apache/hadoop/metrics/spi/NullContextWithUpdateThread.class": "Compiled from \"NullContextWithUpdateThread.java\"\npublic class org.apache.hadoop.metrics.spi.NullContextWithUpdateThread extends org.apache.hadoop.metrics.spi.AbstractMetricsContext {\n  public org.apache.hadoop.metrics.spi.NullContextWithUpdateThread();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  protected void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord);\n  protected void update(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  protected void remove(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n}\n", 
  "org/apache/hadoop/fs/FileContext$20.class": "", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$UserInformationProto.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider$Factory.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.class": "Compiled from \"DefaultMetricsSystem.java\"\npublic final class org.apache.hadoop.metrics2.lib.DefaultMetricsSystem extends java.lang.Enum<org.apache.hadoop.metrics2.lib.DefaultMetricsSystem> {\n  public static final org.apache.hadoop.metrics2.lib.DefaultMetricsSystem INSTANCE;\n  volatile boolean miniClusterMode;\n  final transient org.apache.hadoop.metrics2.lib.UniqueNames mBeanNames;\n  final transient org.apache.hadoop.metrics2.lib.UniqueNames sourceNames;\n  public static org.apache.hadoop.metrics2.lib.DefaultMetricsSystem[] values();\n  public static org.apache.hadoop.metrics2.lib.DefaultMetricsSystem valueOf(java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsSystem initialize(java.lang.String);\n  org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsSystem instance();\n  public static void shutdown();\n  void shutdownInstance();\n  public static org.apache.hadoop.metrics2.MetricsSystem setInstance(org.apache.hadoop.metrics2.MetricsSystem);\n  org.apache.hadoop.metrics2.MetricsSystem setImpl(org.apache.hadoop.metrics2.MetricsSystem);\n  org.apache.hadoop.metrics2.MetricsSystem getImpl();\n  public static void setMiniClusterMode(boolean);\n  public static boolean inMiniClusterMode();\n  public static javax.management.ObjectName newMBeanName(java.lang.String);\n  public static void removeMBeanName(javax.management.ObjectName);\n  public static java.lang.String sourceName(java.lang.String, boolean);\n  synchronized javax.management.ObjectName newObjectName(java.lang.String);\n  synchronized void removeObjectName(java.lang.String);\n  synchronized java.lang.String newSourceName(java.lang.String, boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/CompareUtils$ScalarComparator.class": "Compiled from \"CompareUtils.java\"\nclass org.apache.hadoop.io.file.tfile.CompareUtils {\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$CancelDelegationTokenResponseProto$Builder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ShutdownHookManager.class": "Compiled from \"ShutdownHookManager.java\"\npublic class org.apache.hadoop.util.ShutdownHookManager {\n  public static org.apache.hadoop.util.ShutdownHookManager get();\n  java.util.List<java.lang.Runnable> getShutdownHooksInOrder();\n  public void addShutdownHook(java.lang.Runnable, int);\n  public boolean removeShutdownHook(java.lang.Runnable);\n  public boolean hasShutdownHook(java.lang.Runnable);\n  public boolean isShutdownInProgress();\n  static org.apache.hadoop.util.ShutdownHookManager access$000();\n  static java.util.concurrent.atomic.AtomicBoolean access$100(org.apache.hadoop.util.ShutdownHookManager);\n  static org.apache.commons.logging.Log access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JFile.class": "Compiled from \"JFile.java\"\npublic class org.apache.hadoop.record.compiler.JFile {\n  public org.apache.hadoop.record.compiler.JFile(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JFile>, java.util.ArrayList<org.apache.hadoop.record.compiler.JRecord>);\n  java.lang.String getName();\n  public int genCode(java.lang.String, java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminProtocolServerSideTranslatorPB.class": "Compiled from \"TraceAdminProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.tracing.TraceAdminProtocolServerSideTranslatorPB implements org.apache.hadoop.tracing.TraceAdminProtocolPB,java.io.Closeable {\n  public org.apache.hadoop.tracing.TraceAdminProtocolServerSideTranslatorPB(org.apache.hadoop.tracing.TraceAdminProtocol);\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversResponseProto listSpanReceivers(com.google.protobuf.RpcController, org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto addSpanReceiver(com.google.protobuf.RpcController, org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverResponseProto removeSpanReceiver(com.google.protobuf.RpcController, org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto) throws com.google.protobuf.ServiceException;\n  public long getProtocolVersion(java.lang.String, long) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(java.lang.String, long, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/FSDataOutputStream$PositionCache.class": "Compiled from \"FSDataOutputStream.java\"\npublic class org.apache.hadoop.fs.FSDataOutputStream extends java.io.DataOutputStream implements org.apache.hadoop.fs.Syncable,org.apache.hadoop.fs.CanSetDropBehind {\n  public org.apache.hadoop.fs.FSDataOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream(java.io.OutputStream, org.apache.hadoop.fs.FileSystem$Statistics) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream(java.io.OutputStream, org.apache.hadoop.fs.FileSystem$Statistics, long) throws java.io.IOException;\n  public long getPos() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public java.io.OutputStream getWrappedStream();\n  public void sync() throws java.io.IOException;\n  public void hflush() throws java.io.IOException;\n  public void hsync() throws java.io.IOException;\n  public void setDropBehind(java.lang.Boolean) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.class": "Compiled from \"DelegationTokenAuthenticationHandler.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler implements org.apache.hadoop.security.authentication.server.AuthenticationHandler {\n  protected static final java.lang.String TYPE_POSTFIX;\n  public static final java.lang.String PREFIX;\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String DELEGATION_TOKEN_UGI_ATTRIBUTE;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler(org.apache.hadoop.security.authentication.server.AuthenticationHandler);\n  org.apache.hadoop.security.token.delegation.web.DelegationTokenManager getTokenManager();\n  public void init(java.util.Properties) throws javax.servlet.ServletException;\n  public void setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  public void initTokenManager(java.util.Properties);\n  public void destroy();\n  public java.lang.String getType();\n  public boolean managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  public org.apache.hadoop.security.authentication.server.AuthenticationToken authenticate(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, org.apache.hadoop.security.authentication.client.AuthenticationException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/Errno.class": "Compiled from \"Errno.java\"\npublic final class org.apache.hadoop.io.nativeio.Errno extends java.lang.Enum<org.apache.hadoop.io.nativeio.Errno> {\n  public static final org.apache.hadoop.io.nativeio.Errno EPERM;\n  public static final org.apache.hadoop.io.nativeio.Errno ENOENT;\n  public static final org.apache.hadoop.io.nativeio.Errno ESRCH;\n  public static final org.apache.hadoop.io.nativeio.Errno EINTR;\n  public static final org.apache.hadoop.io.nativeio.Errno EIO;\n  public static final org.apache.hadoop.io.nativeio.Errno ENXIO;\n  public static final org.apache.hadoop.io.nativeio.Errno E2BIG;\n  public static final org.apache.hadoop.io.nativeio.Errno ENOEXEC;\n  public static final org.apache.hadoop.io.nativeio.Errno EBADF;\n  public static final org.apache.hadoop.io.nativeio.Errno ECHILD;\n  public static final org.apache.hadoop.io.nativeio.Errno EAGAIN;\n  public static final org.apache.hadoop.io.nativeio.Errno ENOMEM;\n  public static final org.apache.hadoop.io.nativeio.Errno EACCES;\n  public static final org.apache.hadoop.io.nativeio.Errno EFAULT;\n  public static final org.apache.hadoop.io.nativeio.Errno ENOTBLK;\n  public static final org.apache.hadoop.io.nativeio.Errno EBUSY;\n  public static final org.apache.hadoop.io.nativeio.Errno EEXIST;\n  public static final org.apache.hadoop.io.nativeio.Errno EXDEV;\n  public static final org.apache.hadoop.io.nativeio.Errno ENODEV;\n  public static final org.apache.hadoop.io.nativeio.Errno ENOTDIR;\n  public static final org.apache.hadoop.io.nativeio.Errno EISDIR;\n  public static final org.apache.hadoop.io.nativeio.Errno EINVAL;\n  public static final org.apache.hadoop.io.nativeio.Errno ENFILE;\n  public static final org.apache.hadoop.io.nativeio.Errno EMFILE;\n  public static final org.apache.hadoop.io.nativeio.Errno ENOTTY;\n  public static final org.apache.hadoop.io.nativeio.Errno ETXTBSY;\n  public static final org.apache.hadoop.io.nativeio.Errno EFBIG;\n  public static final org.apache.hadoop.io.nativeio.Errno ENOSPC;\n  public static final org.apache.hadoop.io.nativeio.Errno ESPIPE;\n  public static final org.apache.hadoop.io.nativeio.Errno EROFS;\n  public static final org.apache.hadoop.io.nativeio.Errno EMLINK;\n  public static final org.apache.hadoop.io.nativeio.Errno EPIPE;\n  public static final org.apache.hadoop.io.nativeio.Errno EDOM;\n  public static final org.apache.hadoop.io.nativeio.Errno ERANGE;\n  public static final org.apache.hadoop.io.nativeio.Errno ELOOP;\n  public static final org.apache.hadoop.io.nativeio.Errno ENAMETOOLONG;\n  public static final org.apache.hadoop.io.nativeio.Errno ENOTEMPTY;\n  public static final org.apache.hadoop.io.nativeio.Errno EOVERFLOW;\n  public static final org.apache.hadoop.io.nativeio.Errno UNKNOWN;\n  public static org.apache.hadoop.io.nativeio.Errno[] values();\n  public static org.apache.hadoop.io.nativeio.Errno valueOf(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink$GangliaConfType.class": "Compiled from \"AbstractGangliaSink.java\"\npublic abstract class org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink implements org.apache.hadoop.metrics2.MetricsSink {\n  public final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String DEFAULT_UNITS;\n  public static final int DEFAULT_TMAX;\n  public static final int DEFAULT_DMAX;\n  public static final org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope DEFAULT_SLOPE;\n  public static final int DEFAULT_PORT;\n  public static final boolean DEFAULT_MULTICAST_ENABLED;\n  public static final int DEFAULT_MULTICAST_TTL;\n  public static final java.lang.String SERVERS_PROPERTY;\n  public static final java.lang.String MULTICAST_ENABLED_PROPERTY;\n  public static final java.lang.String MULTICAST_TTL_PROPERTY;\n  public static final int BUFFER_SIZE;\n  public static final java.lang.String SUPPORT_SPARSE_METRICS_PROPERTY;\n  public static final boolean SUPPORT_SPARSE_METRICS_DEFAULT;\n  public static final java.lang.String EQUAL;\n  protected final org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor gangliaMetricVisitor;\n  public org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  public void flush();\n  protected org.apache.hadoop.metrics2.sink.ganglia.GangliaConf getGangliaConfForMetric(java.lang.String);\n  protected java.lang.String getHostName();\n  protected void xdr_string(java.lang.String);\n  protected void xdr_int(int);\n  protected void emitToGangliaHosts() throws java.io.IOException;\n  void resetBuffer();\n  protected boolean isSupportSparseMetrics();\n  void setDatagramSocket(java.net.DatagramSocket);\n  java.net.DatagramSocket getDatagramSocket();\n  static {};\n}\n", 
  "org/apache/hadoop/security/authorize/DefaultImpersonationProvider.class": "Compiled from \"DefaultImpersonationProvider.java\"\npublic class org.apache.hadoop.security.authorize.DefaultImpersonationProvider implements org.apache.hadoop.security.authorize.ImpersonationProvider {\n  public org.apache.hadoop.security.authorize.DefaultImpersonationProvider();\n  public static synchronized org.apache.hadoop.security.authorize.DefaultImpersonationProvider getTestProvider();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void init(java.lang.String);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void authorize(org.apache.hadoop.security.UserGroupInformation, java.lang.String) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  public java.lang.String getProxySuperuserUserConfKey(java.lang.String);\n  public java.lang.String getProxySuperuserGroupConfKey(java.lang.String);\n  public java.lang.String getProxySuperuserIpConfKey(java.lang.String);\n  public java.util.Map<java.lang.String, java.util.Collection<java.lang.String>> getProxyGroups();\n  public java.util.Map<java.lang.String, java.util.Collection<java.lang.String>> getProxyHosts();\n}\n", 
  "org/apache/hadoop/security/ssl/SSLHostnameVerifier$AbstractVerifier.class": "Compiled from \"SSLHostnameVerifier.java\"\npublic interface org.apache.hadoop.security.ssl.SSLHostnameVerifier extends javax.net.ssl.HostnameVerifier {\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT_AND_LOCALHOST;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT_IE6;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier ALLOW_ALL;\n  public abstract boolean verify(java.lang.String, javax.net.ssl.SSLSession);\n  public abstract void check(java.lang.String, javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String, java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String, java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String[], java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/SshFenceByTcpPort.class": "Compiled from \"SshFenceByTcpPort.java\"\npublic class org.apache.hadoop.ha.SshFenceByTcpPort extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.ha.FenceMethod {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String CONF_CONNECT_TIMEOUT_KEY;\n  static final java.lang.String CONF_IDENTITIES_KEY;\n  public org.apache.hadoop.ha.SshFenceByTcpPort();\n  public void checkArgs(java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public boolean tryFence(org.apache.hadoop.ha.HAServiceTarget, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/ComparableVersion$StringItem.class": "Compiled from \"ComparableVersion.java\"\npublic class org.apache.hadoop.util.ComparableVersion implements java.lang.Comparable<org.apache.hadoop.util.ComparableVersion> {\n  public org.apache.hadoop.util.ComparableVersion(java.lang.String);\n  public final void parseVersion(java.lang.String);\n  public int compareTo(org.apache.hadoop.util.ComparableVersion);\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceProtocolService$Stub.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$1.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/CompressionOutputStream.class": "Compiled from \"CompressionOutputStream.java\"\npublic abstract class org.apache.hadoop.io.compress.CompressionOutputStream extends java.io.OutputStream {\n  protected final java.io.OutputStream out;\n  protected org.apache.hadoop.io.compress.CompressionOutputStream(java.io.OutputStream);\n  void setTrackedCompressor(org.apache.hadoop.io.compress.Compressor);\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public abstract void write(byte[], int, int) throws java.io.IOException;\n  public abstract void finish() throws java.io.IOException;\n  public abstract void resetState() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/Syncable.class": "Compiled from \"Syncable.java\"\npublic interface org.apache.hadoop.fs.Syncable {\n  public abstract void sync() throws java.io.IOException;\n  public abstract void hflush() throws java.io.IOException;\n  public abstract void hsync() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto$Builder.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$POSIX$CacheManipulator.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Sorter.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$MonitorHealthRequestProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/PathExistsException.class": "Compiled from \"PathExistsException.java\"\npublic class org.apache.hadoop.fs.PathExistsException extends org.apache.hadoop.fs.PathIOException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.PathExistsException(java.lang.String);\n  protected org.apache.hadoop.fs.PathExistsException(java.lang.String, java.lang.String);\n}\n", 
  "org/apache/hadoop/io/file/tfile/MetaBlockAlreadyExists.class": "Compiled from \"MetaBlockAlreadyExists.java\"\npublic class org.apache.hadoop.io.file.tfile.MetaBlockAlreadyExists extends java.io.IOException {\n  org.apache.hadoop.io.file.tfile.MetaBlockAlreadyExists(java.lang.String);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcRequestHeaderProto$Builder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/DataInputByteBuffer$1.class": "Compiled from \"DataInputByteBuffer.java\"\npublic class org.apache.hadoop.io.DataInputByteBuffer extends java.io.DataInputStream {\n  public org.apache.hadoop.io.DataInputByteBuffer();\n  public void reset(java.nio.ByteBuffer...);\n  public java.nio.ByteBuffer[] getData();\n  public int getPosition();\n  public int getLength();\n}\n", 
  "org/apache/hadoop/fs/shell/CopyCommands.class": "Compiled from \"CopyCommands.java\"\nclass org.apache.hadoop.fs.shell.CopyCommands {\n  org.apache.hadoop.fs.shell.CopyCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/metrics2/MetricsCollector.class": "Compiled from \"MetricsCollector.java\"\npublic interface org.apache.hadoop.metrics2.MetricsCollector {\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder addRecord(java.lang.String);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder addRecord(org.apache.hadoop.metrics2.MetricsInfo);\n}\n", 
  "org/apache/hadoop/ha/ZKFCRpcServer.class": "Compiled from \"ZKFCRpcServer.java\"\npublic class org.apache.hadoop.ha.ZKFCRpcServer implements org.apache.hadoop.ha.ZKFCProtocol {\n  org.apache.hadoop.ha.ZKFCRpcServer(org.apache.hadoop.conf.Configuration, java.net.InetSocketAddress, org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.security.authorize.PolicyProvider) throws java.io.IOException;\n  void start();\n  public java.net.InetSocketAddress getAddress();\n  void stopAndJoin() throws java.lang.InterruptedException;\n  public void cedeActive(int) throws java.io.IOException, org.apache.hadoop.security.AccessControlException;\n  public void gracefulFailover() throws java.io.IOException, org.apache.hadoop.security.AccessControlException;\n}\n", 
  "org/apache/hadoop/ipc/UserIdentityProvider.class": "Compiled from \"UserIdentityProvider.java\"\npublic class org.apache.hadoop.ipc.UserIdentityProvider implements org.apache.hadoop.ipc.IdentityProvider {\n  public org.apache.hadoop.ipc.UserIdentityProvider();\n  public java.lang.String makeIdentity(org.apache.hadoop.ipc.Schedulable);\n}\n", 
  "org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.class": "Compiled from \"MD5MD5CRC32GzipFileChecksum.java\"\npublic class org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum extends org.apache.hadoop.fs.MD5MD5CRC32FileChecksum {\n  public org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum();\n  public org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum(int, long, org.apache.hadoop.io.MD5Hash);\n  public org.apache.hadoop.util.DataChecksum$Type getCrcType();\n}\n", 
  "org/apache/hadoop/crypto/key/kms/ValueQueue$1.class": "Compiled from \"ValueQueue.java\"\npublic class org.apache.hadoop.crypto.key.kms.ValueQueue<E> {\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public void initializeQueuesForKeys(java.lang.String...) throws java.util.concurrent.ExecutionException;\n  public E getNext(java.lang.String) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void drain(java.lang.String);\n  public int getSize(java.lang.String) throws java.util.concurrent.ExecutionException;\n  public java.util.List<E> getAtMost(java.lang.String, int) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void shutdown();\n  static int access$200(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static float access$300(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller access$400(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RefreshCallQueueProtocol.class": "Compiled from \"RefreshCallQueueProtocol.java\"\npublic interface org.apache.hadoop.ipc.RefreshCallQueueProtocol {\n  public static final long versionID;\n  public abstract void refreshCallQueue() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/metrics/MetricsRecord.class": "Compiled from \"MetricsRecord.java\"\npublic interface org.apache.hadoop.metrics.MetricsRecord {\n  public abstract java.lang.String getRecordName();\n  public abstract void setTag(java.lang.String, java.lang.String);\n  public abstract void setTag(java.lang.String, int);\n  public abstract void setTag(java.lang.String, long);\n  public abstract void setTag(java.lang.String, short);\n  public abstract void setTag(java.lang.String, byte);\n  public abstract void removeTag(java.lang.String);\n  public abstract void setMetric(java.lang.String, int);\n  public abstract void setMetric(java.lang.String, long);\n  public abstract void setMetric(java.lang.String, short);\n  public abstract void setMetric(java.lang.String, byte);\n  public abstract void setMetric(java.lang.String, float);\n  public abstract void incrMetric(java.lang.String, int);\n  public abstract void incrMetric(java.lang.String, long);\n  public abstract void incrMetric(java.lang.String, short);\n  public abstract void incrMetric(java.lang.String, byte);\n  public abstract void incrMetric(java.lang.String, float);\n  public abstract void update();\n  public abstract void remove();\n}\n", 
  "org/apache/hadoop/ipc/RPC$Server.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/SplittableCompressionCodec$READ_MODE.class": "Compiled from \"SplittableCompressionCodec.java\"\npublic interface org.apache.hadoop.io.compress.SplittableCompressionCodec extends org.apache.hadoop.io.compress.CompressionCodec {\n  public abstract org.apache.hadoop.io.compress.SplitCompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, long, long, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/SplittableCompressionCodec.class": "Compiled from \"SplittableCompressionCodec.java\"\npublic interface org.apache.hadoop.io.compress.SplittableCompressionCodec extends org.apache.hadoop.io.compress.CompressionCodec {\n  public abstract org.apache.hadoop.io.compress.SplitCompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, long, long, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshResponseProto$Builder.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/VLongWritable.class": "Compiled from \"VLongWritable.java\"\npublic class org.apache.hadoop.io.VLongWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.VLongWritable> {\n  public org.apache.hadoop.io.VLongWritable();\n  public org.apache.hadoop.io.VLongWritable(long);\n  public void set(long);\n  public long get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.VLongWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/conf/Configured.class": "Compiled from \"Configured.java\"\npublic class org.apache.hadoop.conf.Configured implements org.apache.hadoop.conf.Configurable {\n  public org.apache.hadoop.conf.Configured();\n  public org.apache.hadoop.conf.Configured(org.apache.hadoop.conf.Configuration);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n}\n", 
  "org/apache/hadoop/conf/Configuration$DeprecationDelta.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/Groups$1.class": "Compiled from \"Groups.java\"\npublic class org.apache.hadoop.security.Groups {\n  public org.apache.hadoop.security.Groups(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.security.Groups(org.apache.hadoop.conf.Configuration, org.apache.hadoop.util.Timer);\n  java.util.Set<java.lang.String> getNegativeCache();\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void refresh();\n  public void cacheGroupsAdd(java.util.List<java.lang.String>);\n  public static org.apache.hadoop.security.Groups getUserToGroupsMappingService();\n  public static synchronized org.apache.hadoop.security.Groups getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration);\n  public static synchronized org.apache.hadoop.security.Groups getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration);\n  static boolean access$100(org.apache.hadoop.security.Groups);\n  static java.util.Set access$200(org.apache.hadoop.security.Groups);\n  static java.io.IOException access$300(org.apache.hadoop.security.Groups, java.lang.String);\n  static org.apache.hadoop.util.Timer access$400(org.apache.hadoop.security.Groups);\n  static org.apache.hadoop.security.GroupMappingServiceProvider access$500(org.apache.hadoop.security.Groups);\n  static long access$600(org.apache.hadoop.security.Groups);\n  static org.apache.commons.logging.Log access$700();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/CopyCommands$Cp.class": "Compiled from \"CopyCommands.java\"\nclass org.apache.hadoop.fs.shell.CopyCommands {\n  org.apache.hadoop.fs.shell.CopyCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/util/HttpExceptionUtils.class": "Compiled from \"HttpExceptionUtils.java\"\npublic class org.apache.hadoop.util.HttpExceptionUtils {\n  public static final java.lang.String ERROR_JSON;\n  public static final java.lang.String ERROR_EXCEPTION_JSON;\n  public static final java.lang.String ERROR_CLASSNAME_JSON;\n  public static final java.lang.String ERROR_MESSAGE_JSON;\n  public org.apache.hadoop.util.HttpExceptionUtils();\n  public static void createServletExceptionResponse(javax.servlet.http.HttpServletResponse, int, java.lang.Throwable) throws java.io.IOException;\n  public static javax.ws.rs.core.Response createJerseyExceptionResponse(javax.ws.rs.core.Response$Status, java.lang.Throwable);\n  public static void validateResponse(java.net.HttpURLConnection, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/UnsupportedCodecException.class": "Compiled from \"UnsupportedCodecException.java\"\npublic class org.apache.hadoop.crypto.UnsupportedCodecException extends java.lang.RuntimeException {\n  public org.apache.hadoop.crypto.UnsupportedCodecException();\n  public org.apache.hadoop.crypto.UnsupportedCodecException(java.lang.String);\n  public org.apache.hadoop.crypto.UnsupportedCodecException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.crypto.UnsupportedCodecException(java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/fs/viewfs/InodeTree$INodeLink.class": "Compiled from \"InodeTree.java\"\nabstract class org.apache.hadoop.fs.viewfs.InodeTree<T> {\n  static final org.apache.hadoop.fs.Path SlashPath;\n  final org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T> root;\n  final java.lang.String homedirPrefix;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> mountPoints;\n  static final boolean $assertionsDisabled;\n  static java.lang.String[] breakIntoPathComponents(java.lang.String);\n  protected abstract T getTargetFileSystem(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, java.io.IOException;\n  protected abstract T getTargetFileSystem(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T>) throws java.net.URISyntaxException;\n  protected abstract T getTargetFileSystem(java.net.URI[]) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException;\n  protected org.apache.hadoop.fs.viewfs.InodeTree(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.IOException;\n  org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult<T> resolve(java.lang.String, boolean) throws java.io.FileNotFoundException;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> getMountPoints();\n  java.lang.String getHomeDirPrefixValue();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$AddSpanReceiverRequestProto$Builder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/ssl/SSLHostnameVerifier$4.class": "Compiled from \"SSLHostnameVerifier.java\"\npublic interface org.apache.hadoop.security.ssl.SSLHostnameVerifier extends javax.net.ssl.HostnameVerifier {\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT_AND_LOCALHOST;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT_IE6;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier ALLOW_ALL;\n  public abstract boolean verify(java.lang.String, javax.net.ssl.SSLSession);\n  public abstract void check(java.lang.String, javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String, java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String, java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String[], java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslRpcServer.class": "Compiled from \"SaslRpcServer.java\"\npublic class org.apache.hadoop.security.SaslRpcServer {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String SASL_DEFAULT_REALM;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod authMethod;\n  public java.lang.String mechanism;\n  public java.lang.String protocol;\n  public java.lang.String serverId;\n  public org.apache.hadoop.security.SaslRpcServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod) throws java.io.IOException;\n  public javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection, java.util.Map<java.lang.String, ?>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void init(org.apache.hadoop.conf.Configuration);\n  static java.lang.String encodeIdentifier(byte[]);\n  static byte[] decodeIdentifier(java.lang.String);\n  public static <T extends org/apache/hadoop/security/token/TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  static char[] encodePassword(byte[]);\n  public static java.lang.String[] splitKerberosName(java.lang.String);\n  static javax.security.sasl.SaslServerFactory access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MethodMetric$1.class": "Compiled from \"MethodMetric.java\"\nclass org.apache.hadoop.metrics2.lib.MethodMetric extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  org.apache.hadoop.metrics2.lib.MethodMetric(java.lang.Object, java.lang.reflect.Method, org.apache.hadoop.metrics2.MetricsInfo, org.apache.hadoop.metrics2.annotation.Metric$Type);\n  org.apache.hadoop.metrics2.lib.MutableMetric newCounter(java.lang.Class<?>);\n  static boolean isInt(java.lang.Class<?>);\n  static boolean isLong(java.lang.Class<?>);\n  static boolean isFloat(java.lang.Class<?>);\n  static boolean isDouble(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newGauge(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newTag(java.lang.Class<?>);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  static org.apache.hadoop.metrics2.MetricsInfo metricInfo(java.lang.reflect.Method);\n  static java.lang.String nameFrom(java.lang.reflect.Method);\n  static java.lang.Object access$000(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static java.lang.reflect.Method access$100(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.hadoop.metrics2.MetricsInfo access$200(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.commons.logging.Log access$300();\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/SerializationFactory.class": "Compiled from \"SerializationFactory.java\"\npublic class org.apache.hadoop.io.serializer.SerializationFactory extends org.apache.hadoop.conf.Configured {\n  public org.apache.hadoop.io.serializer.SerializationFactory(org.apache.hadoop.conf.Configuration);\n  public <T extends java/lang/Object> org.apache.hadoop.io.serializer.Serializer<T> getSerializer(java.lang.Class<T>);\n  public <T extends java/lang/Object> org.apache.hadoop.io.serializer.Deserializer<T> getDeserializer(java.lang.Class<T>);\n  public <T extends java/lang/Object> org.apache.hadoop.io.serializer.Serialization<T> getSerialization(java.lang.Class<T>);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/RawLocalFileSystem.class": "Compiled from \"RawLocalFileSystem.java\"\npublic class org.apache.hadoop.fs.RawLocalFileSystem extends org.apache.hadoop.fs.FileSystem {\n  static final java.net.URI NAME;\n  public static void useStatIfAvailable();\n  public org.apache.hadoop.fs.RawLocalFileSystem();\n  public java.io.File pathToFile(org.apache.hadoop.fs.Path);\n  public java.net.URI getUri();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStream(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStreamWithMode(org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean mkOneDir(java.io.File) throws java.io.IOException;\n  protected boolean mkOneDirWithMode(org.apache.hadoop.fs.Path, java.io.File, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$CancelDelegationTokenRequestProto$1.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$3.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/CopyCommands$CopyFromLocal.class": "Compiled from \"CopyCommands.java\"\nclass org.apache.hadoop.fs.shell.CopyCommands {\n  org.apache.hadoop.fs.shell.CopyCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/fs/GlobPattern.class": "Compiled from \"GlobPattern.java\"\npublic class org.apache.hadoop.fs.GlobPattern {\n  public org.apache.hadoop.fs.GlobPattern(java.lang.String);\n  public java.util.regex.Pattern compiled();\n  public static java.util.regex.Pattern compile(java.lang.String);\n  public boolean matches(java.lang.CharSequence);\n  public void set(java.lang.String);\n  public boolean hasWildcard();\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceStateProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/JavaKeyStoreProvider$Factory.class": "Compiled from \"JavaKeyStoreProvider.java\"\npublic class org.apache.hadoop.crypto.key.JavaKeyStoreProvider extends org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String KEYSTORE_PASSWORD_FILE_KEY;\n  public static final java.lang.String KEYSTORE_PASSWORD_ENV_VAR;\n  public static final char[] KEYSTORE_PASSWORD_DEFAULT;\n  org.apache.hadoop.crypto.key.JavaKeyStoreProvider(org.apache.hadoop.crypto.key.JavaKeyStoreProvider);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.crypto.key.KeyProvider$KeyVersion innerSetKeyVersion(java.lang.String, java.lang.String, byte[], java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  protected void writeToNew(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean backupToOld(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.lang.String toString();\n  org.apache.hadoop.crypto.key.JavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.crypto.key.JavaKeyStoreProvider$1) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/IntrusiveCollection$IntrusiveIterator.class": "Compiled from \"IntrusiveCollection.java\"\npublic class org.apache.hadoop.util.IntrusiveCollection<E extends org.apache.hadoop.util.IntrusiveCollection$Element> implements java.util.Collection<E> {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.util.IntrusiveCollection();\n  public java.util.Iterator<E> iterator();\n  public int size();\n  public boolean isEmpty();\n  public boolean contains(java.lang.Object);\n  public java.lang.Object[] toArray();\n  public <T extends java/lang/Object> T[] toArray(T[]);\n  public boolean add(E);\n  public boolean addFirst(org.apache.hadoop.util.IntrusiveCollection$Element);\n  public boolean remove(java.lang.Object);\n  public boolean containsAll(java.util.Collection<?>);\n  public boolean addAll(java.util.Collection<? extends E>);\n  public boolean removeAll(java.util.Collection<?>);\n  public boolean retainAll(java.util.Collection<?>);\n  public void clear();\n  public boolean add(java.lang.Object);\n  static org.apache.hadoop.util.IntrusiveCollection$Element access$000(org.apache.hadoop.util.IntrusiveCollection);\n  static org.apache.hadoop.util.IntrusiveCollection$Element access$100(org.apache.hadoop.util.IntrusiveCollection, org.apache.hadoop.util.IntrusiveCollection$Element);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/PermissionParser.class": "Compiled from \"PermissionParser.java\"\nclass org.apache.hadoop.fs.permission.PermissionParser {\n  protected boolean symbolic;\n  protected short userMode;\n  protected short groupMode;\n  protected short othersMode;\n  protected short stickyMode;\n  protected char userType;\n  protected char groupType;\n  protected char othersType;\n  protected char stickyBitType;\n  public org.apache.hadoop.fs.permission.PermissionParser(java.lang.String, java.util.regex.Pattern, java.util.regex.Pattern) throws java.lang.IllegalArgumentException;\n  protected int combineModes(int, boolean);\n  protected int combineModeSegments(char, int, int, boolean);\n}\n", 
  "org/apache/hadoop/util/bloom/Filter.class": "Compiled from \"Filter.java\"\npublic abstract class org.apache.hadoop.util.bloom.Filter implements org.apache.hadoop.io.Writable {\n  protected int vectorSize;\n  protected org.apache.hadoop.util.bloom.HashFunction hash;\n  protected int nbHash;\n  protected int hashType;\n  protected org.apache.hadoop.util.bloom.Filter();\n  protected org.apache.hadoop.util.bloom.Filter(int, int, int);\n  public abstract void add(org.apache.hadoop.util.bloom.Key);\n  public abstract boolean membershipTest(org.apache.hadoop.util.bloom.Key);\n  public abstract void and(org.apache.hadoop.util.bloom.Filter);\n  public abstract void or(org.apache.hadoop.util.bloom.Filter);\n  public abstract void xor(org.apache.hadoop.util.bloom.Filter);\n  public abstract void not();\n  public void add(java.util.List<org.apache.hadoop.util.bloom.Key>);\n  public void add(java.util.Collection<org.apache.hadoop.util.bloom.Key>);\n  public void add(org.apache.hadoop.util.bloom.Key[]);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/VersionInfo.class": "Compiled from \"VersionInfo.java\"\npublic class org.apache.hadoop.util.VersionInfo {\n  protected org.apache.hadoop.util.VersionInfo(java.lang.String);\n  protected java.lang.String _getVersion();\n  protected java.lang.String _getRevision();\n  protected java.lang.String _getBranch();\n  protected java.lang.String _getDate();\n  protected java.lang.String _getUser();\n  protected java.lang.String _getUrl();\n  protected java.lang.String _getSrcChecksum();\n  protected java.lang.String _getBuildVersion();\n  protected java.lang.String _getProtocVersion();\n  public static java.lang.String getVersion();\n  public static java.lang.String getRevision();\n  public static java.lang.String getBranch();\n  public static java.lang.String getDate();\n  public static java.lang.String getUser();\n  public static java.lang.String getUrl();\n  public static java.lang.String getSrcChecksum();\n  public static java.lang.String getBuildVersion();\n  public static java.lang.String getProtocVersion();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto$1.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/meta/VectorTypeID.class": "Compiled from \"VectorTypeID.java\"\npublic class org.apache.hadoop.record.meta.VectorTypeID extends org.apache.hadoop.record.meta.TypeID {\n  public org.apache.hadoop.record.meta.VectorTypeID(org.apache.hadoop.record.meta.TypeID);\n  public org.apache.hadoop.record.meta.TypeID getElementTypeID();\n  void write(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n", 
  "org/apache/hadoop/crypto/key/UserProvider$1.class": "Compiled from \"UserProvider.java\"\npublic class org.apache.hadoop.crypto.key.UserProvider extends org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public boolean isTransient();\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public synchronized void deleteKey(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public java.lang.String toString();\n  public synchronized void flush();\n  public synchronized java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public synchronized java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.crypto.key.UserProvider(org.apache.hadoop.conf.Configuration, org.apache.hadoop.crypto.key.UserProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/compress/CompressionCodec$Util.class": "Compiled from \"CompressionCodec.java\"\npublic interface org.apache.hadoop.io.compress.CompressionCodec {\n  public abstract org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public abstract org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public abstract java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public abstract org.apache.hadoop.io.compress.Compressor createCompressor();\n  public abstract org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public abstract org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public abstract java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public abstract org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public abstract java.lang.String getDefaultExtension();\n}\n", 
  "org/apache/hadoop/io/compress/GzipCodec.class": "Compiled from \"GzipCodec.java\"\npublic class org.apache.hadoop.io.compress.GzipCodec extends org.apache.hadoop.io.compress.DefaultCodec {\n  public org.apache.hadoop.io.compress.GzipCodec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.DirectDecompressor createDirectDecompressor();\n  public java.lang.String getDefaultExtension();\n}\n", 
  "org/apache/hadoop/record/compiler/generated/Rcc.class": "Compiled from \"Rcc.java\"\npublic class org.apache.hadoop.record.compiler.generated.Rcc implements org.apache.hadoop.record.compiler.generated.RccConstants {\n  public org.apache.hadoop.record.compiler.generated.RccTokenManager token_source;\n  org.apache.hadoop.record.compiler.generated.SimpleCharStream jj_input_stream;\n  public org.apache.hadoop.record.compiler.generated.Token token;\n  public org.apache.hadoop.record.compiler.generated.Token jj_nt;\n  public static void main(java.lang.String[]);\n  public static void usage();\n  public static int driver(java.lang.String[]);\n  public final org.apache.hadoop.record.compiler.JFile Input() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final org.apache.hadoop.record.compiler.JFile Include() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final java.util.ArrayList<org.apache.hadoop.record.compiler.JRecord> Module() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final java.lang.String ModuleName() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final java.util.ArrayList<org.apache.hadoop.record.compiler.JRecord> RecordList() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final org.apache.hadoop.record.compiler.JRecord Record() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType> Field() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final org.apache.hadoop.record.compiler.JType Type() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final org.apache.hadoop.record.compiler.JMap Map() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public final org.apache.hadoop.record.compiler.JVector Vector() throws org.apache.hadoop.record.compiler.generated.ParseException;\n  public org.apache.hadoop.record.compiler.generated.Rcc(java.io.InputStream);\n  public org.apache.hadoop.record.compiler.generated.Rcc(java.io.InputStream, java.lang.String);\n  public void ReInit(java.io.InputStream);\n  public void ReInit(java.io.InputStream, java.lang.String);\n  public org.apache.hadoop.record.compiler.generated.Rcc(java.io.Reader);\n  public void ReInit(java.io.Reader);\n  public org.apache.hadoop.record.compiler.generated.Rcc(org.apache.hadoop.record.compiler.generated.RccTokenManager);\n  public void ReInit(org.apache.hadoop.record.compiler.generated.RccTokenManager);\n  public final org.apache.hadoop.record.compiler.generated.Token getNextToken();\n  public final org.apache.hadoop.record.compiler.generated.Token getToken(int);\n  public org.apache.hadoop.record.compiler.generated.ParseException generateParseException();\n  public final void enable_tracing();\n  public final void disable_tracing();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshAuthorizationPolicyProtocolService$BlockingInterface.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$CedeActiveRequestProto.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/Decompressor.class": "Compiled from \"Decompressor.java\"\npublic interface org.apache.hadoop.io.compress.Decompressor {\n  public abstract void setInput(byte[], int, int);\n  public abstract boolean needsInput();\n  public abstract void setDictionary(byte[], int, int);\n  public abstract boolean needsDictionary();\n  public abstract boolean finished();\n  public abstract int decompress(byte[], int, int) throws java.io.IOException;\n  public abstract int getRemaining();\n  public abstract void reset();\n  public abstract void end();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Reader$BufferSizeOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JCompType$CppCompType.class": "Compiled from \"JCompType.java\"\nabstract class org.apache.hadoop.record.compiler.JCompType extends org.apache.hadoop.record.compiler.JType {\n  org.apache.hadoop.record.compiler.JCompType();\n}\n", 
  "org/apache/hadoop/record/compiler/JByte.class": "Compiled from \"JByte.java\"\npublic class org.apache.hadoop.record.compiler.JByte extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JByte();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/util/AsyncDiskService$1.class": "Compiled from \"AsyncDiskService.java\"\npublic class org.apache.hadoop.util.AsyncDiskService {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.util.AsyncDiskService(java.lang.String[]);\n  public synchronized void execute(java.lang.String, java.lang.Runnable);\n  public synchronized void shutdown();\n  public synchronized boolean awaitTermination(long) throws java.lang.InterruptedException;\n  public synchronized java.util.List<java.lang.Runnable> shutdownNow();\n  static java.lang.ThreadGroup access$000(org.apache.hadoop.util.AsyncDiskService);\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$POSIX.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HarFileSystem$HarFSDataInputStream.class": "Compiled from \"HarFileSystem.java\"\npublic class org.apache.hadoop.fs.HarFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final java.lang.String METADATA_CACHE_ENTRIES_KEY;\n  public static final int METADATA_CACHE_ENTRIES_DEFAULT;\n  public static final int VERSION;\n  public org.apache.hadoop.fs.HarFileSystem();\n  public java.lang.String getScheme();\n  public org.apache.hadoop.fs.HarFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public int getHarVersion() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  public java.net.URI getUri();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  static org.apache.hadoop.fs.BlockLocation[] fixBlockLocations(org.apache.hadoop.fs.BlockLocation[], long, long, long);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public static int getHarHash(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long);\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  org.apache.hadoop.fs.HarFileSystem$HarMetaData getMetadata();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  static java.lang.String access$200(org.apache.hadoop.fs.HarFileSystem, java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.hadoop.fs.HarFileSystem$HarMetaData access$300(org.apache.hadoop.fs.HarFileSystem);\n  static java.lang.String access$400(java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.commons.logging.Log access$500();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HAServiceProtocol$StateChangeRequestInfo.class": "Compiled from \"HAServiceProtocol.java\"\npublic interface org.apache.hadoop.ha.HAServiceProtocol {\n  public static final long versionID;\n  public abstract void monitorHealth() throws org.apache.hadoop.ha.HealthCheckFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract void transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws org.apache.hadoop.ha.ServiceFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract void transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws org.apache.hadoop.ha.ServiceFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract org.apache.hadoop.ha.HAServiceStatus getServiceStatus() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$CedeActiveRequestProtoOrBuilder.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderCryptoExtension$CryptoExtension.class": "Compiled from \"KeyProviderCryptoExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderCryptoExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension> {\n  public static final java.lang.String EEK;\n  public static final java.lang.String EK;\n  protected org.apache.hadoop.crypto.key.KeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider, org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension);\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public static org.apache.hadoop.crypto.key.KeyProviderCryptoExtension createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto$1.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Client$ConnectionId.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$26.class": "", 
  "org/apache/hadoop/metrics2/MetricsSystemMXBean.class": "Compiled from \"MetricsSystemMXBean.java\"\npublic interface org.apache.hadoop.metrics2.MetricsSystemMXBean {\n  public abstract void start();\n  public abstract void stop();\n  public abstract void startMetricsMBeans();\n  public abstract void stopMetricsMBeans();\n  public abstract java.lang.String currentConfig();\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocketWatcher$NotificationHandler.class": "Compiled from \"DomainSocketWatcher.java\"\npublic final class org.apache.hadoop.net.unix.DomainSocketWatcher implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final java.lang.Thread watcherThread;\n  static final boolean $assertionsDisabled;\n  public static java.lang.String getLoadingFailureReason();\n  public org.apache.hadoop.net.unix.DomainSocketWatcher(int, java.lang.String) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void add(org.apache.hadoop.net.unix.DomainSocket, org.apache.hadoop.net.unix.DomainSocketWatcher$Handler);\n  public void remove(org.apache.hadoop.net.unix.DomainSocket);\n  public java.lang.String toString();\n  static java.util.concurrent.locks.ReentrantLock access$000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$102(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static boolean access$202(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static int access$300(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static void access$400(org.apache.hadoop.net.unix.DomainSocketWatcher, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet);\n  static void access$500(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static java.util.LinkedList access$600(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.TreeMap access$700(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.concurrent.locks.Condition access$800(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$200(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static int access$900(int, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet) throws java.io.IOException;\n  static void access$1000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$1100(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JType$CType.class": "Compiled from \"JType.java\"\npublic abstract class org.apache.hadoop.record.compiler.JType {\n  org.apache.hadoop.record.compiler.JType$JavaType javaType;\n  org.apache.hadoop.record.compiler.JType$CppType cppType;\n  org.apache.hadoop.record.compiler.JType$CType cType;\n  public org.apache.hadoop.record.compiler.JType();\n  static java.lang.String toCamelCase(java.lang.String);\n  abstract java.lang.String getSignature();\n  void setJavaType(org.apache.hadoop.record.compiler.JType$JavaType);\n  org.apache.hadoop.record.compiler.JType$JavaType getJavaType();\n  void setCppType(org.apache.hadoop.record.compiler.JType$CppType);\n  org.apache.hadoop.record.compiler.JType$CppType getCppType();\n  void setCType(org.apache.hadoop.record.compiler.JType$CType);\n  org.apache.hadoop.record.compiler.JType$CType getCType();\n}\n", 
  "org/apache/hadoop/ha/HealthMonitor$ServiceStateCallback.class": "Compiled from \"HealthMonitor.java\"\npublic class org.apache.hadoop.ha.HealthMonitor {\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.HealthMonitor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  public void addCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public void removeCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public synchronized void addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public synchronized void removeServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public void shutdown();\n  public synchronized org.apache.hadoop.ha.HAServiceProtocol getProxy();\n  protected org.apache.hadoop.ha.HAServiceProtocol createProxy() throws java.io.IOException;\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getHealthState();\n  synchronized org.apache.hadoop.ha.HAServiceStatus getLastServiceStatus();\n  boolean isAlive();\n  void join() throws java.lang.InterruptedException;\n  void start();\n  static org.apache.hadoop.ha.HAServiceTarget access$100(org.apache.hadoop.ha.HealthMonitor);\n  static org.apache.commons.logging.Log access$200();\n  static void access$300(org.apache.hadoop.ha.HealthMonitor, org.apache.hadoop.ha.HealthMonitor$State);\n  static boolean access$400(org.apache.hadoop.ha.HealthMonitor);\n  static void access$500(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static void access$600(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/WritableRpcEngine$Server$WritableRpcInvoker.class": "Compiled from \"WritableRpcEngine.java\"\npublic class org.apache.hadoop.ipc.WritableRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final long writableRpcVersion;\n  public org.apache.hadoop.ipc.WritableRpcEngine();\n  public static synchronized void ensureInitialized();\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$000();\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/VersionedProtocol.class": "Compiled from \"VersionedProtocol.java\"\npublic interface org.apache.hadoop.ipc.VersionedProtocol {\n  public abstract long getProtocolVersion(java.lang.String, long) throws java.io.IOException;\n  public abstract org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(java.lang.String, long, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProvider$Options.class": "Compiled from \"KeyProvider.java\"\npublic abstract class org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String DEFAULT_CIPHER_NAME;\n  public static final java.lang.String DEFAULT_CIPHER;\n  public static final java.lang.String DEFAULT_BITLENGTH_NAME;\n  public static final int DEFAULT_BITLENGTH;\n  public org.apache.hadoop.crypto.key.KeyProvider(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public static org.apache.hadoop.crypto.key.KeyProvider$Options options(org.apache.hadoop.conf.Configuration);\n  public boolean isTransient();\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  protected byte[] generateKey(int, java.lang.String) throws java.security.NoSuchAlgorithmException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public abstract void deleteKey(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public abstract void flush() throws java.io.IOException;\n  public static java.lang.String getBaseName(java.lang.String) throws java.io.IOException;\n  protected static java.lang.String buildVersionName(java.lang.String, int);\n  public static org.apache.hadoop.crypto.key.KeyProvider findProvider(java.util.List<org.apache.hadoop.crypto.key.KeyProvider>, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/shell/CommandWithDestination$TargetFileSystem.class": "Compiled from \"CommandWithDestination.java\"\nabstract class org.apache.hadoop.fs.shell.CommandWithDestination extends org.apache.hadoop.fs.shell.FsCommand {\n  protected org.apache.hadoop.fs.shell.PathData dst;\n  org.apache.hadoop.fs.shell.CommandWithDestination();\n  protected void setOverwrite(boolean);\n  protected void setLazyPersist(boolean);\n  protected void setVerifyChecksum(boolean);\n  protected void setWriteChecksum(boolean);\n  protected void setPreserve(boolean);\n  protected void preserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute);\n  protected void getLocalDestination(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void getRemoteDestination(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  protected void processPathArgument(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void recursePath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected org.apache.hadoop.fs.shell.PathData getTargetPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void copyFileToTarget(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void copyStreamToTarget(java.io.InputStream, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void preserveAttributes(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData, boolean) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/serializer/JavaSerializationComparator.class": "Compiled from \"JavaSerializationComparator.java\"\npublic class org.apache.hadoop.io.serializer.JavaSerializationComparator<T extends java.io.Serializable & java.lang.Comparable<T>> extends org.apache.hadoop.io.serializer.DeserializerComparator<T> {\n  public org.apache.hadoop.io.serializer.JavaSerializationComparator() throws java.io.IOException;\n  public int compare(T, T);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n", 
  "org/apache/hadoop/io/retry/RetryUtils.class": "Compiled from \"RetryUtils.java\"\npublic class org.apache.hadoop.io.retry.RetryUtils {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.io.retry.RetryUtils();\n  public static org.apache.hadoop.io.retry.RetryPolicy getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration, java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.Class<? extends java.lang.Exception>);\n  public static org.apache.hadoop.io.retry.RetryPolicy getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration, java.lang.String, boolean, java.lang.String, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshAuthorizationPolicyProtocolService$1.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/GenericsUtil.class": "Compiled from \"GenericsUtil.java\"\npublic class org.apache.hadoop.util.GenericsUtil {\n  public org.apache.hadoop.util.GenericsUtil();\n  public static <T extends java/lang/Object> java.lang.Class<T> getClass(T);\n  public static <T extends java/lang/Object> T[] toArray(java.lang.Class<T>, java.util.List<T>);\n  public static <T extends java/lang/Object> T[] toArray(java.util.List<T>);\n}\n", 
  "org/apache/hadoop/io/Stringifier.class": "Compiled from \"Stringifier.java\"\npublic interface org.apache.hadoop.io.Stringifier<T> extends java.io.Closeable {\n  public abstract java.lang.String toString(T) throws java.io.IOException;\n  public abstract T fromString(java.lang.String) throws java.io.IOException;\n  public abstract void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/compress/zlib/ZlibCompressor.class": "Compiled from \"ZlibCompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.ZlibCompressor implements org.apache.hadoop.io.compress.Compressor {\n  static boolean isNativeZlibLoaded();\n  protected final void construct(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader, int);\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor();\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.io.compress.zlib.ZlibCompressor(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy, org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader, int);\n  public void reinit(org.apache.hadoop.conf.Configuration);\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public void finish();\n  public boolean finished();\n  public int compress(byte[], int, int) throws java.io.IOException;\n  public long getBytesWritten();\n  public long getBytesRead();\n  public void reset();\n  public void end();\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation$RealUser.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Reader.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/DataInputBuffer.class": "Compiled from \"DataInputBuffer.java\"\npublic class org.apache.hadoop.io.DataInputBuffer extends java.io.DataInputStream {\n  public org.apache.hadoop.io.DataInputBuffer();\n  public void reset(byte[], int);\n  public void reset(byte[], int, int);\n  public byte[] getData();\n  public int getPosition();\n  public int getLength();\n}\n", 
  "org/apache/hadoop/record/compiler/JDouble.class": "Compiled from \"JDouble.java\"\npublic class org.apache.hadoop.record.compiler.JDouble extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JDouble();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/FileSystem$4.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$GracefulFailoverRequestProtoOrBuilder.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/LightWeightCache$Clock.class": "Compiled from \"LightWeightCache.java\"\npublic class org.apache.hadoop.util.LightWeightCache<K, E extends K> extends org.apache.hadoop.util.LightWeightGSet<K, E> {\n  public org.apache.hadoop.util.LightWeightCache(int, int, long, long);\n  org.apache.hadoop.util.LightWeightCache(int, int, long, long, org.apache.hadoop.util.LightWeightCache$Clock);\n  void setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  boolean isExpired(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  public E get(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  static {};\n}\n", 
  "org/apache/hadoop/util/bloom/HashFunction.class": "Compiled from \"HashFunction.java\"\npublic final class org.apache.hadoop.util.bloom.HashFunction {\n  public org.apache.hadoop.util.bloom.HashFunction(int, int, int);\n  public void clear();\n  public int[] hash(org.apache.hadoop.util.bloom.Key);\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$Windows$AccessRight.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/lz4/Lz4Decompressor.class": "Compiled from \"Lz4Decompressor.java\"\npublic class org.apache.hadoop.io.compress.lz4.Lz4Decompressor implements org.apache.hadoop.io.compress.Decompressor {\n  public org.apache.hadoop.io.compress.lz4.Lz4Decompressor(int);\n  public org.apache.hadoop.io.compress.lz4.Lz4Decompressor();\n  public synchronized void setInput(byte[], int, int);\n  synchronized void setInputFromSavedData();\n  public synchronized void setDictionary(byte[], int, int);\n  public synchronized boolean needsInput();\n  public synchronized boolean needsDictionary();\n  public synchronized boolean finished();\n  public synchronized int decompress(byte[], int, int) throws java.io.IOException;\n  public synchronized int getRemaining();\n  public synchronized void reset();\n  public synchronized void end();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RpcEngine.class": "Compiled from \"RpcEngine.java\"\npublic interface org.apache.hadoop.ipc.RpcEngine {\n  public abstract <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public abstract <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/ExitUtil$ExitException.class": "Compiled from \"ExitUtil.java\"\npublic final class org.apache.hadoop.util.ExitUtil {\n  public org.apache.hadoop.util.ExitUtil();\n  public static void disableSystemExit();\n  public static void disableSystemHalt();\n  public static boolean terminateCalled();\n  public static boolean haltCalled();\n  public static org.apache.hadoop.util.ExitUtil$ExitException getFirstExitException();\n  public static org.apache.hadoop.util.ExitUtil$HaltException getFirstHaltException();\n  public static void resetFirstExitException();\n  public static void resetFirstHaltException();\n  public static void terminate(int, java.lang.String) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int, java.lang.String) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  public static void terminate(int, java.lang.Throwable) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int, java.lang.Throwable) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  public static void terminate(int) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/meta/MapTypeID.class": "Compiled from \"MapTypeID.java\"\npublic class org.apache.hadoop.record.meta.MapTypeID extends org.apache.hadoop.record.meta.TypeID {\n  public org.apache.hadoop.record.meta.MapTypeID(org.apache.hadoop.record.meta.TypeID, org.apache.hadoop.record.meta.TypeID);\n  public org.apache.hadoop.record.meta.TypeID getKeyTypeID();\n  public org.apache.hadoop.record.meta.TypeID getValueTypeID();\n  void write(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$RpcResponseMessageWrapper.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueProtocolService$BlockingStub.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$RpcKindMapValue.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/CredentialShell$CreateCommand.class": "Compiled from \"CredentialShell.java\"\npublic class org.apache.hadoop.security.alias.CredentialShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.security.alias.CredentialShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int init(java.lang.String[]) throws java.io.IOException;\n  protected char[] promptForCredential() throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialShell$PasswordReader getPasswordReader();\n  public void setPasswordReader(org.apache.hadoop.security.alias.CredentialShell$PasswordReader);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.security.alias.CredentialShell);\n  static boolean access$300(org.apache.hadoop.security.alias.CredentialShell);\n  static java.lang.String access$400(org.apache.hadoop.security.alias.CredentialShell);\n}\n", 
  "org/apache/hadoop/io/compress/zlib/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.io.compress.zlib.package-info {\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$ZKFCProtocolService$BlockingStub.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/BatchedRemoteIterator$BatchedEntries.class": "Compiled from \"BatchedRemoteIterator.java\"\npublic abstract class org.apache.hadoop.fs.BatchedRemoteIterator<K, E> implements org.apache.hadoop.fs.RemoteIterator<E> {\n  public org.apache.hadoop.fs.BatchedRemoteIterator(K);\n  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<E> makeRequest(K) throws java.io.IOException;\n  public boolean hasNext() throws java.io.IOException;\n  public abstract K elementToPrevKey(E);\n  public E next() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/CanUnbuffer.class": "Compiled from \"CanUnbuffer.java\"\npublic interface org.apache.hadoop.fs.CanUnbuffer {\n  public abstract void unbuffer();\n}\n", 
  "org/apache/hadoop/fs/shell/CommandFormat$TooManyArgumentsException.class": "Compiled from \"CommandFormat.java\"\npublic class org.apache.hadoop.fs.shell.CommandFormat {\n  final int minPar;\n  final int maxPar;\n  final java.util.Map<java.lang.String, java.lang.Boolean> options;\n  boolean ignoreUnknownOpts;\n  public org.apache.hadoop.fs.shell.CommandFormat(java.lang.String, int, int, java.lang.String...);\n  public org.apache.hadoop.fs.shell.CommandFormat(int, int, java.lang.String...);\n  public java.util.List<java.lang.String> parse(java.lang.String[], int);\n  public void parse(java.util.List<java.lang.String>);\n  public boolean getOpt(java.lang.String);\n  public java.util.Set<java.lang.String> getOpts();\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$MetaIndexEntry.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer2$Builder.class": "Compiled from \"HttpServer2.java\"\npublic final class org.apache.hadoop.http.HttpServer2 implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  public static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean);\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public java.net.InetSocketAddress getConnectorAddress(int);\n  public void setThreads(int, int);\n  public void start() throws java.io.IOException;\n  void openListeners() throws java.lang.Exception;\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  org.apache.hadoop.http.HttpServer2(org.apache.hadoop.http.HttpServer2$Builder, org.apache.hadoop.http.HttpServer2$1) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.http.HttpServer2, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.http.HttpServer2, org.mortbay.jetty.Connector);\n  static void access$300(org.apache.hadoop.http.HttpServer2);\n  static {};\n}\n", 
  "org/apache/hadoop/record/RecordInput.class": "Compiled from \"RecordInput.java\"\npublic interface org.apache.hadoop.record.RecordInput {\n  public abstract byte readByte(java.lang.String) throws java.io.IOException;\n  public abstract boolean readBool(java.lang.String) throws java.io.IOException;\n  public abstract int readInt(java.lang.String) throws java.io.IOException;\n  public abstract long readLong(java.lang.String) throws java.io.IOException;\n  public abstract float readFloat(java.lang.String) throws java.io.IOException;\n  public abstract double readDouble(java.lang.String) throws java.io.IOException;\n  public abstract java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public abstract void startRecord(java.lang.String) throws java.io.IOException;\n  public abstract void endRecord(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public abstract void endVector(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public abstract void endMap(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/Shell$ShellCommandExecutor.class": "Compiled from \"Shell.java\"\npublic abstract class org.apache.hadoop.util.Shell {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int WINDOWS_MAX_SHELL_LENGHT;\n  public static final java.lang.String USER_NAME_COMMAND;\n  public static final java.lang.Object WindowsProcessLaunchLock;\n  public static final org.apache.hadoop.util.Shell$OSType osType;\n  public static final boolean WINDOWS;\n  public static final boolean SOLARIS;\n  public static final boolean MAC;\n  public static final boolean FREEBSD;\n  public static final boolean LINUX;\n  public static final boolean OTHER;\n  public static final boolean PPC_64;\n  public static final java.lang.String SET_PERMISSION_COMMAND;\n  public static final java.lang.String SET_OWNER_COMMAND;\n  public static final java.lang.String SET_GROUP_COMMAND;\n  public static final java.lang.String LINK_COMMAND;\n  public static final java.lang.String READ_LINK_COMMAND;\n  protected long timeOutInterval;\n  public static final java.lang.String WINUTILS;\n  public static final boolean isSetsidAvailable;\n  public static final java.lang.String TOKEN_SEPARATOR_REGEX;\n  public static boolean isJava7OrAbove();\n  public static void checkWindowsCommandLineLength(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String[] getGroupsCommand();\n  public static java.lang.String[] getGroupsForUserCommand(java.lang.String);\n  public static java.lang.String[] getUsersForNetgroupCommand(java.lang.String);\n  public static java.lang.String[] getGetPermissionCommand();\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean);\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean, java.lang.String);\n  public static java.lang.String[] getSetOwnerCommand(java.lang.String);\n  public static java.lang.String[] getSymlinkCommand(java.lang.String, java.lang.String);\n  public static java.lang.String[] getReadlinkCommand(java.lang.String);\n  public static java.lang.String[] getCheckProcessIsAliveCommand(java.lang.String);\n  public static java.lang.String[] getSignalKillCommand(int, java.lang.String);\n  public static java.lang.String getEnvironmentVariableRegex();\n  public static java.io.File appendScriptExtension(java.io.File, java.lang.String);\n  public static java.lang.String appendScriptExtension(java.lang.String);\n  public static java.lang.String[] getRunScriptCommand(java.io.File);\n  public static final java.lang.String getHadoopHome() throws java.io.IOException;\n  public static final java.lang.String getQualifiedBinPath(java.lang.String) throws java.io.IOException;\n  public static final java.lang.String getWinUtilsPath();\n  public org.apache.hadoop.util.Shell();\n  public org.apache.hadoop.util.Shell(long);\n  public org.apache.hadoop.util.Shell(long, boolean);\n  protected void setEnvironment(java.util.Map<java.lang.String, java.lang.String>);\n  protected void setWorkingDirectory(java.io.File);\n  protected void run() throws java.io.IOException;\n  protected abstract java.lang.String[] getExecString();\n  protected abstract void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public java.lang.String getEnvironment(java.lang.String);\n  public java.lang.Process getProcess();\n  public int getExitCode();\n  public boolean isTimedOut();\n  public static java.lang.String execCommand(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String[], long) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String...) throws java.io.IOException;\n  static java.util.concurrent.atomic.AtomicBoolean access$000(org.apache.hadoop.util.Shell);\n  static void access$100(org.apache.hadoop.util.Shell);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$RenewDelegationTokenRequestProtoOrBuilder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/Find.class": "Compiled from \"Find.java\"\npublic class org.apache.hadoop.fs.shell.find.Find extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.shell.find.Find();\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  void setRootExpression(org.apache.hadoop.fs.shell.find.Expression);\n  org.apache.hadoop.fs.shell.find.Expression getRootExpression();\n  org.apache.hadoop.fs.shell.find.FindOptions getOptions();\n  protected void recursePath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected boolean isPathRecursable(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void postProcessPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/CsvRecordInput.class": "Compiled from \"CsvRecordInput.java\"\npublic class org.apache.hadoop.record.CsvRecordInput implements org.apache.hadoop.record.RecordInput {\n  public org.apache.hadoop.record.CsvRecordInput(java.io.InputStream);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  static java.io.PushbackReader access$000(org.apache.hadoop.record.CsvRecordInput);\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ConfigPair$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/IdMappingConstant.class": "Compiled from \"IdMappingConstant.java\"\npublic class org.apache.hadoop.security.IdMappingConstant {\n  public static final java.lang.String USERGROUPID_UPDATE_MILLIS_KEY;\n  public static final long USERGROUPID_UPDATE_MILLIS_DEFAULT;\n  public static final long USERGROUPID_UPDATE_MILLIS_MIN;\n  public static final java.lang.String UNKNOWN_USER;\n  public static final java.lang.String UNKNOWN_GROUP;\n  public static final java.lang.String STATIC_ID_MAPPING_FILE_KEY;\n  public static final java.lang.String STATIC_ID_MAPPING_FILE_DEFAULT;\n  public org.apache.hadoop.security.IdMappingConstant();\n}\n", 
  "org/apache/hadoop/fs/FileContext$31.class": "", 
  "org/apache/hadoop/ha/ZKFailoverController$3.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$AddSpanReceiverResponseProto$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.class": "Compiled from \"KeyProviderDelegationTokenExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension> {\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public static org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  static {};\n}\n", 
  "org/apache/hadoop/io/MapFile$Writer.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/http/lib/StaticUserWebFilter.class": "Compiled from \"StaticUserWebFilter.java\"\npublic class org.apache.hadoop.http.lib.StaticUserWebFilter extends org.apache.hadoop.http.FilterInitializer {\n  static final java.lang.String DEPRECATED_UGI_KEY;\n  public org.apache.hadoop.http.lib.StaticUserWebFilter();\n  public void initFilter(org.apache.hadoop.http.FilterContainer, org.apache.hadoop.conf.Configuration);\n  static java.lang.String getUsernameFromConf(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SetFile$Writer.class": "Compiled from \"SetFile.java\"\npublic class org.apache.hadoop.io.SetFile extends org.apache.hadoop.io.MapFile {\n  protected org.apache.hadoop.io.SetFile();\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFs$InternalDirOfViewFs.class": "Compiled from \"ViewFs.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFs extends org.apache.hadoop.fs.AbstractFileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  final org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.AbstractFileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  org.apache.hadoop.fs.viewfs.ViewFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus() throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setVerifyChecksum(boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFs$MountPoint[] getMountPoints();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public boolean isValidName(java.lang.String);\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricGaugeFloat.class": "Compiled from \"MetricGaugeFloat.java\"\nclass org.apache.hadoop.metrics2.impl.MetricGaugeFloat extends org.apache.hadoop.metrics2.AbstractMetric {\n  final float value;\n  org.apache.hadoop.metrics2.impl.MetricGaugeFloat(org.apache.hadoop.metrics2.MetricsInfo, float);\n  public java.lang.Float value();\n  public org.apache.hadoop.metrics2.MetricType type();\n  public void visit(org.apache.hadoop.metrics2.MetricsVisitor);\n  public java.lang.Number value();\n}\n", 
  "org/apache/hadoop/util/Options$FSDataOutputStreamOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/util/MachineList.class": "Compiled from \"MachineList.java\"\npublic class org.apache.hadoop.util.MachineList {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String WILDCARD_VALUE;\n  public org.apache.hadoop.util.MachineList(java.lang.String);\n  public org.apache.hadoop.util.MachineList(java.util.Collection<java.lang.String>);\n  public org.apache.hadoop.util.MachineList(java.util.Collection<java.lang.String>, org.apache.hadoop.util.MachineList$InetAddressFactory);\n  public boolean includes(java.lang.String);\n  public java.util.Collection<java.lang.String> getCollection();\n  static {};\n}\n", 
  "org/apache/hadoop/record/BinaryRecordInput.class": "Compiled from \"BinaryRecordInput.java\"\npublic class org.apache.hadoop.record.BinaryRecordInput implements org.apache.hadoop.record.RecordInput {\n  public static org.apache.hadoop.record.BinaryRecordInput get(java.io.DataInput);\n  public org.apache.hadoop.record.BinaryRecordInput(java.io.InputStream);\n  public org.apache.hadoop.record.BinaryRecordInput(java.io.DataInput);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.record.BinaryRecordInput(org.apache.hadoop.record.BinaryRecordInput$1);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/CommandWithDestination$FileAttribute.class": "Compiled from \"CommandWithDestination.java\"\nabstract class org.apache.hadoop.fs.shell.CommandWithDestination extends org.apache.hadoop.fs.shell.FsCommand {\n  protected org.apache.hadoop.fs.shell.PathData dst;\n  org.apache.hadoop.fs.shell.CommandWithDestination();\n  protected void setOverwrite(boolean);\n  protected void setLazyPersist(boolean);\n  protected void setVerifyChecksum(boolean);\n  protected void setWriteChecksum(boolean);\n  protected void setPreserve(boolean);\n  protected void preserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute);\n  protected void getLocalDestination(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void getRemoteDestination(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  protected void processPathArgument(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void recursePath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected org.apache.hadoop.fs.shell.PathData getTargetPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void copyFileToTarget(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void copyStreamToTarget(java.io.InputStream, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void preserveAttributes(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData, boolean) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/shell/XAttrCommands$GetfattrCommand.class": "Compiled from \"XAttrCommands.java\"\nclass org.apache.hadoop.fs.shell.XAttrCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.XAttrCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/record/compiler/JRecord.class": "Compiled from \"JRecord.java\"\npublic class org.apache.hadoop.record.compiler.JRecord extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JRecord(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType>>);\n  java.lang.String getSignature();\n  void genCppCode(java.io.FileWriter, java.io.FileWriter, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n  void genJavaCode(java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/crypto/CipherOption.class": "Compiled from \"CipherOption.java\"\npublic class org.apache.hadoop.crypto.CipherOption {\n  public org.apache.hadoop.crypto.CipherOption(org.apache.hadoop.crypto.CipherSuite);\n  public org.apache.hadoop.crypto.CipherOption(org.apache.hadoop.crypto.CipherSuite, byte[], byte[], byte[], byte[]);\n  public org.apache.hadoop.crypto.CipherSuite getCipherSuite();\n  public byte[] getInKey();\n  public byte[] getInIv();\n  public byte[] getOutKey();\n  public byte[] getOutIv();\n}\n", 
  "org/apache/hadoop/fs/shell/SetReplication.class": "Compiled from \"SetReplication.java\"\nclass org.apache.hadoop.fs.shell.SetReplication extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  protected short newRep;\n  protected java.util.List<org.apache.hadoop.fs.shell.PathData> waitList;\n  protected boolean waitOpt;\n  org.apache.hadoop.fs.shell.SetReplication();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/metrics2/MetricsPlugin.class": "Compiled from \"MetricsPlugin.java\"\npublic interface org.apache.hadoop.metrics2.MetricsPlugin {\n  public abstract void init(org.apache.commons.configuration.SubsetConfiguration);\n}\n", 
  "org/apache/hadoop/record/compiler/JMap.class": "Compiled from \"JMap.java\"\npublic class org.apache.hadoop.record.compiler.JMap extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JMap(org.apache.hadoop.record.compiler.JType, org.apache.hadoop.record.compiler.JType);\n  java.lang.String getSignature();\n  static java.lang.String access$000(java.lang.String);\n  static void access$100();\n  static void access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProtoOrBuilder.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/DelegationTokenRenewer.class": "Compiled from \"DelegationTokenRenewer.java\"\npublic class org.apache.hadoop.fs.DelegationTokenRenewer extends java.lang.Thread {\n  public static long renewCycle;\n  protected int getRenewQueueLength();\n  public static synchronized org.apache.hadoop.fs.DelegationTokenRenewer getInstance();\n  static synchronized void reset();\n  public <T extends org/apache/hadoop/fs/FileSystem & org/apache/hadoop/fs/DelegationTokenRenewer$Renewable> org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction<T> addRenewAction(T);\n  public <T extends org/apache/hadoop/fs/FileSystem & org/apache/hadoop/fs/DelegationTokenRenewer$Renewable> void removeRenewAction(T) throws java.io.IOException;\n  public void run();\n  static {};\n}\n", 
  "org/apache/hadoop/util/HeapSort.class": "Compiled from \"HeapSort.java\"\npublic final class org.apache.hadoop.util.HeapSort implements org.apache.hadoop.util.IndexedSorter {\n  public org.apache.hadoop.util.HeapSort();\n  public void sort(org.apache.hadoop.util.IndexedSortable, int, int);\n  public void sort(org.apache.hadoop.util.IndexedSortable, int, int, org.apache.hadoop.util.Progressable);\n}\n", 
  "org/apache/hadoop/io/compress/BZip2Codec$BZip2CompressionOutputStream.class": "Compiled from \"BZip2Codec.java\"\npublic class org.apache.hadoop.io.compress.BZip2Codec implements org.apache.hadoop.conf.Configurable,org.apache.hadoop.io.compress.SplittableCompressionCodec {\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public org.apache.hadoop.io.compress.BZip2Codec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.SplitCompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, long, long, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.String getDefaultExtension();\n  static int access$000();\n  static int access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/ValueQueue$2.class": "Compiled from \"ValueQueue.java\"\npublic class org.apache.hadoop.crypto.key.kms.ValueQueue<E> {\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public void initializeQueuesForKeys(java.lang.String...) throws java.util.concurrent.ExecutionException;\n  public E getNext(java.lang.String) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void drain(java.lang.String);\n  public int getSize(java.lang.String) throws java.util.concurrent.ExecutionException;\n  public java.util.List<E> getAtMost(java.lang.String, int) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void shutdown();\n  static int access$200(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static float access$300(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller access$400(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static {};\n}\n", 
  "org/apache/hadoop/io/FastByteComparisons$LexicographicalComparerHolder.class": "Compiled from \"FastByteComparisons.java\"\nabstract class org.apache.hadoop.io.FastByteComparisons {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.io.FastByteComparisons();\n  public static int compareTo(byte[], int, int, byte[], int, int);\n  static org.apache.hadoop.io.FastByteComparisons$Comparer access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/RawComparable.class": "Compiled from \"RawComparable.java\"\npublic interface org.apache.hadoop.io.file.tfile.RawComparable {\n  public abstract byte[] buffer();\n  public abstract int offset();\n  public abstract int size();\n}\n", 
  "org/apache/hadoop/io/BloomMapFile$Reader.class": "Compiled from \"BloomMapFile.java\"\npublic class org.apache.hadoop.io.BloomMapFile {\n  public static final java.lang.String BLOOM_FILE_NAME;\n  public static final int HASH_COUNT;\n  public org.apache.hadoop.io.BloomMapFile();\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  static byte[] access$000(org.apache.hadoop.io.DataOutputBuffer);\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/BlockLocation.class": "Compiled from \"BlockLocation.java\"\npublic class org.apache.hadoop.fs.BlockLocation {\n  public org.apache.hadoop.fs.BlockLocation();\n  public org.apache.hadoop.fs.BlockLocation(org.apache.hadoop.fs.BlockLocation);\n  public org.apache.hadoop.fs.BlockLocation(java.lang.String[], java.lang.String[], long, long);\n  public org.apache.hadoop.fs.BlockLocation(java.lang.String[], java.lang.String[], long, long, boolean);\n  public org.apache.hadoop.fs.BlockLocation(java.lang.String[], java.lang.String[], java.lang.String[], long, long);\n  public org.apache.hadoop.fs.BlockLocation(java.lang.String[], java.lang.String[], java.lang.String[], long, long, boolean);\n  public org.apache.hadoop.fs.BlockLocation(java.lang.String[], java.lang.String[], java.lang.String[], java.lang.String[], long, long, boolean);\n  public java.lang.String[] getHosts() throws java.io.IOException;\n  public java.lang.String[] getCachedHosts();\n  public java.lang.String[] getNames() throws java.io.IOException;\n  public java.lang.String[] getTopologyPaths() throws java.io.IOException;\n  public long getOffset();\n  public long getLength();\n  public boolean isCorrupt();\n  public void setOffset(long);\n  public void setLength(long);\n  public void setCorrupt(boolean);\n  public void setHosts(java.lang.String[]) throws java.io.IOException;\n  public void setCachedHosts(java.lang.String[]);\n  public void setNames(java.lang.String[]) throws java.io.IOException;\n  public void setTopologyPaths(java.lang.String[]) throws java.io.IOException;\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSystemImpl$InitMode.class": "Compiled from \"MetricsSystemImpl.java\"\npublic class org.apache.hadoop.metrics2.impl.MetricsSystemImpl extends org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSource {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String MS_NAME;\n  static final java.lang.String MS_STATS_NAME;\n  static final java.lang.String MS_STATS_DESC;\n  static final java.lang.String MS_CONTROL_NAME;\n  static final java.lang.String MS_INIT_MODE_KEY;\n  org.apache.hadoop.metrics2.lib.MutableStat snapshotStat;\n  org.apache.hadoop.metrics2.lib.MutableStat publishStat;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong droppedPubAll;\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl(java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl();\n  public synchronized org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public synchronized void unregisterSource(java.lang.String);\n  synchronized void registerSource(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource);\n  public synchronized <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  synchronized void registerSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink);\n  public synchronized void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public synchronized void startMetricsMBeans();\n  public synchronized void stopMetricsMBeans();\n  public synchronized java.lang.String currentConfig();\n  synchronized void onTimerEvent();\n  public synchronized void publishMetricsNow();\n  synchronized org.apache.hadoop.metrics2.impl.MetricsBuffer sampleMetrics();\n  synchronized void publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, boolean);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static java.lang.String getHostname();\n  public synchronized void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n  public synchronized boolean shutdown();\n  public org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter getSourceAdapter(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Client$Connection$3.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Compression$Algorithm$2.class": "Compiled from \"Compression.java\"\nfinal class org.apache.hadoop.io.file.tfile.Compression {\n  static final org.apache.commons.logging.Log LOG;\n  static org.apache.hadoop.io.file.tfile.Compression$Algorithm getCompressionAlgorithmByName(java.lang.String);\n  static java.lang.String[] getSupportedAlgorithms();\n  static {};\n}\n", 
  "org/apache/hadoop/conf/ConfServlet.class": "Compiled from \"ConfServlet.java\"\npublic class org.apache.hadoop.conf.ConfServlet extends javax.servlet.http.HttpServlet {\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.conf.ConfServlet();\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException;\n  static void writeResponse(org.apache.hadoop.conf.Configuration, java.io.Writer, java.lang.String) throws java.io.IOException, org.apache.hadoop.conf.ConfServlet$BadFormatException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsStatus.class": "Compiled from \"FsStatus.java\"\npublic class org.apache.hadoop.fs.FsStatus implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.fs.FsStatus(long, long, long);\n  public long getCapacity();\n  public long getUsed();\n  public long getRemaining();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/record/compiler/JFloat.class": "Compiled from \"JFloat.java\"\npublic class org.apache.hadoop.record.compiler.JFloat extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JFloat();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MsInfo.class": "Compiled from \"MsInfo.java\"\npublic final class org.apache.hadoop.metrics2.impl.MsInfo extends java.lang.Enum<org.apache.hadoop.metrics2.impl.MsInfo> implements org.apache.hadoop.metrics2.MetricsInfo {\n  public static final org.apache.hadoop.metrics2.impl.MsInfo NumActiveSources;\n  public static final org.apache.hadoop.metrics2.impl.MsInfo NumAllSources;\n  public static final org.apache.hadoop.metrics2.impl.MsInfo NumActiveSinks;\n  public static final org.apache.hadoop.metrics2.impl.MsInfo NumAllSinks;\n  public static final org.apache.hadoop.metrics2.impl.MsInfo Context;\n  public static final org.apache.hadoop.metrics2.impl.MsInfo Hostname;\n  public static final org.apache.hadoop.metrics2.impl.MsInfo SessionId;\n  public static final org.apache.hadoop.metrics2.impl.MsInfo ProcessName;\n  public static org.apache.hadoop.metrics2.impl.MsInfo[] values();\n  public static org.apache.hadoop.metrics2.impl.MsInfo valueOf(java.lang.String);\n  public java.lang.String description();\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToActiveResponseProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolServerSideTranslatorPB.class": "Compiled from \"RefreshUserMappingsProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB implements org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB {\n  public org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB(org.apache.hadoop.security.RefreshUserMappingsProtocol);\n  public org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto refreshUserToGroupsMappings(com.google.protobuf.RpcController, org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto refreshSuperUserGroupsConfiguration(com.google.protobuf.RpcController, org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto) throws com.google.protobuf.ServiceException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/WritableComparable.class": "Compiled from \"WritableComparable.java\"\npublic interface org.apache.hadoop.io.WritableComparable<T> extends org.apache.hadoop.io.Writable, java.lang.Comparable<T> {\n}\n", 
  "org/apache/hadoop/fs/FileContext$9.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/DoNotPool.class": "Compiled from \"DoNotPool.java\"\npublic interface org.apache.hadoop.io.compress.DoNotPool extends java.lang.annotation.Annotation {\n}\n", 
  "org/apache/hadoop/ha/HAServiceProtocol.class": "Compiled from \"HAServiceProtocol.java\"\npublic interface org.apache.hadoop.ha.HAServiceProtocol {\n  public static final long versionID;\n  public abstract void monitorHealth() throws org.apache.hadoop.ha.HealthCheckFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract void transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws org.apache.hadoop.ha.ServiceFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract void transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws org.apache.hadoop.ha.ServiceFailedException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public abstract org.apache.hadoop.ha.HAServiceStatus getServiceStatus() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$RenewDelegationTokenResponseProto.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/curator/ChildReaper$1.class": "Compiled from \"ChildReaper.java\"\npublic class org.apache.hadoop.util.curator.ChildReaper implements java.io.Closeable {\n  static final int DEFAULT_REAPING_THRESHOLD_MS;\n  public static <E extends java/lang/Object> java.util.Set<E> newConcurrentHashSet();\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, int);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, java.util.concurrent.ScheduledExecutorService, int);\n  public org.apache.hadoop.util.curator.ChildReaper(org.apache.curator.framework.CuratorFramework, java.lang.String, org.apache.curator.framework.recipes.locks.Reaper$Mode, java.util.concurrent.ScheduledExecutorService, int, java.lang.String);\n  public void start() throws java.lang.Exception;\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.util.curator.ChildReaper addPath(java.lang.String);\n  public boolean removePath(java.lang.String);\n  static void access$000(org.apache.hadoop.util.curator.ChildReaper);\n  static {};\n}\n", 
  "org/apache/hadoop/util/JvmPauseMonitor.class": "Compiled from \"JvmPauseMonitor.java\"\npublic class org.apache.hadoop.util.JvmPauseMonitor {\n  public org.apache.hadoop.util.JvmPauseMonitor(org.apache.hadoop.conf.Configuration);\n  public void start();\n  public void stop();\n  public boolean isStarted();\n  public long getNumGcWarnThreadholdExceeded();\n  public long getNumGcInfoThresholdExceeded();\n  public long getTotalGcExtraSleepTime();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static java.util.Map access$400(org.apache.hadoop.util.JvmPauseMonitor);\n  static boolean access$500(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$600(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$704(org.apache.hadoop.util.JvmPauseMonitor);\n  static java.lang.String access$800(org.apache.hadoop.util.JvmPauseMonitor, long, java.util.Map, java.util.Map);\n  static org.apache.commons.logging.Log access$900();\n  static long access$1000(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$1104(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$1214(org.apache.hadoop.util.JvmPauseMonitor, long);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HarFileSystem$HarMetaData.class": "Compiled from \"HarFileSystem.java\"\npublic class org.apache.hadoop.fs.HarFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final java.lang.String METADATA_CACHE_ENTRIES_KEY;\n  public static final int METADATA_CACHE_ENTRIES_DEFAULT;\n  public static final int VERSION;\n  public org.apache.hadoop.fs.HarFileSystem();\n  public java.lang.String getScheme();\n  public org.apache.hadoop.fs.HarFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public int getHarVersion() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  public java.net.URI getUri();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  static org.apache.hadoop.fs.BlockLocation[] fixBlockLocations(org.apache.hadoop.fs.BlockLocation[], long, long, long);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public static int getHarHash(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long);\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  org.apache.hadoop.fs.HarFileSystem$HarMetaData getMetadata();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  static java.lang.String access$200(org.apache.hadoop.fs.HarFileSystem, java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.hadoop.fs.HarFileSystem$HarMetaData access$300(org.apache.hadoop.fs.HarFileSystem);\n  static java.lang.String access$400(java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.commons.logging.Log access$500();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ChecksumFs$ChecksumFSInputChecker.class": "Compiled from \"ChecksumFs.java\"\npublic abstract class org.apache.hadoop.fs.ChecksumFs extends org.apache.hadoop.fs.FilterFs {\n  public static double getApproxChkSumLength(long);\n  public org.apache.hadoop.fs.ChecksumFs(org.apache.hadoop.fs.AbstractFileSystem) throws java.io.IOException, java.net.URISyntaxException;\n  public void setVerifyChecksum(boolean);\n  public org.apache.hadoop.fs.AbstractFileSystem getRawFs();\n  public org.apache.hadoop.fs.Path getChecksumFile(org.apache.hadoop.fs.Path);\n  public static boolean isChecksumFile(org.apache.hadoop.fs.Path);\n  public long getChecksumFileLength(org.apache.hadoop.fs.Path, long);\n  public int getBytesPerSum();\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public static long getChecksumLength(long, int);\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  static int access$000(org.apache.hadoop.fs.ChecksumFs, int, int) throws java.io.IOException;\n  static byte[] access$100();\n  static boolean access$200(org.apache.hadoop.fs.ChecksumFs);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/MetricsRecordImpl.class": "Compiled from \"MetricsRecordImpl.java\"\npublic class org.apache.hadoop.metrics.spi.MetricsRecordImpl implements org.apache.hadoop.metrics.MetricsRecord {\n  protected org.apache.hadoop.metrics.spi.MetricsRecordImpl(java.lang.String, org.apache.hadoop.metrics.spi.AbstractMetricsContext);\n  public java.lang.String getRecordName();\n  public void setTag(java.lang.String, java.lang.String);\n  public void setTag(java.lang.String, int);\n  public void setTag(java.lang.String, long);\n  public void setTag(java.lang.String, short);\n  public void setTag(java.lang.String, byte);\n  public void removeTag(java.lang.String);\n  public void setMetric(java.lang.String, int);\n  public void setMetric(java.lang.String, long);\n  public void setMetric(java.lang.String, short);\n  public void setMetric(java.lang.String, byte);\n  public void setMetric(java.lang.String, float);\n  public void incrMetric(java.lang.String, int);\n  public void incrMetric(java.lang.String, long);\n  public void incrMetric(java.lang.String, short);\n  public void incrMetric(java.lang.String, byte);\n  public void incrMetric(java.lang.String, float);\n  public void update();\n  public void remove();\n  org.apache.hadoop.metrics.spi.AbstractMetricsContext$TagMap getTagTable();\n  java.util.Map<java.lang.String, org.apache.hadoop.metrics.spi.MetricValue> getMetricTable();\n}\n", 
  "org/apache/hadoop/record/meta/StructTypeID.class": "Compiled from \"StructTypeID.java\"\npublic class org.apache.hadoop.record.meta.StructTypeID extends org.apache.hadoop.record.meta.TypeID {\n  org.apache.hadoop.record.meta.StructTypeID();\n  public org.apache.hadoop.record.meta.StructTypeID(org.apache.hadoop.record.meta.RecordTypeInfo);\n  void add(org.apache.hadoop.record.meta.FieldTypeInfo);\n  public java.util.Collection<org.apache.hadoop.record.meta.FieldTypeInfo> getFieldTypeInfos();\n  org.apache.hadoop.record.meta.StructTypeID findStruct(java.lang.String);\n  void write(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  void writeRest(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  void read(org.apache.hadoop.record.RecordInput, java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcKindProto$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Writer.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RpcConstants.class": "Compiled from \"RpcConstants.java\"\npublic class org.apache.hadoop.ipc.RpcConstants {\n  public static final int AUTHORIZATION_FAILED_CALL_ID;\n  public static final int INVALID_CALL_ID;\n  public static final int CONNECTION_CONTEXT_CALL_ID;\n  public static final int PING_CALL_ID;\n  public static final byte[] DUMMY_CLIENT_ID;\n  public static final int INVALID_RETRY_COUNT;\n  public static final java.nio.ByteBuffer HEADER;\n  public static final byte CURRENT_VERSION;\n  static {};\n}\n", 
  "org/apache/hadoop/io/MD5Hash$1.class": "Compiled from \"MD5Hash.java\"\npublic class org.apache.hadoop.io.MD5Hash implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.MD5Hash> {\n  public static final int MD5_LEN;\n  public org.apache.hadoop.io.MD5Hash();\n  public org.apache.hadoop.io.MD5Hash(java.lang.String);\n  public org.apache.hadoop.io.MD5Hash(byte[]);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static org.apache.hadoop.io.MD5Hash read(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void set(org.apache.hadoop.io.MD5Hash);\n  public byte[] getDigest();\n  public static org.apache.hadoop.io.MD5Hash digest(byte[]);\n  public static java.security.MessageDigest getDigester();\n  public static org.apache.hadoop.io.MD5Hash digest(java.io.InputStream) throws java.io.IOException;\n  public static org.apache.hadoop.io.MD5Hash digest(byte[], int, int);\n  public static org.apache.hadoop.io.MD5Hash digest(java.lang.String);\n  public static org.apache.hadoop.io.MD5Hash digest(org.apache.hadoop.io.UTF8);\n  public long halfDigest();\n  public int quarterDigest();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.MD5Hash);\n  public java.lang.String toString();\n  public void setDigest(java.lang.String);\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/CachingKeyProvider$CacheExtension$2.class": "Compiled from \"CachingKeyProvider.java\"\npublic class org.apache.hadoop.crypto.key.CachingKeyProvider extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension> {\n  public org.apache.hadoop.crypto.key.CachingKeyProvider(org.apache.hadoop.crypto.key.KeyProvider, long, long);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink$GangliaSlope.class": "Compiled from \"AbstractGangliaSink.java\"\npublic abstract class org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink implements org.apache.hadoop.metrics2.MetricsSink {\n  public final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String DEFAULT_UNITS;\n  public static final int DEFAULT_TMAX;\n  public static final int DEFAULT_DMAX;\n  public static final org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope DEFAULT_SLOPE;\n  public static final int DEFAULT_PORT;\n  public static final boolean DEFAULT_MULTICAST_ENABLED;\n  public static final int DEFAULT_MULTICAST_TTL;\n  public static final java.lang.String SERVERS_PROPERTY;\n  public static final java.lang.String MULTICAST_ENABLED_PROPERTY;\n  public static final java.lang.String MULTICAST_TTL_PROPERTY;\n  public static final int BUFFER_SIZE;\n  public static final java.lang.String SUPPORT_SPARSE_METRICS_PROPERTY;\n  public static final boolean SUPPORT_SPARSE_METRICS_DEFAULT;\n  public static final java.lang.String EQUAL;\n  protected final org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor gangliaMetricVisitor;\n  public org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  public void flush();\n  protected org.apache.hadoop.metrics2.sink.ganglia.GangliaConf getGangliaConfForMetric(java.lang.String);\n  protected java.lang.String getHostName();\n  protected void xdr_string(java.lang.String);\n  protected void xdr_int(int);\n  protected void emitToGangliaHosts() throws java.io.IOException;\n  void resetBuffer();\n  protected boolean isSupportSparseMetrics();\n  void setDatagramSocket(java.net.DatagramSocket);\n  java.net.DatagramSocket getDatagramSocket();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceProtocolService$Interface.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/FsPermission$1.class": "Compiled from \"FsPermission.java\"\npublic class org.apache.hadoop.fs.permission.FsPermission implements org.apache.hadoop.io.Writable {\n  static final org.apache.hadoop.io.WritableFactory FACTORY;\n  public static final int MAX_PERMISSION_LENGTH;\n  public static final java.lang.String DEPRECATED_UMASK_LABEL;\n  public static final java.lang.String UMASK_LABEL;\n  public static final int DEFAULT_UMASK;\n  public static org.apache.hadoop.fs.permission.FsPermission createImmutable(short);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, boolean);\n  public org.apache.hadoop.fs.permission.FsPermission(short);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.permission.FsPermission(java.lang.String);\n  public org.apache.hadoop.fs.permission.FsAction getUserAction();\n  public org.apache.hadoop.fs.permission.FsAction getGroupAction();\n  public org.apache.hadoop.fs.permission.FsAction getOtherAction();\n  public void fromShort(short);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static org.apache.hadoop.fs.permission.FsPermission read(java.io.DataInput) throws java.io.IOException;\n  public short toShort();\n  public short toExtendedShort();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.permission.FsPermission applyUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public static org.apache.hadoop.fs.permission.FsPermission getUMask(org.apache.hadoop.conf.Configuration);\n  public boolean getStickyBit();\n  public boolean getAclBit();\n  public boolean getEncryptedBit();\n  public static void setUMask(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.permission.FsPermission);\n  public static org.apache.hadoop.fs.permission.FsPermission getDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getDirDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getFileDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getCachePoolDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission valueOf(java.lang.String);\n  org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsPermission$1);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$FileSystemOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticator$1.class": "Compiled from \"KerberosDelegationTokenAuthenticator.java\"\npublic class org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator {\n  public org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator();\n}\n", 
  "org/apache/hadoop/fs/shell/XAttrCommands.class": "Compiled from \"XAttrCommands.java\"\nclass org.apache.hadoop.fs.shell.XAttrCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.XAttrCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/metrics2/sink/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics2.sink.package-info {\n}\n", 
  "org/apache/hadoop/util/ComparableVersion$1.class": "Compiled from \"ComparableVersion.java\"\npublic class org.apache.hadoop.util.ComparableVersion implements java.lang.Comparable<org.apache.hadoop.util.ComparableVersion> {\n  public org.apache.hadoop.util.ComparableVersion(java.lang.String);\n  public final void parseVersion(java.lang.String);\n  public int compareTo(org.apache.hadoop.util.ComparableVersion);\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/fs/FsUrlStreamHandler.class": "Compiled from \"FsUrlStreamHandler.java\"\nclass org.apache.hadoop.fs.FsUrlStreamHandler extends java.net.URLStreamHandler {\n  org.apache.hadoop.fs.FsUrlStreamHandler(org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.fs.FsUrlStreamHandler();\n  protected org.apache.hadoop.fs.FsUrlConnection openConnection(java.net.URL) throws java.io.IOException;\n  protected java.net.URLConnection openConnection(java.net.URL) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/NullWritable.class": "Compiled from \"NullWritable.java\"\npublic class org.apache.hadoop.io.NullWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.NullWritable> {\n  public static org.apache.hadoop.io.NullWritable get();\n  public java.lang.String toString();\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.NullWritable);\n  public boolean equals(java.lang.Object);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$1.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFs.class": "Compiled from \"ViewFs.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFs extends org.apache.hadoop.fs.AbstractFileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  final org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.AbstractFileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  org.apache.hadoop.fs.viewfs.ViewFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus() throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setVerifyChecksum(boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFs$MountPoint[] getMountPoints();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public boolean isValidName(java.lang.String);\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController$ElectorCallbacks.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Compression$Algorithm.class": "Compiled from \"Compression.java\"\nfinal class org.apache.hadoop.io.file.tfile.Compression {\n  static final org.apache.commons.logging.Log LOG;\n  static org.apache.hadoop.io.file.tfile.Compression$Algorithm getCompressionAlgorithmByName(java.lang.String);\n  static java.lang.String[] getSupportedAlgorithms();\n  static {};\n}\n", 
  "org/apache/hadoop/util/ProgramDriver$ProgramDescription.class": "Compiled from \"ProgramDriver.java\"\npublic class org.apache.hadoop.util.ProgramDriver {\n  java.util.Map<java.lang.String, org.apache.hadoop.util.ProgramDriver$ProgramDescription> programs;\n  public org.apache.hadoop.util.ProgramDriver();\n  public void addClass(java.lang.String, java.lang.Class<?>, java.lang.String) throws java.lang.Throwable;\n  public int run(java.lang.String[]) throws java.lang.Throwable;\n  public void driver(java.lang.String[]) throws java.lang.Throwable;\n}\n", 
  "org/apache/hadoop/metrics/MetricsServlet$TagsMetricsPair.class": "Compiled from \"MetricsServlet.java\"\npublic class org.apache.hadoop.metrics.MetricsServlet extends javax.servlet.http.HttpServlet {\n  public org.apache.hadoop.metrics.MetricsServlet();\n  java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.util.List<org.apache.hadoop.metrics.MetricsServlet$TagsMetricsPair>>> makeMap(java.util.Collection<org.apache.hadoop.metrics.MetricsContext>) throws java.io.IOException;\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException;\n  void printMap(java.io.PrintWriter, java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.util.List<org.apache.hadoop.metrics.MetricsServlet$TagsMetricsPair>>>);\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProtoOrBuilder.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicy$RetryAction$RetryDecision.class": "Compiled from \"RetryPolicy.java\"\npublic interface org.apache.hadoop.io.retry.RetryPolicy {\n  public abstract org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception, int, int, boolean) throws java.lang.Exception;\n}\n", 
  "org/apache/hadoop/fs/viewfs/InodeTree$ResultKind.class": "Compiled from \"InodeTree.java\"\nabstract class org.apache.hadoop.fs.viewfs.InodeTree<T> {\n  static final org.apache.hadoop.fs.Path SlashPath;\n  final org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T> root;\n  final java.lang.String homedirPrefix;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> mountPoints;\n  static final boolean $assertionsDisabled;\n  static java.lang.String[] breakIntoPathComponents(java.lang.String);\n  protected abstract T getTargetFileSystem(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, java.io.IOException;\n  protected abstract T getTargetFileSystem(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T>) throws java.net.URISyntaxException;\n  protected abstract T getTargetFileSystem(java.net.URI[]) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException;\n  protected org.apache.hadoop.fs.viewfs.InodeTree(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.IOException;\n  org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult<T> resolve(java.lang.String, boolean) throws java.io.FileNotFoundException;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> getMountPoints();\n  java.lang.String getHomeDirPrefixValue();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolVersionsResponseProto$Builder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.class": "Compiled from \"ZKDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent> {\n  public static final java.lang.String ZK_DTSM_ZK_NUM_RETRIES;\n  public static final java.lang.String ZK_DTSM_ZK_SESSION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_SHUTDOWN_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH;\n  public static final java.lang.String ZK_DTSM_ZK_AUTH_TYPE;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_STRING;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_KEYTAB;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_PRINCIPAL;\n  public static final int ZK_DTSM_ZK_NUM_RETRIES_DEFAULT;\n  public static final int ZK_DTSM_ZK_SESSION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_CONNECTION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_SHUTDOWN_TIMEOUT_DEFAULT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH_DEAFULT;\n  public static void setCurator(org.apache.curator.framework.CuratorFramework);\n  public org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager(org.apache.hadoop.conf.Configuration);\n  public void startThreads() throws java.io.IOException;\n  public void stopThreads();\n  protected int getDelegationTokenSeqNum();\n  protected int incrementDelegationTokenSeqNum();\n  protected void setDelegationTokenSeqNum(int);\n  protected int getCurrentKeyId();\n  protected int incrementCurrentKeyId();\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  static java.lang.String getNodePath(java.lang.String, java.lang.String);\n  public java.util.concurrent.ExecutorService getListenerThreadPool();\n  static void access$100(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, byte[]) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, java.lang.String);\n  static void access$300(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static void access$400(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocketWatcher$Handler.class": "Compiled from \"DomainSocketWatcher.java\"\npublic final class org.apache.hadoop.net.unix.DomainSocketWatcher implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final java.lang.Thread watcherThread;\n  static final boolean $assertionsDisabled;\n  public static java.lang.String getLoadingFailureReason();\n  public org.apache.hadoop.net.unix.DomainSocketWatcher(int, java.lang.String) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void add(org.apache.hadoop.net.unix.DomainSocket, org.apache.hadoop.net.unix.DomainSocketWatcher$Handler);\n  public void remove(org.apache.hadoop.net.unix.DomainSocket);\n  public java.lang.String toString();\n  static java.util.concurrent.locks.ReentrantLock access$000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$102(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static boolean access$202(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static int access$300(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static void access$400(org.apache.hadoop.net.unix.DomainSocketWatcher, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet);\n  static void access$500(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static java.util.LinkedList access$600(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.TreeMap access$700(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.concurrent.locks.Condition access$800(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$200(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static int access$900(int, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet) throws java.io.IOException;\n  static void access$1000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$1100(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns$CacheWith2Keys.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$DataIndex.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/util/NativeLibraryChecker.class": "Compiled from \"NativeLibraryChecker.java\"\npublic class org.apache.hadoop.util.NativeLibraryChecker {\n  public org.apache.hadoop.util.NativeLibraryChecker();\n  public static void main(java.lang.String[]);\n}\n", 
  "org/apache/hadoop/record/compiler/JString.class": "Compiled from \"JString.java\"\npublic class org.apache.hadoop.record.compiler.JString extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JString();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/util/NativeCodeLoader.class": "Compiled from \"NativeCodeLoader.java\"\npublic class org.apache.hadoop.util.NativeCodeLoader {\n  public org.apache.hadoop.util.NativeCodeLoader();\n  public static boolean isNativeCodeLoaded();\n  public static native boolean buildSupportsSnappy();\n  public static native boolean buildSupportsOpenssl();\n  public static native java.lang.String getLibraryName();\n  public boolean getLoadNativeLibraries(org.apache.hadoop.conf.Configuration);\n  public void setLoadNativeLibraries(org.apache.hadoop.conf.Configuration, boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/util/UTF8ByteArrayUtils.class": "Compiled from \"UTF8ByteArrayUtils.java\"\npublic class org.apache.hadoop.util.UTF8ByteArrayUtils {\n  public org.apache.hadoop.util.UTF8ByteArrayUtils();\n  public static int findByte(byte[], int, int, byte);\n  public static int findBytes(byte[], int, int, byte[]);\n  public static int findNthByte(byte[], int, int, byte, int);\n  public static int findNthByte(byte[], byte, int);\n}\n", 
  "org/apache/hadoop/io/compress/lz4/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.io.compress.lz4.package-info {\n}\n", 
  "org/apache/hadoop/fs/BatchedRemoteIterator.class": "Compiled from \"BatchedRemoteIterator.java\"\npublic abstract class org.apache.hadoop.fs.BatchedRemoteIterator<K, E> implements org.apache.hadoop.fs.RemoteIterator<E> {\n  public org.apache.hadoop.fs.BatchedRemoteIterator(K);\n  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<E> makeRequest(K) throws java.io.IOException;\n  public boolean hasNext() throws java.io.IOException;\n  public abstract K elementToPrevKey(E);\n  public E next() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JCompType.class": "Compiled from \"JCompType.java\"\nabstract class org.apache.hadoop.record.compiler.JCompType extends org.apache.hadoop.record.compiler.JType {\n  org.apache.hadoop.record.compiler.JCompType();\n}\n", 
  "org/apache/hadoop/io/BoundedByteArrayOutputStream.class": "Compiled from \"BoundedByteArrayOutputStream.java\"\npublic class org.apache.hadoop.io.BoundedByteArrayOutputStream extends java.io.OutputStream {\n  public org.apache.hadoop.io.BoundedByteArrayOutputStream(int);\n  public org.apache.hadoop.io.BoundedByteArrayOutputStream(int, int);\n  protected org.apache.hadoop.io.BoundedByteArrayOutputStream(byte[], int, int);\n  protected void resetBuffer(byte[], int, int);\n  public void write(int) throws java.io.IOException;\n  public void write(byte[], int, int) throws java.io.IOException;\n  public void reset(int);\n  public void reset();\n  public int getLimit();\n  public byte[] getBuffer();\n  public int size();\n  public int available();\n}\n", 
  "org/apache/hadoop/ipc/Server$1.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsShellPermissions$Chown.class": "Compiled from \"FsShellPermissions.java\"\npublic class org.apache.hadoop.fs.FsShellPermissions extends org.apache.hadoop.fs.shell.FsCommand {\n  static org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.fs.FsShellPermissions();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  static java.lang.String access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HarFileSystem$Store.class": "Compiled from \"HarFileSystem.java\"\npublic class org.apache.hadoop.fs.HarFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final java.lang.String METADATA_CACHE_ENTRIES_KEY;\n  public static final int METADATA_CACHE_ENTRIES_DEFAULT;\n  public static final int VERSION;\n  public org.apache.hadoop.fs.HarFileSystem();\n  public java.lang.String getScheme();\n  public org.apache.hadoop.fs.HarFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public int getHarVersion() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  public java.net.URI getUri();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  static org.apache.hadoop.fs.BlockLocation[] fixBlockLocations(org.apache.hadoop.fs.BlockLocation[], long, long, long);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public static int getHarHash(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long);\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  org.apache.hadoop.fs.HarFileSystem$HarMetaData getMetadata();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  static java.lang.String access$200(org.apache.hadoop.fs.HarFileSystem, java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.hadoop.fs.HarFileSystem$HarMetaData access$300(org.apache.hadoop.fs.HarFileSystem);\n  static java.lang.String access$400(java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.commons.logging.Log access$500();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/ExpressionFactory.class": "Compiled from \"ExpressionFactory.java\"\nfinal class org.apache.hadoop.fs.shell.find.ExpressionFactory {\n  static org.apache.hadoop.fs.shell.find.ExpressionFactory getExpressionFactory();\n  void registerExpression(java.lang.Class<? extends org.apache.hadoop.fs.shell.find.Expression>);\n  void addClass(java.lang.Class<? extends org.apache.hadoop.fs.shell.find.Expression>, java.lang.String...) throws java.io.IOException;\n  boolean isExpression(java.lang.String);\n  org.apache.hadoop.fs.shell.find.Expression getExpression(java.lang.String, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.fs.shell.find.Expression createExpression(java.lang.Class<? extends org.apache.hadoop.fs.shell.find.Expression>, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.fs.shell.find.Expression createExpression(java.lang.String, org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/io/TwoDArrayWritable.class": "Compiled from \"TwoDArrayWritable.java\"\npublic class org.apache.hadoop.io.TwoDArrayWritable implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.io.TwoDArrayWritable(java.lang.Class);\n  public org.apache.hadoop.io.TwoDArrayWritable(java.lang.Class, org.apache.hadoop.io.Writable[][]);\n  public java.lang.Object toArray();\n  public void set(org.apache.hadoop.io.Writable[][]);\n  public org.apache.hadoop.io.Writable[][] get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/HealthMonitor$State.class": "Compiled from \"HealthMonitor.java\"\npublic class org.apache.hadoop.ha.HealthMonitor {\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.HealthMonitor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  public void addCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public void removeCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public synchronized void addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public synchronized void removeServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public void shutdown();\n  public synchronized org.apache.hadoop.ha.HAServiceProtocol getProxy();\n  protected org.apache.hadoop.ha.HAServiceProtocol createProxy() throws java.io.IOException;\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getHealthState();\n  synchronized org.apache.hadoop.ha.HAServiceStatus getLastServiceStatus();\n  boolean isAlive();\n  void join() throws java.lang.InterruptedException;\n  void start();\n  static org.apache.hadoop.ha.HAServiceTarget access$100(org.apache.hadoop.ha.HealthMonitor);\n  static org.apache.commons.logging.Log access$200();\n  static void access$300(org.apache.hadoop.ha.HealthMonitor, org.apache.hadoop.ha.HealthMonitor$State);\n  static boolean access$400(org.apache.hadoop.ha.HealthMonitor);\n  static void access$500(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static void access$600(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/LocalFileSystemConfigKeys.class": "Compiled from \"LocalFileSystemConfigKeys.java\"\npublic class org.apache.hadoop.fs.LocalFileSystemConfigKeys extends org.apache.hadoop.fs.CommonConfigurationKeys {\n  public static final java.lang.String LOCAL_FS_BLOCK_SIZE_KEY;\n  public static final long LOCAL_FS_BLOCK_SIZE_DEFAULT;\n  public static final java.lang.String LOCAL_FS_REPLICATION_KEY;\n  public static final short LOCAL_FS_REPLICATION_DEFAULT;\n  public static final java.lang.String LOCAL_FS_STREAM_BUFFER_SIZE_KEY;\n  public static final int LOCAL_FS_STREAM_BUFFER_SIZE_DEFAULT;\n  public static final java.lang.String LOCAL_FS_BYTES_PER_CHECKSUM_KEY;\n  public static final int LOCAL_FS_BYTES_PER_CHECKSUM_DEFAULT;\n  public static final java.lang.String LOCAL_FS_CLIENT_WRITE_PACKET_SIZE_KEY;\n  public static final int LOCAL_FS_CLIENT_WRITE_PACKET_SIZE_DEFAULT;\n  public org.apache.hadoop.fs.LocalFileSystemConfigKeys();\n}\n", 
  "org/apache/hadoop/record/compiler/JBuffer$CppBuffer.class": "Compiled from \"JBuffer.java\"\npublic class org.apache.hadoop.record.compiler.JBuffer extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JBuffer();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/service/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.service.package-info {\n}\n", 
  "org/apache/hadoop/tracing/SpanReceiverInfo$ConfigurationPair.class": "Compiled from \"SpanReceiverInfo.java\"\npublic class org.apache.hadoop.tracing.SpanReceiverInfo {\n  final java.util.List<org.apache.hadoop.tracing.SpanReceiverInfo$ConfigurationPair> configPairs;\n  org.apache.hadoop.tracing.SpanReceiverInfo(long, java.lang.String);\n  public long getId();\n  public java.lang.String getClassName();\n}\n", 
  "org/apache/hadoop/io/DataOutputOutputStream.class": "Compiled from \"DataOutputOutputStream.java\"\npublic class org.apache.hadoop.io.DataOutputOutputStream extends java.io.OutputStream {\n  public static java.io.OutputStream constructOutputStream(java.io.DataOutput);\n  public void write(int) throws java.io.IOException;\n  public void write(byte[], int, int) throws java.io.IOException;\n  public void write(byte[]) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Compression$FinishOnFlushCompressionStream.class": "Compiled from \"Compression.java\"\nfinal class org.apache.hadoop.io.file.tfile.Compression {\n  static final org.apache.commons.logging.Log LOG;\n  static org.apache.hadoop.io.file.tfile.Compression$Algorithm getCompressionAlgorithmByName(java.lang.String);\n  static java.lang.String[] getSupportedAlgorithms();\n  static {};\n}\n", 
  "org/apache/hadoop/security/LdapGroupsMapping.class": "Compiled from \"LdapGroupsMapping.java\"\npublic class org.apache.hadoop.security.LdapGroupsMapping implements org.apache.hadoop.security.GroupMappingServiceProvider,org.apache.hadoop.conf.Configurable {\n  public static final java.lang.String LDAP_CONFIG_PREFIX;\n  public static final java.lang.String LDAP_URL_KEY;\n  public static final java.lang.String LDAP_URL_DEFAULT;\n  public static final java.lang.String LDAP_USE_SSL_KEY;\n  public static final java.lang.Boolean LDAP_USE_SSL_DEFAULT;\n  public static final java.lang.String LDAP_KEYSTORE_KEY;\n  public static final java.lang.String LDAP_KEYSTORE_DEFAULT;\n  public static final java.lang.String LDAP_KEYSTORE_PASSWORD_KEY;\n  public static final java.lang.String LDAP_KEYSTORE_PASSWORD_DEFAULT;\n  public static final java.lang.String LDAP_KEYSTORE_PASSWORD_FILE_KEY;\n  public static final java.lang.String LDAP_KEYSTORE_PASSWORD_FILE_DEFAULT;\n  public static final java.lang.String BIND_USER_KEY;\n  public static final java.lang.String BIND_USER_DEFAULT;\n  public static final java.lang.String BIND_PASSWORD_KEY;\n  public static final java.lang.String BIND_PASSWORD_DEFAULT;\n  public static final java.lang.String BIND_PASSWORD_FILE_KEY;\n  public static final java.lang.String BIND_PASSWORD_FILE_DEFAULT;\n  public static final java.lang.String BASE_DN_KEY;\n  public static final java.lang.String BASE_DN_DEFAULT;\n  public static final java.lang.String USER_SEARCH_FILTER_KEY;\n  public static final java.lang.String USER_SEARCH_FILTER_DEFAULT;\n  public static final java.lang.String GROUP_SEARCH_FILTER_KEY;\n  public static final java.lang.String GROUP_SEARCH_FILTER_DEFAULT;\n  public static final java.lang.String GROUP_MEMBERSHIP_ATTR_KEY;\n  public static final java.lang.String GROUP_MEMBERSHIP_ATTR_DEFAULT;\n  public static final java.lang.String GROUP_NAME_ATTR_KEY;\n  public static final java.lang.String GROUP_NAME_ATTR_DEFAULT;\n  public static final java.lang.String DIRECTORY_SEARCH_TIMEOUT;\n  public static final int DIRECTORY_SEARCH_TIMEOUT_DEFAULT;\n  public static int RECONNECT_RETRY_COUNT;\n  public org.apache.hadoop.security.LdapGroupsMapping();\n  public synchronized java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  java.util.List<java.lang.String> doGetGroups(java.lang.String) throws javax.naming.NamingException;\n  javax.naming.directory.DirContext getDirContext() throws javax.naming.NamingException;\n  public void cacheGroupsRefresh() throws java.io.IOException;\n  public void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n  public synchronized org.apache.hadoop.conf.Configuration getConf();\n  public synchronized void setConf(org.apache.hadoop.conf.Configuration);\n  java.lang.String getPassword(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String);\n  java.lang.String extractPassword(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/CodeGenerator.class": "Compiled from \"CodeGenerator.java\"\nabstract class org.apache.hadoop.record.compiler.CodeGenerator {\n  org.apache.hadoop.record.compiler.CodeGenerator();\n  static void register(java.lang.String, org.apache.hadoop.record.compiler.CodeGenerator);\n  static org.apache.hadoop.record.compiler.CodeGenerator get(java.lang.String);\n  abstract void genCode(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JFile>, java.util.ArrayList<org.apache.hadoop.record.compiler.JRecord>, java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/AuthenticationFilterInitializer.class": "Compiled from \"AuthenticationFilterInitializer.java\"\npublic class org.apache.hadoop.security.AuthenticationFilterInitializer extends org.apache.hadoop.http.FilterInitializer {\n  static final java.lang.String PREFIX;\n  public org.apache.hadoop.security.AuthenticationFilterInitializer();\n  public void initFilter(org.apache.hadoop.http.FilterContainer, org.apache.hadoop.conf.Configuration);\n  public static java.util.Map<java.lang.String, java.lang.String> getFilterConfigMap(org.apache.hadoop.conf.Configuration, java.lang.String);\n}\n", 
  "org/apache/hadoop/io/BooleanWritable$Comparator.class": "Compiled from \"BooleanWritable.java\"\npublic class org.apache.hadoop.io.BooleanWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.BooleanWritable> {\n  public org.apache.hadoop.io.BooleanWritable();\n  public org.apache.hadoop.io.BooleanWritable(boolean);\n  public void set(boolean);\n  public boolean get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.BooleanWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/util/LimitInputStream.class": "Compiled from \"LimitInputStream.java\"\npublic final class org.apache.hadoop.util.LimitInputStream extends java.io.FilterInputStream {\n  public org.apache.hadoop.util.LimitInputStream(java.io.InputStream, long);\n  public int available() throws java.io.IOException;\n  public synchronized void mark(int);\n  public int read() throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public synchronized void reset() throws java.io.IOException;\n  public long skip(long) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/HardLink$HardLinkCommandGetter.class": "Compiled from \"HardLink.java\"\npublic class org.apache.hadoop.fs.HardLink {\n  public final org.apache.hadoop.fs.HardLink$LinkStats linkStats;\n  public org.apache.hadoop.fs.HardLink();\n  public static void createHardLink(java.io.File, java.io.File) throws java.io.IOException;\n  public static void createHardLinkMult(java.io.File, java.lang.String[], java.io.File) throws java.io.IOException;\n  public static int getLinkCount(java.io.File) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/CredentialProvider$CredentialEntry.class": "Compiled from \"CredentialProvider.java\"\npublic abstract class org.apache.hadoop.security.alias.CredentialProvider {\n  public static final java.lang.String CLEAR_TEXT_FALLBACK;\n  public org.apache.hadoop.security.alias.CredentialProvider();\n  public boolean isTransient();\n  public abstract void flush() throws java.io.IOException;\n  public abstract org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry getCredentialEntry(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> getAliases() throws java.io.IOException;\n  public abstract org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry createCredentialEntry(java.lang.String, char[]) throws java.io.IOException;\n  public abstract void deleteCredentialEntry(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/RawLocalFileSystem$1.class": "Compiled from \"RawLocalFileSystem.java\"\npublic class org.apache.hadoop.fs.RawLocalFileSystem extends org.apache.hadoop.fs.FileSystem {\n  static final java.net.URI NAME;\n  public static void useStatIfAvailable();\n  public org.apache.hadoop.fs.RawLocalFileSystem();\n  public java.io.File pathToFile(org.apache.hadoop.fs.Path);\n  public java.net.URI getUri();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStream(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStreamWithMode(org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean mkOneDir(java.io.File) throws java.io.IOException;\n  protected boolean mkOneDirWithMode(org.apache.hadoop.fs.Path, java.io.File, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/OpensslCipher$AlgMode.class": "Compiled from \"OpensslCipher.java\"\npublic final class org.apache.hadoop.crypto.OpensslCipher {\n  public static final int ENCRYPT_MODE;\n  public static final int DECRYPT_MODE;\n  public static java.lang.String getLoadingFailureReason();\n  public static final org.apache.hadoop.crypto.OpensslCipher getInstance(java.lang.String) throws java.security.NoSuchAlgorithmException, javax.crypto.NoSuchPaddingException;\n  public void init(int, byte[], byte[]);\n  public int update(java.nio.ByteBuffer, java.nio.ByteBuffer) throws javax.crypto.ShortBufferException;\n  public int doFinal(java.nio.ByteBuffer) throws javax.crypto.ShortBufferException, javax.crypto.IllegalBlockSizeException, javax.crypto.BadPaddingException;\n  public void clean();\n  protected void finalize() throws java.lang.Throwable;\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$GetDelegationTokenRequestProtoOrBuilder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderCryptoExtension$DefaultCryptoExtension.class": "Compiled from \"KeyProviderCryptoExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderCryptoExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension> {\n  public static final java.lang.String EEK;\n  public static final java.lang.String EK;\n  protected org.apache.hadoop.crypto.key.KeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider, org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension);\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public static org.apache.hadoop.crypto.key.KeyProviderCryptoExtension createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/Server$Responder.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns$Info$1.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/Util.class": "Compiled from \"Util.java\"\npublic class org.apache.hadoop.metrics.spi.Util {\n  public static java.util.List<java.net.InetSocketAddress> parse(java.lang.String, int);\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$POSIX$IdCache.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/util/DiskChecker.class": "Compiled from \"DiskChecker.java\"\npublic class org.apache.hadoop.util.DiskChecker {\n  public org.apache.hadoop.util.DiskChecker();\n  public static boolean mkdirsWithExistsCheck(java.io.File);\n  public static void checkDirs(java.io.File) throws org.apache.hadoop.util.DiskChecker$DiskErrorException;\n  public static void checkDir(java.io.File) throws org.apache.hadoop.util.DiskChecker$DiskErrorException;\n  public static void mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static void checkDir(org.apache.hadoop.fs.LocalFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.util.DiskChecker$DiskErrorException, java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/ArrayFile$Writer.class": "Compiled from \"ArrayFile.java\"\npublic class org.apache.hadoop.io.ArrayFile extends org.apache.hadoop.io.MapFile {\n  protected org.apache.hadoop.io.ArrayFile();\n}\n", 
  "org/apache/hadoop/metrics2/sink/GraphiteSink.class": "Compiled from \"GraphiteSink.java\"\npublic class org.apache.hadoop.metrics2.sink.GraphiteSink implements org.apache.hadoop.metrics2.MetricsSink,java.io.Closeable {\n  public org.apache.hadoop.metrics2.sink.GraphiteSink();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  public void putMetrics(org.apache.hadoop.metrics2.MetricsRecord);\n  public void flush();\n  public void close() throws java.io.IOException;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto$SaslAuth$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSelector.class": "Compiled from \"AbstractDelegationTokenSelector.java\"\npublic class org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> implements org.apache.hadoop.security.token.TokenSelector<TokenIdent> {\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.security.token.Token<TokenIdent> selectToken(org.apache.hadoop.io.Text, java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>>);\n}\n", 
  "org/apache/hadoop/metrics2/MetricsSource.class": "Compiled from \"MetricsSource.java\"\npublic interface org.apache.hadoop.metrics2.MetricsSource {\n  public abstract void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n}\n", 
  "org/apache/hadoop/metrics2/lib/MetricsSourceBuilder$1.class": "Compiled from \"MetricsSourceBuilder.java\"\npublic class org.apache.hadoop.metrics2.lib.MetricsSourceBuilder {\n  org.apache.hadoop.metrics2.lib.MetricsSourceBuilder(java.lang.Object, org.apache.hadoop.metrics2.lib.MutableMetricsFactory);\n  public org.apache.hadoop.metrics2.MetricsSource build();\n  public org.apache.hadoop.metrics2.MetricsInfo info();\n  static org.apache.hadoop.metrics2.lib.MetricsRegistry access$000(org.apache.hadoop.metrics2.lib.MetricsSourceBuilder);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/Decryptor.class": "Compiled from \"Decryptor.java\"\npublic interface org.apache.hadoop.crypto.Decryptor {\n  public abstract void init(byte[], byte[]) throws java.io.IOException;\n  public abstract boolean isContextReset();\n  public abstract void decrypt(java.nio.ByteBuffer, java.nio.ByteBuffer) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager$JaasConfiguration.class": "Compiled from \"ZKDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent> {\n  public static final java.lang.String ZK_DTSM_ZK_NUM_RETRIES;\n  public static final java.lang.String ZK_DTSM_ZK_SESSION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZK_SHUTDOWN_TIMEOUT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH;\n  public static final java.lang.String ZK_DTSM_ZK_AUTH_TYPE;\n  public static final java.lang.String ZK_DTSM_ZK_CONNECTION_STRING;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_KEYTAB;\n  public static final java.lang.String ZK_DTSM_ZK_KERBEROS_PRINCIPAL;\n  public static final int ZK_DTSM_ZK_NUM_RETRIES_DEFAULT;\n  public static final int ZK_DTSM_ZK_SESSION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_CONNECTION_TIMEOUT_DEFAULT;\n  public static final int ZK_DTSM_ZK_SHUTDOWN_TIMEOUT_DEFAULT;\n  public static final java.lang.String ZK_DTSM_ZNODE_WORKING_PATH_DEAFULT;\n  public static void setCurator(org.apache.curator.framework.CuratorFramework);\n  public org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager(org.apache.hadoop.conf.Configuration);\n  public void startThreads() throws java.io.IOException;\n  public void stopThreads();\n  protected int getDelegationTokenSeqNum();\n  protected int incrementDelegationTokenSeqNum();\n  protected void setDelegationTokenSeqNum(int);\n  protected int getCurrentKeyId();\n  protected int incrementCurrentKeyId();\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  static java.lang.String getNodePath(java.lang.String, java.lang.String);\n  public java.util.concurrent.ExecutorService getListenerThreadPool();\n  static void access$100(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, byte[]) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, java.lang.String);\n  static void access$300(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static void access$400(org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager, org.apache.curator.framework.recipes.cache.ChildData) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsServerDefaults$1.class": "Compiled from \"FsServerDefaults.java\"\npublic class org.apache.hadoop.fs.FsServerDefaults implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.fs.FsServerDefaults();\n  public org.apache.hadoop.fs.FsServerDefaults(long, int, int, short, int, boolean, long, org.apache.hadoop.util.DataChecksum$Type);\n  public long getBlockSize();\n  public int getBytesPerChecksum();\n  public int getWritePacketSize();\n  public short getReplication();\n  public int getFileBufferSize();\n  public boolean getEncryptDataTransfer();\n  public long getTrashInterval();\n  public org.apache.hadoop.util.DataChecksum$Type getChecksumType();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/ShutdownHookManager$2.class": "Compiled from \"ShutdownHookManager.java\"\npublic class org.apache.hadoop.util.ShutdownHookManager {\n  public static org.apache.hadoop.util.ShutdownHookManager get();\n  java.util.List<java.lang.Runnable> getShutdownHooksInOrder();\n  public void addShutdownHook(java.lang.Runnable, int);\n  public boolean removeShutdownHook(java.lang.Runnable);\n  public boolean hasShutdownHook(java.lang.Runnable);\n  public boolean isShutdownInProgress();\n  static org.apache.hadoop.util.ShutdownHookManager access$000();\n  static java.util.concurrent.atomic.AtomicBoolean access$100(org.apache.hadoop.util.ShutdownHookManager);\n  static org.apache.commons.logging.Log access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HealthMonitor$Callback.class": "Compiled from \"HealthMonitor.java\"\npublic class org.apache.hadoop.ha.HealthMonitor {\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.HealthMonitor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  public void addCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public void removeCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public synchronized void addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public synchronized void removeServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public void shutdown();\n  public synchronized org.apache.hadoop.ha.HAServiceProtocol getProxy();\n  protected org.apache.hadoop.ha.HAServiceProtocol createProxy() throws java.io.IOException;\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getHealthState();\n  synchronized org.apache.hadoop.ha.HAServiceStatus getLastServiceStatus();\n  boolean isAlive();\n  void join() throws java.lang.InterruptedException;\n  void start();\n  static org.apache.hadoop.ha.HAServiceTarget access$100(org.apache.hadoop.ha.HealthMonitor);\n  static org.apache.commons.logging.Log access$200();\n  static void access$300(org.apache.hadoop.ha.HealthMonitor, org.apache.hadoop.ha.HealthMonitor$State);\n  static boolean access$400(org.apache.hadoop.ha.HealthMonitor);\n  static void access$500(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static void access$600(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/hash/JenkinsHash.class": "Compiled from \"JenkinsHash.java\"\npublic class org.apache.hadoop.util.hash.JenkinsHash extends org.apache.hadoop.util.hash.Hash {\n  public org.apache.hadoop.util.hash.JenkinsHash();\n  public static org.apache.hadoop.util.hash.Hash getInstance();\n  public int hash(byte[], int, int);\n  public static void main(java.lang.String[]) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/sink/ganglia/GangliaConf.class": "Compiled from \"GangliaConf.java\"\nclass org.apache.hadoop.metrics2.sink.ganglia.GangliaConf {\n  org.apache.hadoop.metrics2.sink.ganglia.GangliaConf();\n  public java.lang.String toString();\n  java.lang.String getUnits();\n  void setUnits(java.lang.String);\n  org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope getSlope();\n  void setSlope(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope);\n  int getDmax();\n  void setDmax(int);\n  int getTmax();\n  void setTmax(int);\n}\n", 
  "org/apache/hadoop/io/file/tfile/Chunk$ChunkEncoder.class": "Compiled from \"Chunk.java\"\nfinal class org.apache.hadoop.io.file.tfile.Chunk {\n}\n", 
  "org/apache/hadoop/io/retry/RetryUtils$1.class": "Compiled from \"RetryUtils.java\"\npublic class org.apache.hadoop.io.retry.RetryUtils {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.io.retry.RetryUtils();\n  public static org.apache.hadoop.io.retry.RetryPolicy getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration, java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.Class<? extends java.lang.Exception>);\n  public static org.apache.hadoop.io.retry.RetryPolicy getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration, java.lang.String, boolean, java.lang.String, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/NodeFencer$1.class": "Compiled from \"NodeFencer.java\"\npublic class org.apache.hadoop.ha.NodeFencer {\n  org.apache.hadoop.ha.NodeFencer(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public static org.apache.hadoop.ha.NodeFencer create(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public boolean fence(org.apache.hadoop.ha.HAServiceTarget);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/util/SampleStat.class": "Compiled from \"SampleStat.java\"\npublic class org.apache.hadoop.metrics2.util.SampleStat {\n  public org.apache.hadoop.metrics2.util.SampleStat();\n  public void reset();\n  void reset(long, double, double, double, double, org.apache.hadoop.metrics2.util.SampleStat$MinMax);\n  public void copyTo(org.apache.hadoop.metrics2.util.SampleStat);\n  public org.apache.hadoop.metrics2.util.SampleStat add(double);\n  public org.apache.hadoop.metrics2.util.SampleStat add(long, double);\n  public long numSamples();\n  public double mean();\n  public double variance();\n  public double stddev();\n  public double min();\n  public double max();\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$Writer$KeyRegister.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/FairCallQueue$MetricsProxy.class": "Compiled from \"FairCallQueue.java\"\npublic class org.apache.hadoop.ipc.FairCallQueue<E extends org.apache.hadoop.ipc.Schedulable> extends java.util.AbstractQueue<E> implements java.util.concurrent.BlockingQueue<E> {\n  public static final int IPC_CALLQUEUE_PRIORITY_LEVELS_DEFAULT;\n  public static final java.lang.String IPC_CALLQUEUE_PRIORITY_LEVELS_KEY;\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.FairCallQueue(int, java.lang.String, org.apache.hadoop.conf.Configuration);\n  public void put(E) throws java.lang.InterruptedException;\n  public boolean offer(E, long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public boolean offer(E);\n  public E take() throws java.lang.InterruptedException;\n  public E poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public E poll();\n  public E peek();\n  public int size();\n  public java.util.Iterator<E> iterator();\n  public int drainTo(java.util.Collection<? super E>, int);\n  public int drainTo(java.util.Collection<? super E>);\n  public int remainingCapacity();\n  public int[] getQueueSizes();\n  public long[] getOverflowedCalls();\n  public void setScheduler(org.apache.hadoop.ipc.RpcScheduler);\n  public void setMultiplexer(org.apache.hadoop.ipc.RpcMultiplexer);\n  public java.lang.Object peek();\n  public java.lang.Object poll();\n  public boolean offer(java.lang.Object);\n  public java.lang.Object poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public java.lang.Object take() throws java.lang.InterruptedException;\n  public boolean offer(java.lang.Object, long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public void put(java.lang.Object) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$GetServiceStatusRequestProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/LongWritable$DecreasingComparator.class": "Compiled from \"LongWritable.java\"\npublic class org.apache.hadoop.io.LongWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.LongWritable> {\n  public org.apache.hadoop.io.LongWritable();\n  public org.apache.hadoop.io.LongWritable(long);\n  public void set(long);\n  public long get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.LongWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RetryCache.class": "Compiled from \"RetryCache.java\"\npublic class org.apache.hadoop.ipc.RetryCache {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.RetryCache(java.lang.String, double, long);\n  public void lock();\n  public void unlock();\n  public org.apache.hadoop.util.LightWeightGSet<org.apache.hadoop.ipc.RetryCache$CacheEntry, org.apache.hadoop.ipc.RetryCache$CacheEntry> getCacheSet();\n  public org.apache.hadoop.ipc.metrics.RetryCacheMetrics getMetricsForTests();\n  public java.lang.String getCacheName();\n  public void addCacheEntry(byte[], int);\n  public void addCacheEntryWithPayload(byte[], int, java.lang.Object);\n  public static org.apache.hadoop.ipc.RetryCache$CacheEntry waitForCompletion(org.apache.hadoop.ipc.RetryCache);\n  public static org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload waitForCompletion(org.apache.hadoop.ipc.RetryCache, java.lang.Object);\n  public static void setState(org.apache.hadoop.ipc.RetryCache$CacheEntry, boolean);\n  public static void setState(org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload, boolean, java.lang.Object);\n  public static void clear(org.apache.hadoop.ipc.RetryCache);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$2.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsShell$UnknownCommandException.class": "Compiled from \"FsShell.java\"\npublic class org.apache.hadoop.fs.FsShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.fs.shell.CommandFactory commandFactory;\n  public org.apache.hadoop.fs.FsShell();\n  public org.apache.hadoop.fs.FsShell(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.fs.FileSystem getFS() throws java.io.IOException;\n  protected org.apache.hadoop.fs.Trash getTrash() throws java.io.IOException;\n  protected void init() throws java.io.IOException;\n  protected void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.Path getCurrentTrashDir() throws java.io.IOException;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public void close() throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  protected static org.apache.hadoop.fs.FsShell newShellInstance();\n  static void access$000(org.apache.hadoop.fs.FsShell, java.io.PrintStream);\n  static void access$100(org.apache.hadoop.fs.FsShell, java.io.PrintStream, java.lang.String);\n  static void access$200(org.apache.hadoop.fs.FsShell, java.io.PrintStream);\n  static void access$300(org.apache.hadoop.fs.FsShell, java.io.PrintStream, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshAuthorizationPolicyProtocolService$2.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$5.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$ParsedTimeDuration$5.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/Groups.class": "Compiled from \"Groups.java\"\npublic class org.apache.hadoop.security.Groups {\n  public org.apache.hadoop.security.Groups(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.security.Groups(org.apache.hadoop.conf.Configuration, org.apache.hadoop.util.Timer);\n  java.util.Set<java.lang.String> getNegativeCache();\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void refresh();\n  public void cacheGroupsAdd(java.util.List<java.lang.String>);\n  public static org.apache.hadoop.security.Groups getUserToGroupsMappingService();\n  public static synchronized org.apache.hadoop.security.Groups getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration);\n  public static synchronized org.apache.hadoop.security.Groups getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration);\n  static boolean access$100(org.apache.hadoop.security.Groups);\n  static java.util.Set access$200(org.apache.hadoop.security.Groups);\n  static java.io.IOException access$300(org.apache.hadoop.security.Groups, java.lang.String);\n  static org.apache.hadoop.util.Timer access$400(org.apache.hadoop.security.Groups);\n  static org.apache.hadoop.security.GroupMappingServiceProvider access$500(org.apache.hadoop.security.Groups);\n  static long access$600(org.apache.hadoop.security.Groups);\n  static org.apache.commons.logging.Log access$700();\n  static {};\n}\n", 
  "org/apache/hadoop/io/SetFile$Reader.class": "Compiled from \"SetFile.java\"\npublic class org.apache.hadoop.io.SetFile extends org.apache.hadoop.io.MapFile {\n  protected org.apache.hadoop.io.SetFile();\n}\n", 
  "org/apache/hadoop/metrics2/annotation/Metrics.class": "Compiled from \"Metrics.java\"\npublic interface org.apache.hadoop.metrics2.annotation.Metrics extends java.lang.annotation.Annotation {\n  public abstract java.lang.String name();\n  public abstract java.lang.String about();\n  public abstract java.lang.String context();\n}\n", 
  "org/apache/hadoop/ipc/RpcScheduler.class": "Compiled from \"RpcScheduler.java\"\npublic interface org.apache.hadoop.ipc.RpcScheduler {\n  public abstract int getPriorityLevel(org.apache.hadoop.ipc.Schedulable);\n}\n", 
  "org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.class": "Compiled from \"CryptoFSDataInputStream.java\"\npublic class org.apache.hadoop.fs.crypto.CryptoFSDataInputStream extends org.apache.hadoop.fs.FSDataInputStream {\n  public org.apache.hadoop.fs.crypto.CryptoFSDataInputStream(org.apache.hadoop.fs.FSDataInputStream, org.apache.hadoop.crypto.CryptoCodec, int, byte[], byte[]) throws java.io.IOException;\n  public org.apache.hadoop.fs.crypto.CryptoFSDataInputStream(org.apache.hadoop.fs.FSDataInputStream, org.apache.hadoop.crypto.CryptoCodec, byte[], byte[]) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/IOUtils.class": "Compiled from \"IOUtils.java\"\npublic class org.apache.hadoop.io.IOUtils {\n  public org.apache.hadoop.io.IOUtils();\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, int, boolean) throws java.io.IOException;\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, int) throws java.io.IOException;\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, long, boolean) throws java.io.IOException;\n  public static int wrappedReadForCompressedData(java.io.InputStream, byte[], int, int) throws java.io.IOException;\n  public static void readFully(java.io.InputStream, byte[], int, int) throws java.io.IOException;\n  public static void skipFully(java.io.InputStream, long) throws java.io.IOException;\n  public static void cleanup(org.apache.commons.logging.Log, java.io.Closeable...);\n  public static void closeStream(java.io.Closeable);\n  public static void closeSocket(java.net.Socket);\n  public static void writeFully(java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  public static void writeFully(java.nio.channels.FileChannel, java.nio.ByteBuffer, long) throws java.io.IOException;\n  public static java.util.List<java.lang.String> listDirectory(java.io.File, java.io.FilenameFilter) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/service/CompositeService$CompositeServiceShutdownHook.class": "Compiled from \"CompositeService.java\"\npublic class org.apache.hadoop.service.CompositeService extends org.apache.hadoop.service.AbstractService {\n  protected static final boolean STOP_ONLY_STARTED_SERVICES;\n  public org.apache.hadoop.service.CompositeService(java.lang.String);\n  public java.util.List<org.apache.hadoop.service.Service> getServices();\n  protected void addService(org.apache.hadoop.service.Service);\n  protected boolean addIfService(java.lang.Object);\n  protected synchronized boolean removeService(org.apache.hadoop.service.Service);\n  protected void serviceInit(org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  protected void serviceStart() throws java.lang.Exception;\n  protected void serviceStop() throws java.lang.Exception;\n  static {};\n}\n", 
  "org/apache/hadoop/log/LogLevel$Servlet.class": "Compiled from \"LogLevel.java\"\npublic class org.apache.hadoop.log.LogLevel {\n  public static final java.lang.String USAGES;\n  static final java.lang.String MARKER;\n  static final java.util.regex.Pattern TAG;\n  public org.apache.hadoop.log.LogLevel();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider$TimeoutConnConfigurator.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFsFileStatus.class": "Compiled from \"ViewFsFileStatus.java\"\nclass org.apache.hadoop.fs.viewfs.ViewFsFileStatus extends org.apache.hadoop.fs.FileStatus {\n  final org.apache.hadoop.fs.FileStatus myFs;\n  org.apache.hadoop.fs.Path modifiedPath;\n  org.apache.hadoop.fs.viewfs.ViewFsFileStatus(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.Path);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public long getLen();\n  public boolean isFile();\n  public boolean isDirectory();\n  public boolean isDir();\n  public boolean isSymlink();\n  public long getBlockSize();\n  public short getReplication();\n  public long getModificationTime();\n  public long getAccessTime();\n  public org.apache.hadoop.fs.permission.FsPermission getPermission();\n  public java.lang.String getOwner();\n  public java.lang.String getGroup();\n  public org.apache.hadoop.fs.Path getPath();\n  public void setPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getSymlink() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts$BufferSize.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableRates.class": "Compiled from \"MutableRates.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableRates extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.metrics2.lib.MutableRates(org.apache.hadoop.metrics2.lib.MetricsRegistry);\n  public void init(java.lang.Class<?>);\n  public void add(java.lang.String, long);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/io/ShortWritable.class": "Compiled from \"ShortWritable.java\"\npublic class org.apache.hadoop.io.ShortWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.ShortWritable> {\n  public org.apache.hadoop.io.ShortWritable();\n  public org.apache.hadoop.io.ShortWritable(short);\n  public void set(short);\n  public short get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.ShortWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/zlib/ZlibDecompressor$ZlibDirectDecompressor.class": "Compiled from \"ZlibDecompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.ZlibDecompressor implements org.apache.hadoop.io.compress.Decompressor {\n  static final boolean $assertionsDisabled;\n  static boolean isNativeZlibLoaded();\n  public org.apache.hadoop.io.compress.zlib.ZlibDecompressor(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader, int);\n  public org.apache.hadoop.io.compress.zlib.ZlibDecompressor();\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public boolean needsDictionary();\n  public boolean finished();\n  public int decompress(byte[], int, int) throws java.io.IOException;\n  public long getBytesWritten();\n  public long getBytesRead();\n  public int getRemaining();\n  public void reset();\n  public void end();\n  protected void finalize();\n  int inflateDirect(java.nio.ByteBuffer, java.nio.ByteBuffer) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$MultipleLinearRandomRetry$Pair.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$11.class": "", 
  "org/apache/hadoop/io/RawComparator.class": "Compiled from \"RawComparator.java\"\npublic interface org.apache.hadoop.io.RawComparator<T> extends java.util.Comparator<T> {\n  public abstract int compare(byte[], int, int, byte[], int, int);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolSignatureRequestProto.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/JavaKeyStoreProvider$1.class": "Compiled from \"JavaKeyStoreProvider.java\"\npublic class org.apache.hadoop.crypto.key.JavaKeyStoreProvider extends org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String KEYSTORE_PASSWORD_FILE_KEY;\n  public static final java.lang.String KEYSTORE_PASSWORD_ENV_VAR;\n  public static final char[] KEYSTORE_PASSWORD_DEFAULT;\n  org.apache.hadoop.crypto.key.JavaKeyStoreProvider(org.apache.hadoop.crypto.key.JavaKeyStoreProvider);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.crypto.key.KeyProvider$KeyVersion innerSetKeyVersion(java.lang.String, java.lang.String, byte[], java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  protected void writeToNew(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean backupToOld(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.lang.String toString();\n  org.apache.hadoop.crypto.key.JavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.crypto.key.JavaKeyStoreProvider$1) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$TraceAdminService$BlockingInterface.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer$SelectChannelConnectorWithSafeStartup.class": "Compiled from \"HttpServer.java\"\npublic class org.apache.hadoop.http.HttpServer implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.Connector listener;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[]) throws java.io.IOException;\n  public org.mortbay.jetty.Connector createBaseListener(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean) throws java.io.IOException;\n  protected void addContext(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public void setThreads(int, int);\n  public void addSslListener(java.net.InetSocketAddress, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void addSslListener(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  protected void initSpnego(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void start() throws java.io.IOException;\n  void openListener() throws java.lang.Exception;\n  public java.net.InetSocketAddress getListenerAddress();\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  static org.apache.hadoop.security.ssl.SSLFactory access$000(org.apache.hadoop.http.HttpServer);\n  static {};\n}\n", 
  "org/apache/hadoop/net/NetUtils.class": "Compiled from \"NetUtils.java\"\npublic class org.apache.hadoop.net.NetUtils {\n  public static final java.lang.String UNKNOWN_HOST;\n  public static final java.lang.String HADOOP_WIKI;\n  public org.apache.hadoop.net.NetUtils();\n  public static javax.net.SocketFactory getSocketFactory(org.apache.hadoop.conf.Configuration, java.lang.Class<?>);\n  public static javax.net.SocketFactory getDefaultSocketFactory(org.apache.hadoop.conf.Configuration);\n  public static javax.net.SocketFactory getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public static java.net.InetSocketAddress createSocketAddr(java.lang.String);\n  public static java.net.InetSocketAddress createSocketAddr(java.lang.String, int);\n  public static java.net.InetSocketAddress createSocketAddr(java.lang.String, int, java.lang.String);\n  public static java.net.InetSocketAddress createSocketAddrForHost(java.lang.String, int);\n  public static java.net.URI getCanonicalUri(java.net.URI, int);\n  public static void addStaticResolution(java.lang.String, java.lang.String);\n  public static java.lang.String getStaticResolution(java.lang.String);\n  public static java.util.List<java.lang.String[]> getAllStaticResolutions();\n  public static java.net.InetSocketAddress getConnectAddress(org.apache.hadoop.ipc.Server);\n  public static java.net.InetSocketAddress getConnectAddress(java.net.InetSocketAddress);\n  public static org.apache.hadoop.net.SocketInputWrapper getInputStream(java.net.Socket) throws java.io.IOException;\n  public static org.apache.hadoop.net.SocketInputWrapper getInputStream(java.net.Socket, long) throws java.io.IOException;\n  public static java.io.OutputStream getOutputStream(java.net.Socket) throws java.io.IOException;\n  public static java.io.OutputStream getOutputStream(java.net.Socket, long) throws java.io.IOException;\n  public static void connect(java.net.Socket, java.net.SocketAddress, int) throws java.io.IOException;\n  public static void connect(java.net.Socket, java.net.SocketAddress, java.net.SocketAddress, int) throws java.io.IOException;\n  public static java.lang.String normalizeHostName(java.lang.String);\n  public static java.util.List<java.lang.String> normalizeHostNames(java.util.Collection<java.lang.String>);\n  public static void verifyHostnames(java.lang.String[]) throws java.net.UnknownHostException;\n  public static java.lang.String getHostNameOfIP(java.lang.String);\n  public static java.lang.String getHostname();\n  public static java.lang.String getHostPortString(java.net.InetSocketAddress);\n  public static java.net.InetAddress getLocalInetAddress(java.lang.String) throws java.net.SocketException;\n  public static boolean isLocalAddress(java.net.InetAddress);\n  public static java.io.IOException wrapException(java.lang.String, int, java.lang.String, int, java.io.IOException);\n  public static boolean isValidSubnet(java.lang.String);\n  public static java.util.List<java.net.InetAddress> getIPs(java.lang.String, boolean);\n  public static int getFreeSocketPort();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/BadFencingConfigurationException.class": "Compiled from \"BadFencingConfigurationException.java\"\npublic class org.apache.hadoop.ha.BadFencingConfigurationException extends java.io.IOException {\n  public org.apache.hadoop.ha.BadFencingConfigurationException(java.lang.String);\n  public org.apache.hadoop.ha.BadFencingConfigurationException(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/conf/Configuration$ParsedTimeDuration$6.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$6.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$8.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/record/BinaryRecordOutput.class": "Compiled from \"BinaryRecordOutput.java\"\npublic class org.apache.hadoop.record.BinaryRecordOutput implements org.apache.hadoop.record.RecordOutput {\n  public static org.apache.hadoop.record.BinaryRecordOutput get(java.io.DataOutput);\n  public org.apache.hadoop.record.BinaryRecordOutput(java.io.OutputStream);\n  public org.apache.hadoop.record.BinaryRecordOutput(java.io.DataOutput);\n  public void writeByte(byte, java.lang.String) throws java.io.IOException;\n  public void writeBool(boolean, java.lang.String) throws java.io.IOException;\n  public void writeInt(int, java.lang.String) throws java.io.IOException;\n  public void writeLong(long, java.lang.String) throws java.io.IOException;\n  public void writeFloat(float, java.lang.String) throws java.io.IOException;\n  public void writeDouble(double, java.lang.String) throws java.io.IOException;\n  public void writeString(java.lang.String, java.lang.String) throws java.io.IOException;\n  public void writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String) throws java.io.IOException;\n  public void startRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public void endRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public void startVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public void endVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public void startMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n  public void endMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.record.BinaryRecordOutput(org.apache.hadoop.record.BinaryRecordOutput$1);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Chunk$ChunkDecoder.class": "Compiled from \"Chunk.java\"\nfinal class org.apache.hadoop.io.file.tfile.Chunk {\n}\n", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$ZKFCProtocolService$2.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsFilter.class": "Compiled from \"MetricsFilter.java\"\npublic abstract class org.apache.hadoop.metrics2.MetricsFilter implements org.apache.hadoop.metrics2.MetricsPlugin {\n  public org.apache.hadoop.metrics2.MetricsFilter();\n  public abstract boolean accepts(java.lang.String);\n  public abstract boolean accepts(org.apache.hadoop.metrics2.MetricsTag);\n  public abstract boolean accepts(java.lang.Iterable<org.apache.hadoop.metrics2.MetricsTag>);\n  public boolean accepts(org.apache.hadoop.metrics2.MetricsRecord);\n}\n", 
  "org/apache/hadoop/security/token/delegation/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.security.token.delegation.package-info {\n}\n", 
  "org/apache/hadoop/io/AbstractMapWritable.class": "Compiled from \"AbstractMapWritable.java\"\npublic abstract class org.apache.hadoop.io.AbstractMapWritable implements org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configurable {\n  java.util.Map<java.lang.Class<?>, java.lang.Byte> classToIdMap;\n  java.util.Map<java.lang.Byte, java.lang.Class<?>> idToClassMap;\n  byte getNewClasses();\n  protected synchronized void addToMap(java.lang.Class<?>);\n  protected java.lang.Class<?> getClass(byte);\n  protected byte getId(java.lang.Class<?>);\n  protected synchronized void copy(org.apache.hadoop.io.Writable);\n  protected org.apache.hadoop.io.AbstractMapWritable();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/Shell$ShellTimeoutTimerTask.class": "Compiled from \"Shell.java\"\npublic abstract class org.apache.hadoop.util.Shell {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int WINDOWS_MAX_SHELL_LENGHT;\n  public static final java.lang.String USER_NAME_COMMAND;\n  public static final java.lang.Object WindowsProcessLaunchLock;\n  public static final org.apache.hadoop.util.Shell$OSType osType;\n  public static final boolean WINDOWS;\n  public static final boolean SOLARIS;\n  public static final boolean MAC;\n  public static final boolean FREEBSD;\n  public static final boolean LINUX;\n  public static final boolean OTHER;\n  public static final boolean PPC_64;\n  public static final java.lang.String SET_PERMISSION_COMMAND;\n  public static final java.lang.String SET_OWNER_COMMAND;\n  public static final java.lang.String SET_GROUP_COMMAND;\n  public static final java.lang.String LINK_COMMAND;\n  public static final java.lang.String READ_LINK_COMMAND;\n  protected long timeOutInterval;\n  public static final java.lang.String WINUTILS;\n  public static final boolean isSetsidAvailable;\n  public static final java.lang.String TOKEN_SEPARATOR_REGEX;\n  public static boolean isJava7OrAbove();\n  public static void checkWindowsCommandLineLength(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String[] getGroupsCommand();\n  public static java.lang.String[] getGroupsForUserCommand(java.lang.String);\n  public static java.lang.String[] getUsersForNetgroupCommand(java.lang.String);\n  public static java.lang.String[] getGetPermissionCommand();\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean);\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean, java.lang.String);\n  public static java.lang.String[] getSetOwnerCommand(java.lang.String);\n  public static java.lang.String[] getSymlinkCommand(java.lang.String, java.lang.String);\n  public static java.lang.String[] getReadlinkCommand(java.lang.String);\n  public static java.lang.String[] getCheckProcessIsAliveCommand(java.lang.String);\n  public static java.lang.String[] getSignalKillCommand(int, java.lang.String);\n  public static java.lang.String getEnvironmentVariableRegex();\n  public static java.io.File appendScriptExtension(java.io.File, java.lang.String);\n  public static java.lang.String appendScriptExtension(java.lang.String);\n  public static java.lang.String[] getRunScriptCommand(java.io.File);\n  public static final java.lang.String getHadoopHome() throws java.io.IOException;\n  public static final java.lang.String getQualifiedBinPath(java.lang.String) throws java.io.IOException;\n  public static final java.lang.String getWinUtilsPath();\n  public org.apache.hadoop.util.Shell();\n  public org.apache.hadoop.util.Shell(long);\n  public org.apache.hadoop.util.Shell(long, boolean);\n  protected void setEnvironment(java.util.Map<java.lang.String, java.lang.String>);\n  protected void setWorkingDirectory(java.io.File);\n  protected void run() throws java.io.IOException;\n  protected abstract java.lang.String[] getExecString();\n  protected abstract void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public java.lang.String getEnvironment(java.lang.String);\n  public java.lang.Process getProcess();\n  public int getExitCode();\n  public boolean isTimedOut();\n  public static java.lang.String execCommand(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String[], long) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String...) throws java.io.IOException;\n  static java.util.concurrent.atomic.AtomicBoolean access$000(org.apache.hadoop.util.Shell);\n  static void access$100(org.apache.hadoop.util.Shell);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Daemon.class": "Compiled from \"Daemon.java\"\npublic class org.apache.hadoop.util.Daemon extends java.lang.Thread {\n  java.lang.Runnable runnable;\n  public org.apache.hadoop.util.Daemon();\n  public org.apache.hadoop.util.Daemon(java.lang.Runnable);\n  public org.apache.hadoop.util.Daemon(java.lang.ThreadGroup, java.lang.Runnable);\n  public java.lang.Runnable getRunnable();\n}\n", 
  "org/apache/hadoop/io/MapFile$Reader$ComparatorOption.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/SecureIOUtils.class": "Compiled from \"SecureIOUtils.java\"\npublic class org.apache.hadoop.io.SecureIOUtils {\n  public org.apache.hadoop.io.SecureIOUtils();\n  public static java.io.RandomAccessFile openForRandomRead(java.io.File, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  protected static java.io.RandomAccessFile forceSecureOpenForRandomRead(java.io.File, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FSDataInputStream openFSDataInputStream(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  protected static org.apache.hadoop.fs.FSDataInputStream forceSecureOpenFSDataInputStream(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.io.FileInputStream openForRead(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  protected static java.io.FileInputStream forceSecureOpenForRead(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.io.FileOutputStream createForWrite(java.io.File, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JFloat$JavaFloat.class": "Compiled from \"JFloat.java\"\npublic class org.apache.hadoop.record.compiler.JFloat extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JFloat();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/viewfs/NotInMountpointException.class": "Compiled from \"NotInMountpointException.java\"\npublic class org.apache.hadoop.fs.viewfs.NotInMountpointException extends java.lang.UnsupportedOperationException {\n  final java.lang.String msg;\n  public org.apache.hadoop.fs.viewfs.NotInMountpointException(org.apache.hadoop.fs.Path, java.lang.String);\n  public org.apache.hadoop.fs.viewfs.NotInMountpointException(java.lang.String);\n  public java.lang.String getMessage();\n}\n", 
  "org/apache/hadoop/util/ZKUtil$BadAclFormatException.class": "Compiled from \"ZKUtil.java\"\npublic class org.apache.hadoop.util.ZKUtil {\n  public org.apache.hadoop.util.ZKUtil();\n  public static int removeSpecificPerms(int, int);\n  public static java.util.List<org.apache.zookeeper.data.ACL> parseACLs(java.lang.String) throws org.apache.hadoop.util.ZKUtil$BadAclFormatException;\n  public static java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo> parseAuth(java.lang.String) throws org.apache.hadoop.util.ZKUtil$BadAuthFormatException;\n  public static java.lang.String resolveConfIndirection(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$5.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$FileContextFinalizer.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/Serializer.class": "Compiled from \"Serializer.java\"\npublic interface org.apache.hadoop.io.serializer.Serializer<T> {\n  public abstract void open(java.io.OutputStream) throws java.io.IOException;\n  public abstract void serialize(T) throws java.io.IOException;\n  public abstract void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/LightWeightCache$Entry.class": "Compiled from \"LightWeightCache.java\"\npublic class org.apache.hadoop.util.LightWeightCache<K, E extends K> extends org.apache.hadoop.util.LightWeightGSet<K, E> {\n  public org.apache.hadoop.util.LightWeightCache(int, int, long, long);\n  org.apache.hadoop.util.LightWeightCache(int, int, long, long, org.apache.hadoop.util.LightWeightCache$Clock);\n  void setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  boolean isExpired(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  public E get(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/CachingKeyProvider$CacheExtension$3.class": "Compiled from \"CachingKeyProvider.java\"\npublic class org.apache.hadoop.crypto.key.CachingKeyProvider extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension> {\n  public org.apache.hadoop.crypto.key.CachingKeyProvider(org.apache.hadoop.crypto.key.KeyProvider, long, long);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$AddSpanReceiverRequestProto$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/StorageType.class": "Compiled from \"StorageType.java\"\npublic final class org.apache.hadoop.fs.StorageType extends java.lang.Enum<org.apache.hadoop.fs.StorageType> {\n  public static final org.apache.hadoop.fs.StorageType RAM_DISK;\n  public static final org.apache.hadoop.fs.StorageType SSD;\n  public static final org.apache.hadoop.fs.StorageType DISK;\n  public static final org.apache.hadoop.fs.StorageType ARCHIVE;\n  public static final org.apache.hadoop.fs.StorageType DEFAULT;\n  public static final org.apache.hadoop.fs.StorageType[] EMPTY_ARRAY;\n  public static org.apache.hadoop.fs.StorageType[] values();\n  public static org.apache.hadoop.fs.StorageType valueOf(java.lang.String);\n  public boolean isTransient();\n  public boolean supportTypeQuota();\n  public boolean isMovable();\n  public static java.util.List<org.apache.hadoop.fs.StorageType> asList();\n  public static java.util.List<org.apache.hadoop.fs.StorageType> getMovableTypes();\n  public static java.util.List<org.apache.hadoop.fs.StorageType> getTypesSupportingQuota();\n  public static org.apache.hadoop.fs.StorageType parseStorageType(int);\n  public static org.apache.hadoop.fs.StorageType parseStorageType(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/PathIOException.class": "Compiled from \"PathIOException.java\"\npublic class org.apache.hadoop.fs.PathIOException extends java.io.IOException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.PathIOException(java.lang.String);\n  public org.apache.hadoop.fs.PathIOException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.fs.PathIOException(java.lang.String, java.lang.String);\n  protected org.apache.hadoop.fs.PathIOException(java.lang.String, java.lang.String, java.lang.Throwable);\n  public java.lang.String getMessage();\n  public org.apache.hadoop.fs.Path getPath();\n  public org.apache.hadoop.fs.Path getTargetPath();\n  public void setOperation(java.lang.String);\n  public void setTargetPath(java.lang.String);\n}\n", 
  "org/apache/hadoop/record/Buffer.class": "Compiled from \"Buffer.java\"\npublic class org.apache.hadoop.record.Buffer implements java.lang.Comparable,java.lang.Cloneable {\n  public org.apache.hadoop.record.Buffer();\n  public org.apache.hadoop.record.Buffer(byte[]);\n  public org.apache.hadoop.record.Buffer(byte[], int, int);\n  public void set(byte[]);\n  public final void copy(byte[], int, int);\n  public byte[] get();\n  public int getCount();\n  public int getCapacity();\n  public void setCapacity(int);\n  public void reset();\n  public void truncate();\n  public void append(byte[], int, int);\n  public void append(byte[]);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n  public java.lang.String toString(java.lang.String) throws java.io.UnsupportedEncodingException;\n  public java.lang.Object clone() throws java.lang.CloneNotSupportedException;\n}\n", 
  "org/apache/hadoop/tools/GetGroupsBase.class": "Compiled from \"GetGroupsBase.java\"\npublic abstract class org.apache.hadoop.tools.GetGroupsBase extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  protected org.apache.hadoop.tools.GetGroupsBase(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.tools.GetGroupsBase(org.apache.hadoop.conf.Configuration, java.io.PrintStream);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected abstract java.net.InetSocketAddress getProtocolAddress(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.tools.GetUserMappingsProtocol getUgmProtocol() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/ReadaheadPool$1.class": "Compiled from \"ReadaheadPool.java\"\npublic class org.apache.hadoop.io.ReadaheadPool {\n  static final org.apache.commons.logging.Log LOG;\n  public static org.apache.hadoop.io.ReadaheadPool getInstance();\n  public org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest readaheadStream(java.lang.String, java.io.FileDescriptor, long, long, long, org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest);\n  public org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest submitReadahead(java.lang.String, java.io.FileDescriptor, long, long);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RpcException.class": "Compiled from \"RpcException.java\"\npublic class org.apache.hadoop.ipc.RpcException extends java.io.IOException {\n  org.apache.hadoop.ipc.RpcException(java.lang.String);\n  org.apache.hadoop.ipc.RpcException(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/security/alias/CredentialShell.class": "Compiled from \"CredentialShell.java\"\npublic class org.apache.hadoop.security.alias.CredentialShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.security.alias.CredentialShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int init(java.lang.String[]) throws java.io.IOException;\n  protected char[] promptForCredential() throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialShell$PasswordReader getPasswordReader();\n  public void setPasswordReader(org.apache.hadoop.security.alias.CredentialShell$PasswordReader);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.security.alias.CredentialShell);\n  static boolean access$300(org.apache.hadoop.security.alias.CredentialShell);\n  static java.lang.String access$400(org.apache.hadoop.security.alias.CredentialShell);\n}\n", 
  "org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.class": "Compiled from \"GenericRefreshProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB implements org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB {\n  public org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB(org.apache.hadoop.ipc.GenericRefreshProtocol);\n  public org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto refresh(com.google.protobuf.RpcController, org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto) throws com.google.protobuf.ServiceException;\n}\n", 
  "org/apache/hadoop/ipc/Server.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/io/InputBuffer$Buffer.class": "Compiled from \"InputBuffer.java\"\npublic class org.apache.hadoop.io.InputBuffer extends java.io.FilterInputStream {\n  public org.apache.hadoop.io.InputBuffer();\n  public void reset(byte[], int);\n  public void reset(byte[], int, int);\n  public int getPosition();\n  public int getLength();\n}\n", 
  "org/apache/hadoop/crypto/CryptoCodec.class": "Compiled from \"CryptoCodec.java\"\npublic abstract class org.apache.hadoop.crypto.CryptoCodec implements org.apache.hadoop.conf.Configurable {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.CryptoCodec();\n  public static org.apache.hadoop.crypto.CryptoCodec getInstance(org.apache.hadoop.conf.Configuration, org.apache.hadoop.crypto.CipherSuite);\n  public static org.apache.hadoop.crypto.CryptoCodec getInstance(org.apache.hadoop.conf.Configuration);\n  public abstract org.apache.hadoop.crypto.CipherSuite getCipherSuite();\n  public abstract org.apache.hadoop.crypto.Encryptor createEncryptor() throws java.security.GeneralSecurityException;\n  public abstract org.apache.hadoop.crypto.Decryptor createDecryptor() throws java.security.GeneralSecurityException;\n  public abstract void calculateIV(byte[], long, byte[]);\n  public abstract void generateSecureRandom(byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.class": "Compiled from \"ShellBasedUnixGroupsNetgroupMapping.java\"\npublic class org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping extends org.apache.hadoop.security.ShellBasedUnixGroupsMapping {\n  public org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping();\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void cacheGroupsRefresh() throws java.io.IOException;\n  public void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n  protected java.util.List<java.lang.String> getUsersForNetgroup(java.lang.String) throws java.io.IOException;\n  protected java.lang.String execShellGetUserForNetgroup(java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$KeyClassOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer2$1.class": "Compiled from \"HttpServer2.java\"\npublic final class org.apache.hadoop.http.HttpServer2 implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  public static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean);\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public java.net.InetSocketAddress getConnectorAddress(int);\n  public void setThreads(int, int);\n  public void start() throws java.io.IOException;\n  void openListeners() throws java.lang.Exception;\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  org.apache.hadoop.http.HttpServer2(org.apache.hadoop.http.HttpServer2$Builder, org.apache.hadoop.http.HttpServer2$1) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.http.HttpServer2, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.http.HttpServer2, org.mortbay.jetty.Connector);\n  static void access$300(org.apache.hadoop.http.HttpServer2);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/WritableRpcEngine$Server.class": "Compiled from \"WritableRpcEngine.java\"\npublic class org.apache.hadoop.ipc.WritableRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final long writableRpcVersion;\n  public org.apache.hadoop.ipc.WritableRpcEngine();\n  public static synchronized void ensureInitialized();\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$000();\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.class": "Compiled from \"SharedFileDescriptorFactory.java\"\npublic class org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory {\n  public static final org.apache.commons.logging.Log LOG;\n  public static java.lang.String getLoadingFailureReason();\n  public static org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory create(java.lang.String, java.lang.String[]) throws java.io.IOException;\n  public java.lang.String getPath();\n  public java.io.FileInputStream createDescriptor(java.lang.String, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/Options$LongOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/util/ToolRunner.class": "Compiled from \"ToolRunner.java\"\npublic class org.apache.hadoop.util.ToolRunner {\n  public org.apache.hadoop.util.ToolRunner();\n  public static int run(org.apache.hadoop.conf.Configuration, org.apache.hadoop.util.Tool, java.lang.String[]) throws java.lang.Exception;\n  public static int run(org.apache.hadoop.util.Tool, java.lang.String[]) throws java.lang.Exception;\n  public static void printGenericCommandUsage(java.io.PrintStream);\n  public static boolean confirmPrompt(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolSignatureRequestProtoOrBuilder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueProtocolService$1.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto$1.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JField.class": "Compiled from \"JField.java\"\npublic class org.apache.hadoop.record.compiler.JField<T> {\n  public org.apache.hadoop.record.compiler.JField(java.lang.String, T);\n  java.lang.String getName();\n  T getType();\n}\n", 
  "org/apache/hadoop/io/compress/DefaultCodec.class": "Compiled from \"DefaultCodec.java\"\npublic class org.apache.hadoop.io.compress.DefaultCodec implements org.apache.hadoop.conf.Configurable,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.compress.DirectDecompressionCodec {\n  org.apache.hadoop.conf.Configuration conf;\n  public org.apache.hadoop.io.compress.DefaultCodec();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public org.apache.hadoop.io.compress.DirectDecompressor createDirectDecompressor();\n  public java.lang.String getDefaultExtension();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/CommonConfigurationKeysPublic.class": "Compiled from \"CommonConfigurationKeysPublic.java\"\npublic class org.apache.hadoop.fs.CommonConfigurationKeysPublic {\n  public static final java.lang.String IO_NATIVE_LIB_AVAILABLE_KEY;\n  public static final boolean IO_NATIVE_LIB_AVAILABLE_DEFAULT;\n  public static final java.lang.String NET_TOPOLOGY_SCRIPT_NUMBER_ARGS_KEY;\n  public static final int NET_TOPOLOGY_SCRIPT_NUMBER_ARGS_DEFAULT;\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String FS_DEFAULT_NAME_DEFAULT;\n  public static final java.lang.String FS_DF_INTERVAL_KEY;\n  public static final long FS_DF_INTERVAL_DEFAULT;\n  public static final java.lang.String FS_DU_INTERVAL_KEY;\n  public static final long FS_DU_INTERVAL_DEFAULT;\n  public static final java.lang.String FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_KEY;\n  public static final boolean FS_CLIENT_RESOLVE_REMOTE_SYMLINKS_DEFAULT;\n  public static final java.lang.String NET_TOPOLOGY_SCRIPT_FILE_NAME_KEY;\n  public static final java.lang.String NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY;\n  public static final java.lang.String NET_TOPOLOGY_IMPL_KEY;\n  public static final java.lang.String NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY;\n  public static final java.lang.String NET_DEPENDENCY_SCRIPT_FILE_NAME_KEY;\n  public static final java.lang.String FS_TRASH_CHECKPOINT_INTERVAL_KEY;\n  public static final long FS_TRASH_CHECKPOINT_INTERVAL_DEFAULT;\n  public static final long FS_LOCAL_BLOCK_SIZE_DEFAULT;\n  public static final java.lang.String FS_AUTOMATIC_CLOSE_KEY;\n  public static final boolean FS_AUTOMATIC_CLOSE_DEFAULT;\n  public static final java.lang.String FS_FILE_IMPL_KEY;\n  public static final java.lang.String FS_FTP_HOST_KEY;\n  public static final java.lang.String FS_FTP_HOST_PORT_KEY;\n  public static final java.lang.String FS_TRASH_INTERVAL_KEY;\n  public static final long FS_TRASH_INTERVAL_DEFAULT;\n  public static final java.lang.String IO_MAPFILE_BLOOM_SIZE_KEY;\n  public static final int IO_MAPFILE_BLOOM_SIZE_DEFAULT;\n  public static final java.lang.String IO_MAPFILE_BLOOM_ERROR_RATE_KEY;\n  public static final float IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT;\n  public static final java.lang.String IO_COMPRESSION_CODEC_LZO_CLASS_KEY;\n  public static final java.lang.String IO_MAP_INDEX_INTERVAL_KEY;\n  public static final int IO_MAP_INDEX_INTERVAL_DEFAULT;\n  public static final java.lang.String IO_MAP_INDEX_SKIP_KEY;\n  public static final int IO_MAP_INDEX_SKIP_DEFAULT;\n  public static final java.lang.String IO_SEQFILE_COMPRESS_BLOCKSIZE_KEY;\n  public static final int IO_SEQFILE_COMPRESS_BLOCKSIZE_DEFAULT;\n  public static final java.lang.String IO_FILE_BUFFER_SIZE_KEY;\n  public static final int IO_FILE_BUFFER_SIZE_DEFAULT;\n  public static final java.lang.String IO_SKIP_CHECKSUM_ERRORS_KEY;\n  public static final boolean IO_SKIP_CHECKSUM_ERRORS_DEFAULT;\n  public static final java.lang.String IO_SORT_MB_KEY;\n  public static final int IO_SORT_MB_DEFAULT;\n  public static final java.lang.String IO_SORT_FACTOR_KEY;\n  public static final int IO_SORT_FACTOR_DEFAULT;\n  public static final java.lang.String IO_SERIALIZATIONS_KEY;\n  public static final java.lang.String TFILE_IO_CHUNK_SIZE_KEY;\n  public static final int TFILE_IO_CHUNK_SIZE_DEFAULT;\n  public static final java.lang.String TFILE_FS_INPUT_BUFFER_SIZE_KEY;\n  public static final int TFILE_FS_INPUT_BUFFER_SIZE_DEFAULT;\n  public static final java.lang.String TFILE_FS_OUTPUT_BUFFER_SIZE_KEY;\n  public static final int TFILE_FS_OUTPUT_BUFFER_SIZE_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY;\n  public static final int IPC_CLIENT_CONNECTION_MAXIDLETIME_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_CONNECT_TIMEOUT_KEY;\n  public static final int IPC_CLIENT_CONNECT_TIMEOUT_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_CONNECT_MAX_RETRIES_KEY;\n  public static final int IPC_CLIENT_CONNECT_MAX_RETRIES_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_CONNECT_RETRY_INTERVAL_KEY;\n  public static final int IPC_CLIENT_CONNECT_RETRY_INTERVAL_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY;\n  public static final int IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_TCPNODELAY_KEY;\n  public static final boolean IPC_CLIENT_TCPNODELAY_DEFAULT;\n  public static final java.lang.String IPC_SERVER_LISTEN_QUEUE_SIZE_KEY;\n  public static final int IPC_SERVER_LISTEN_QUEUE_SIZE_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_KILL_MAX_KEY;\n  public static final int IPC_CLIENT_KILL_MAX_DEFAULT;\n  public static final java.lang.String IPC_CLIENT_IDLETHRESHOLD_KEY;\n  public static final int IPC_CLIENT_IDLETHRESHOLD_DEFAULT;\n  public static final java.lang.String IPC_SERVER_TCPNODELAY_KEY;\n  public static final boolean IPC_SERVER_TCPNODELAY_DEFAULT;\n  public static final java.lang.String IPC_SERVER_MAX_CONNECTIONS_KEY;\n  public static final int IPC_SERVER_MAX_CONNECTIONS_DEFAULT;\n  public static final java.lang.String HADOOP_RPC_SOCKET_FACTORY_CLASS_DEFAULT_KEY;\n  public static final java.lang.String HADOOP_RPC_SOCKET_FACTORY_CLASS_DEFAULT_DEFAULT;\n  public static final java.lang.String HADOOP_SOCKS_SERVER_KEY;\n  public static final java.lang.String HADOOP_UTIL_HASH_TYPE_KEY;\n  public static final java.lang.String HADOOP_UTIL_HASH_TYPE_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_GROUP_MAPPING;\n  public static final java.lang.String HADOOP_SECURITY_GROUPS_CACHE_SECS;\n  public static final long HADOOP_SECURITY_GROUPS_CACHE_SECS_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_GROUPS_NEGATIVE_CACHE_SECS;\n  public static final long HADOOP_SECURITY_GROUPS_NEGATIVE_CACHE_SECS_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_GROUPS_CACHE_WARN_AFTER_MS;\n  public static final long HADOOP_SECURITY_GROUPS_CACHE_WARN_AFTER_MS_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_AUTHENTICATION;\n  public static final java.lang.String HADOOP_SECURITY_AUTHORIZATION;\n  public static final java.lang.String HADOOP_SECURITY_INSTRUMENTATION_REQUIRES_ADMIN;\n  public static final java.lang.String HADOOP_SECURITY_SERVICE_USER_NAME_KEY;\n  public static final java.lang.String HADOOP_SECURITY_AUTH_TO_LOCAL;\n  public static final java.lang.String HADOOP_SSL_ENABLED_KEY;\n  public static final boolean HADOOP_SSL_ENABLED_DEFAULT;\n  public static final java.lang.String HTTP_POLICY_HTTP_ONLY;\n  public static final java.lang.String HTTP_POLICY_HTTPS_ONLY;\n  public static final java.lang.String HADOOP_RPC_PROTECTION;\n  public static final java.lang.String HADOOP_SECURITY_SASL_PROPS_RESOLVER_CLASS;\n  public static final java.lang.String HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX;\n  public static final java.lang.String HADOOP_SECURITY_CRYPTO_CIPHER_SUITE_KEY;\n  public static final java.lang.String HADOOP_SECURITY_CRYPTO_CIPHER_SUITE_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_CRYPTO_JCE_PROVIDER_KEY;\n  public static final java.lang.String HADOOP_SECURITY_CRYPTO_BUFFER_SIZE_KEY;\n  public static final int HADOOP_SECURITY_CRYPTO_BUFFER_SIZE_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_IMPERSONATION_PROVIDER_CLASS;\n  public static final java.lang.String KMS_CLIENT_ENC_KEY_CACHE_SIZE;\n  public static final int KMS_CLIENT_ENC_KEY_CACHE_SIZE_DEFAULT;\n  public static final java.lang.String KMS_CLIENT_ENC_KEY_CACHE_LOW_WATERMARK;\n  public static final float KMS_CLIENT_ENC_KEY_CACHE_LOW_WATERMARK_DEFAULT;\n  public static final java.lang.String KMS_CLIENT_ENC_KEY_CACHE_NUM_REFILL_THREADS;\n  public static final int KMS_CLIENT_ENC_KEY_CACHE_NUM_REFILL_THREADS_DEFAULT;\n  public static final java.lang.String KMS_CLIENT_ENC_KEY_CACHE_EXPIRY_MS;\n  public static final int KMS_CLIENT_ENC_KEY_CACHE_EXPIRY_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_JAVA_SECURE_RANDOM_ALGORITHM_KEY;\n  public static final java.lang.String HADOOP_SECURITY_JAVA_SECURE_RANDOM_ALGORITHM_DEFAULT;\n  public static final java.lang.String HADOOP_SECURITY_SECURE_RANDOM_IMPL_KEY;\n  public static final java.lang.String HADOOP_SECURITY_SECURE_RANDOM_DEVICE_FILE_PATH_KEY;\n  public static final java.lang.String HADOOP_SECURITY_SECURE_RANDOM_DEVICE_FILE_PATH_DEFAULT;\n  public org.apache.hadoop.fs.CommonConfigurationKeysPublic();\n}\n", 
  "org/apache/hadoop/fs/FSInputChecker.class": "Compiled from \"FSInputChecker.java\"\npublic abstract class org.apache.hadoop.fs.FSInputChecker extends org.apache.hadoop.fs.FSInputStream {\n  public static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.fs.Path file;\n  protected static final int CHECKSUM_SIZE;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.fs.FSInputChecker(org.apache.hadoop.fs.Path, int);\n  protected org.apache.hadoop.fs.FSInputChecker(org.apache.hadoop.fs.Path, int, boolean, java.util.zip.Checksum, int, int);\n  protected abstract int readChunk(long, byte[], int, int, byte[]) throws java.io.IOException;\n  protected abstract long getChunkPosition(long);\n  protected synchronized boolean needChecksum();\n  public synchronized int read() throws java.io.IOException;\n  public synchronized int read(byte[], int, int) throws java.io.IOException;\n  public static long checksum2long(byte[]);\n  public synchronized long getPos() throws java.io.IOException;\n  public synchronized int available() throws java.io.IOException;\n  public synchronized long skip(long) throws java.io.IOException;\n  public synchronized void seek(long) throws java.io.IOException;\n  protected static int readFully(java.io.InputStream, byte[], int, int) throws java.io.IOException;\n  protected final synchronized void set(boolean, java.util.zip.Checksum, int, int);\n  public final boolean markSupported();\n  public final void mark(int);\n  public final void reset() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolPB.class": "Compiled from \"RefreshAuthorizationPolicyProtocolPB.java\"\npublic interface org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB extends org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshAuthorizationPolicyProtocolService$BlockingInterface {\n}\n", 
  "org/apache/hadoop/security/ShellBasedIdMapping$StaticMapping.class": "Compiled from \"ShellBasedIdMapping.java\"\npublic class org.apache.hadoop.security.ShellBasedIdMapping implements org.apache.hadoop.security.IdMappingServiceProvider {\n  static final java.lang.String GET_ALL_USERS_CMD;\n  static final java.lang.String GET_ALL_GROUPS_CMD;\n  static final java.lang.String MAC_GET_ALL_USERS_CMD;\n  static final java.lang.String MAC_GET_ALL_GROUPS_CMD;\n  public org.apache.hadoop.security.ShellBasedIdMapping(org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.security.ShellBasedIdMapping(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public long getTimeout();\n  public com.google.common.collect.BiMap<java.lang.Integer, java.lang.String> getUidNameMap();\n  public com.google.common.collect.BiMap<java.lang.Integer, java.lang.String> getGidNameMap();\n  public synchronized void clearNameMaps();\n  public static boolean updateMapInternal(com.google.common.collect.BiMap<java.lang.Integer, java.lang.String>, java.lang.String, java.lang.String, java.lang.String, java.util.Map<java.lang.Integer, java.lang.Integer>) throws java.io.IOException;\n  public synchronized void updateMaps() throws java.io.IOException;\n  static org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping parseStaticMap(java.io.File) throws java.io.IOException;\n  public synchronized int getUid(java.lang.String) throws java.io.IOException;\n  public synchronized int getGid(java.lang.String) throws java.io.IOException;\n  public synchronized java.lang.String getUserName(int, java.lang.String);\n  public synchronized java.lang.String getGroupName(int, java.lang.String);\n  public int getUidAllowingUnknown(java.lang.String);\n  public int getGidAllowingUnknown(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsRecordBuilder.class": "Compiled from \"MetricsRecordBuilder.java\"\npublic abstract class org.apache.hadoop.metrics2.MetricsRecordBuilder {\n  public org.apache.hadoop.metrics2.MetricsRecordBuilder();\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder add(org.apache.hadoop.metrics2.MetricsTag);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder add(org.apache.hadoop.metrics2.AbstractMetric);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder setContext(java.lang.String);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder addCounter(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder addCounter(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder addGauge(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder addGauge(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder addGauge(org.apache.hadoop.metrics2.MetricsInfo, float);\n  public abstract org.apache.hadoop.metrics2.MetricsRecordBuilder addGauge(org.apache.hadoop.metrics2.MetricsInfo, double);\n  public abstract org.apache.hadoop.metrics2.MetricsCollector parent();\n  public org.apache.hadoop.metrics2.MetricsCollector endRecord();\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$ProviderCallable.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/avro/AvroSpecificSerialization.class": "Compiled from \"AvroSpecificSerialization.java\"\npublic class org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization extends org.apache.hadoop.io.serializer.avro.AvroSerialization<org.apache.avro.specific.SpecificRecord> {\n  public org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.avro.io.DatumReader getReader(java.lang.Class<org.apache.avro.specific.SpecificRecord>);\n  public org.apache.avro.Schema getSchema(org.apache.avro.specific.SpecificRecord);\n  public org.apache.avro.io.DatumWriter getWriter(java.lang.Class<org.apache.avro.specific.SpecificRecord>);\n  public org.apache.avro.Schema getSchema(java.lang.Object);\n}\n", 
  "org/apache/hadoop/fs/FilterFileSystem.class": "Compiled from \"FilterFileSystem.java\"\npublic class org.apache.hadoop.fs.FilterFileSystem extends org.apache.hadoop.fs.FileSystem {\n  protected org.apache.hadoop.fs.FileSystem fs;\n  protected java.lang.String swapScheme;\n  public org.apache.hadoop.fs.FilterFileSystem();\n  public org.apache.hadoop.fs.FilterFileSystem(org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.fs.FileSystem getRawFileSystem();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public short getDefaultReplication();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void close() throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolVersionsResponseProtoOrBuilder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$11.class": "", 
  "org/apache/hadoop/record/compiler/JMap$CppMap.class": "Compiled from \"JMap.java\"\npublic class org.apache.hadoop.record.compiler.JMap extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JMap(org.apache.hadoop.record.compiler.JType, org.apache.hadoop.record.compiler.JType);\n  java.lang.String getSignature();\n  static java.lang.String access$000(java.lang.String);\n  static void access$100();\n  static void access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/AclEntryType.class": "Compiled from \"AclEntryType.java\"\npublic final class org.apache.hadoop.fs.permission.AclEntryType extends java.lang.Enum<org.apache.hadoop.fs.permission.AclEntryType> {\n  public static final org.apache.hadoop.fs.permission.AclEntryType USER;\n  public static final org.apache.hadoop.fs.permission.AclEntryType GROUP;\n  public static final org.apache.hadoop.fs.permission.AclEntryType MASK;\n  public static final org.apache.hadoop.fs.permission.AclEntryType OTHER;\n  public static org.apache.hadoop.fs.permission.AclEntryType[] values();\n  public static org.apache.hadoop.fs.permission.AclEntryType valueOf(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsUrlConnection.class": "Compiled from \"FsUrlConnection.java\"\nclass org.apache.hadoop.fs.FsUrlConnection extends java.net.URLConnection {\n  org.apache.hadoop.fs.FsUrlConnection(org.apache.hadoop.conf.Configuration, java.net.URL);\n  public void connect() throws java.io.IOException;\n  public java.io.InputStream getInputStream() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshRequestProtoOrBuilder.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.class": "Compiled from \"CBZip2InputStream.java\"\npublic class org.apache.hadoop.io.compress.bzip2.CBZip2InputStream extends java.io.InputStream implements org.apache.hadoop.io.compress.bzip2.BZip2Constants {\n  public static final long BLOCK_DELIMITER;\n  public static final long EOS_DELIMITER;\n  org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE readMode;\n  public long getProcessedByteCount();\n  protected void updateProcessedByteCount(int);\n  public void updateReportedByteCount(int);\n  public boolean skipToNextMarker(long, int) throws java.io.IOException, java.lang.IllegalArgumentException;\n  protected void reportCRCError() throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2InputStream(java.io.InputStream, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n  public static long numberOfBytesTillNextMarker(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2InputStream(java.io.InputStream) throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$12.class": "", 
  "org/apache/hadoop/fs/LocalFileSystem.class": "Compiled from \"LocalFileSystem.java\"\npublic class org.apache.hadoop.fs.LocalFileSystem extends org.apache.hadoop.fs.ChecksumFileSystem {\n  static final java.net.URI NAME;\n  public org.apache.hadoop.fs.LocalFileSystem();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public org.apache.hadoop.fs.FileSystem getRaw();\n  public org.apache.hadoop.fs.LocalFileSystem(org.apache.hadoop.fs.FileSystem);\n  public java.io.File pathToFile(org.apache.hadoop.fs.Path);\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpConfig$Policy.class": "Compiled from \"HttpConfig.java\"\npublic class org.apache.hadoop.http.HttpConfig {\n  public org.apache.hadoop.http.HttpConfig();\n}\n", 
  "org/apache/hadoop/fs/FileContext$35.class": "", 
  "org/apache/hadoop/net/DNSToSwitchMapping.class": "Compiled from \"DNSToSwitchMapping.java\"\npublic interface org.apache.hadoop.net.DNSToSwitchMapping {\n  public abstract java.util.List<java.lang.String> resolve(java.util.List<java.lang.String>);\n  public abstract void reloadCachedMappings();\n  public abstract void reloadCachedMappings(java.util.List<java.lang.String>);\n}\n", 
  "org/apache/hadoop/log/metrics/EventCounter$EventCounts.class": "Compiled from \"EventCounter.java\"\npublic class org.apache.hadoop.log.metrics.EventCounter extends org.apache.log4j.AppenderSkeleton {\n  public org.apache.hadoop.log.metrics.EventCounter();\n  public static long getFatal();\n  public static long getError();\n  public static long getWarn();\n  public static long getInfo();\n  public void append(org.apache.log4j.spi.LoggingEvent);\n  public void close();\n  public boolean requiresLayout();\n  static {};\n}\n", 
  "org/apache/hadoop/util/StringUtils$1.class": "Compiled from \"StringUtils.java\"\npublic class org.apache.hadoop.util.StringUtils {\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  public static final java.util.regex.Pattern SHELL_ENV_VAR_PATTERN;\n  public static final java.util.regex.Pattern WIN_ENV_VAR_PATTERN;\n  public static final java.util.regex.Pattern ENV_VAR_PATTERN;\n  public static final java.lang.String[] emptyStringArray;\n  public static final char COMMA;\n  public static final java.lang.String COMMA_STR;\n  public static final char ESCAPE_CHAR;\n  public org.apache.hadoop.util.StringUtils();\n  public static java.lang.String stringifyException(java.lang.Throwable);\n  public static java.lang.String simpleHostname(java.lang.String);\n  public static java.lang.String humanReadableInt(long);\n  public static java.lang.String format(java.lang.String, java.lang.Object...);\n  public static java.lang.String formatPercent(double, int);\n  public static java.lang.String arrayToString(java.lang.String[]);\n  public static java.lang.String byteToHexString(byte[], int, int);\n  public static java.lang.String byteToHexString(byte[]);\n  public static byte[] hexStringToByte(java.lang.String);\n  public static java.lang.String uriToString(java.net.URI[]);\n  public static java.net.URI[] stringToURI(java.lang.String[]);\n  public static org.apache.hadoop.fs.Path[] stringToPath(java.lang.String[]);\n  public static java.lang.String formatTimeDiff(long, long);\n  public static java.lang.String formatTime(long);\n  public static java.lang.String getFormattedTimeWithDiff(java.text.DateFormat, long, long);\n  public static java.lang.String[] getStrings(java.lang.String);\n  public static java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public static java.util.Collection<java.lang.String> getStringCollection(java.lang.String, java.lang.String);\n  public static java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public static java.lang.String[] getTrimmedStrings(java.lang.String);\n  public static java.util.Set<java.lang.String> getTrimmedStrings(java.util.Collection<java.lang.String>);\n  public static java.lang.String[] split(java.lang.String);\n  public static java.lang.String[] split(java.lang.String, char, char);\n  public static java.lang.String[] split(java.lang.String, char);\n  public static int findNext(java.lang.String, char, char, int, java.lang.StringBuilder);\n  public static java.lang.String escapeString(java.lang.String);\n  public static java.lang.String escapeString(java.lang.String, char, char);\n  public static java.lang.String escapeString(java.lang.String, char, char[]);\n  public static java.lang.String unEscapeString(java.lang.String);\n  public static java.lang.String unEscapeString(java.lang.String, char, char);\n  public static java.lang.String unEscapeString(java.lang.String, char, char[]);\n  public static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.apache.commons.logging.Log);\n  public static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.slf4j.Logger);\n  static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.apache.hadoop.util.LogAdapter);\n  public static java.lang.String escapeHTML(java.lang.String);\n  public static java.lang.String byteDesc(long);\n  public static java.lang.String limitDecimalTo2(double);\n  public static java.lang.String join(java.lang.CharSequence, java.lang.Iterable<?>);\n  public static java.lang.String join(java.lang.CharSequence, java.lang.String[]);\n  public static java.lang.String camelize(java.lang.String);\n  public static java.lang.String replaceTokens(java.lang.String, java.util.regex.Pattern, java.util.Map<java.lang.String, java.lang.String>);\n  public static java.lang.String getStackTrace(java.lang.Thread);\n  public static java.lang.String popOptionWithArgument(java.lang.String, java.util.List<java.lang.String>) throws java.lang.IllegalArgumentException;\n  public static boolean popOption(java.lang.String, java.util.List<java.lang.String>);\n  public static java.lang.String popFirstNonOption(java.util.List<java.lang.String>);\n  public static java.lang.String toLowerCase(java.lang.String);\n  public static java.lang.String toUpperCase(java.lang.String);\n  public static boolean equalsIgnoreCase(java.lang.String, java.lang.String);\n  static java.lang.String access$000(java.lang.String, java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticationHandler.class": "Compiled from \"KerberosDelegationTokenAuthenticationHandler.java\"\npublic class org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler {\n  public org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler();\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$MonitorHealthRequestProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$UserInformationProto$Builder.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/ganglia/GangliaContext31.class": "Compiled from \"GangliaContext31.java\"\npublic class org.apache.hadoop.metrics.ganglia.GangliaContext31 extends org.apache.hadoop.metrics.ganglia.GangliaContext {\n  java.lang.String hostName;\n  public org.apache.hadoop.metrics.ganglia.GangliaContext31();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  protected void emitMetric(java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/XmlRecordInput$Value.class": "Compiled from \"XmlRecordInput.java\"\npublic class org.apache.hadoop.record.XmlRecordInput implements org.apache.hadoop.record.RecordInput {\n  public org.apache.hadoop.record.XmlRecordInput(java.io.InputStream);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  static int access$000(org.apache.hadoop.record.XmlRecordInput);\n  static java.util.ArrayList access$100(org.apache.hadoop.record.XmlRecordInput);\n  static int access$008(org.apache.hadoop.record.XmlRecordInput);\n}\n", 
  "org/apache/hadoop/io/compress/GzipCodec$GzipZlibCompressor.class": "Compiled from \"GzipCodec.java\"\npublic class org.apache.hadoop.io.compress.GzipCodec extends org.apache.hadoop.io.compress.DefaultCodec {\n  public org.apache.hadoop.io.compress.GzipCodec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.DirectDecompressor createDirectDecompressor();\n  public java.lang.String getDefaultExtension();\n}\n", 
  "org/apache/hadoop/ha/HAServiceStatus.class": "Compiled from \"HAServiceStatus.java\"\npublic class org.apache.hadoop.ha.HAServiceStatus {\n  public org.apache.hadoop.ha.HAServiceStatus(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  public org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getState();\n  public org.apache.hadoop.ha.HAServiceStatus setReadyToBecomeActive();\n  public org.apache.hadoop.ha.HAServiceStatus setNotReadyToBecomeActive(java.lang.String);\n  public boolean isReadyToBecomeActive();\n  public java.lang.String getNotReadyReason();\n}\n", 
  "org/apache/hadoop/fs/shell/CommandFormat.class": "Compiled from \"CommandFormat.java\"\npublic class org.apache.hadoop.fs.shell.CommandFormat {\n  final int minPar;\n  final int maxPar;\n  final java.util.Map<java.lang.String, java.lang.Boolean> options;\n  boolean ignoreUnknownOpts;\n  public org.apache.hadoop.fs.shell.CommandFormat(java.lang.String, int, int, java.lang.String...);\n  public org.apache.hadoop.fs.shell.CommandFormat(int, int, java.lang.String...);\n  public java.util.List<java.lang.String> parse(java.lang.String[], int);\n  public void parse(java.util.List<java.lang.String>);\n  public boolean getOpt(java.lang.String);\n  public java.util.Set<java.lang.String> getOpts();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsBuffer.class": "Compiled from \"MetricsBuffer.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsBuffer implements java.lang.Iterable<org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry> {\n  org.apache.hadoop.metrics2.impl.MetricsBuffer(java.lang.Iterable<org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry>);\n  public java.util.Iterator<org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry> iterator();\n}\n", 
  "org/apache/hadoop/fs/shell/Ls$Lsr.class": "Compiled from \"Ls.java\"\nclass org.apache.hadoop.fs.shell.Ls extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  protected final java.text.SimpleDateFormat dateFormat;\n  protected int maxRepl;\n  protected int maxLen;\n  protected int maxOwner;\n  protected int maxGroup;\n  protected java.lang.String lineFormat;\n  protected boolean dirRecurse;\n  protected boolean humanReadable;\n  org.apache.hadoop.fs.shell.Ls();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected java.lang.String formatSize(long);\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processPathArgument(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPaths(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData...) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshProtocolService$2.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.class": "Compiled from \"AbstractDelegationTokenIdentifier.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier extends org.apache.hadoop.security.token.TokenIdentifier {\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier();\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier(org.apache.hadoop.io.Text, org.apache.hadoop.io.Text, org.apache.hadoop.io.Text);\n  public abstract org.apache.hadoop.io.Text getKind();\n  public org.apache.hadoop.security.UserGroupInformation getUser();\n  public org.apache.hadoop.io.Text getOwner();\n  public void setOwner(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text getRenewer();\n  public void setRenewer(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text getRealUser();\n  public void setRealUser(org.apache.hadoop.io.Text);\n  public void setIssueDate(long);\n  public long getIssueDate();\n  public void setMaxDate(long);\n  public long getMaxDate();\n  public void setSequenceNumber(int);\n  public int getSequenceNumber();\n  public void setMasterKeyId(int);\n  public int getMasterKeyId();\n  protected static boolean isEqual(java.lang.Object, java.lang.Object);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  void writeImpl(java.io.DataOutput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/fs/shell/CommandFormat$NotEnoughArgumentsException.class": "Compiled from \"CommandFormat.java\"\npublic class org.apache.hadoop.fs.shell.CommandFormat {\n  final int minPar;\n  final int maxPar;\n  final java.util.Map<java.lang.String, java.lang.Boolean> options;\n  boolean ignoreUnknownOpts;\n  public org.apache.hadoop.fs.shell.CommandFormat(java.lang.String, int, int, java.lang.String...);\n  public org.apache.hadoop.fs.shell.CommandFormat(int, int, java.lang.String...);\n  public java.util.List<java.lang.String> parse(java.lang.String[], int);\n  public void parse(java.util.List<java.lang.String>);\n  public boolean getOpt(java.lang.String);\n  public java.util.Set<java.lang.String> getOpts();\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAStateChangeRequestInfoProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$IpcConnectionContextProto.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ListSpanReceiversResponseProto.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ZKFCProtocol.class": "Compiled from \"ZKFCProtocol.java\"\npublic interface org.apache.hadoop.ha.ZKFCProtocol {\n  public static final long versionID;\n  public abstract void cedeActive(int) throws java.io.IOException, org.apache.hadoop.security.AccessControlException;\n  public abstract void gracefulFailover() throws java.io.IOException, org.apache.hadoop.security.AccessControlException;\n}\n", 
  "org/apache/hadoop/fs/shell/CommandUtils.class": "Compiled from \"CommandUtils.java\"\nfinal class org.apache.hadoop.fs.shell.CommandUtils {\n  org.apache.hadoop.fs.shell.CommandUtils();\n  static java.lang.String formatDescription(java.lang.String, java.lang.String...);\n}\n", 
  "org/apache/hadoop/fs/DF.class": "Compiled from \"DF.java\"\npublic class org.apache.hadoop.fs.DF extends org.apache.hadoop.util.Shell {\n  public static final long DF_INTERVAL_DEFAULT;\n  public org.apache.hadoop.fs.DF(java.io.File, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.DF(java.io.File, long) throws java.io.IOException;\n  public java.lang.String getDirPath();\n  public java.lang.String getFilesystem() throws java.io.IOException;\n  public long getCapacity();\n  public long getUsed();\n  public long getAvailable();\n  public int getPercentUsed();\n  public java.lang.String getMount() throws java.io.IOException;\n  public java.lang.String toString();\n  protected java.lang.String[] getExecString();\n  protected void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  protected void parseOutput() throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$RetryForever.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Count.class": "Compiled from \"Count.java\"\npublic class org.apache.hadoop.fs.shell.Count extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.shell.Count();\n  public org.apache.hadoop.fs.shell.Count(java.lang.String[], int, org.apache.hadoop.conf.Configuration);\n  protected void processOptions(java.util.LinkedList<java.lang.String>);\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  boolean isShowQuotas();\n  boolean isHumanReadable();\n}\n", 
  "org/apache/hadoop/fs/viewfs/ChRootedFileSystem.class": "Compiled from \"ChRootedFileSystem.java\"\nclass org.apache.hadoop.fs.viewfs.ChRootedFileSystem extends org.apache.hadoop.fs.FilterFileSystem {\n  protected org.apache.hadoop.fs.FileSystem getMyFs();\n  protected org.apache.hadoop.fs.Path fullPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ChRootedFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.net.URI getUri();\n  java.lang.String stripOutRoot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.Path getResolvedQualifiedPath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JBoolean$JavaBoolean.class": "Compiled from \"JBoolean.java\"\npublic class org.apache.hadoop.record.compiler.JBoolean extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JBoolean();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/shell/find/Find$3.class": "Compiled from \"Find.java\"\npublic class org.apache.hadoop.fs.shell.find.Find extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.shell.find.Find();\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  void setRootExpression(org.apache.hadoop.fs.shell.find.Expression);\n  org.apache.hadoop.fs.shell.find.Expression getRootExpression();\n  org.apache.hadoop.fs.shell.find.FindOptions getOptions();\n  protected void recursePath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected boolean isPathRecursable(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void postProcessPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/WritableComparator.class": "Compiled from \"WritableComparator.java\"\npublic class org.apache.hadoop.io.WritableComparator implements org.apache.hadoop.io.RawComparator,org.apache.hadoop.conf.Configurable {\n  public static org.apache.hadoop.io.WritableComparator get(java.lang.Class<? extends org.apache.hadoop.io.WritableComparable>);\n  public static org.apache.hadoop.io.WritableComparator get(java.lang.Class<? extends org.apache.hadoop.io.WritableComparable>, org.apache.hadoop.conf.Configuration);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public static void define(java.lang.Class, org.apache.hadoop.io.WritableComparator);\n  protected org.apache.hadoop.io.WritableComparator();\n  protected org.apache.hadoop.io.WritableComparator(java.lang.Class<? extends org.apache.hadoop.io.WritableComparable>);\n  protected org.apache.hadoop.io.WritableComparator(java.lang.Class<? extends org.apache.hadoop.io.WritableComparable>, boolean);\n  protected org.apache.hadoop.io.WritableComparator(java.lang.Class<? extends org.apache.hadoop.io.WritableComparable>, org.apache.hadoop.conf.Configuration, boolean);\n  public java.lang.Class<? extends org.apache.hadoop.io.WritableComparable> getKeyClass();\n  public org.apache.hadoop.io.WritableComparable newKey();\n  public int compare(byte[], int, int, byte[], int, int);\n  public int compare(org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.WritableComparable);\n  public int compare(java.lang.Object, java.lang.Object);\n  public static int compareBytes(byte[], int, int, byte[], int, int);\n  public static int hashBytes(byte[], int, int);\n  public static int hashBytes(byte[], int);\n  public static int readUnsignedShort(byte[], int);\n  public static int readInt(byte[], int);\n  public static float readFloat(byte[], int);\n  public static long readLong(byte[], int);\n  public static double readDouble(byte[], int);\n  public static long readVLong(byte[], int) throws java.io.IOException;\n  public static int readVInt(byte[], int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshRequestProto$1.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$BufferSizeOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshAuthorizationPolicyProtocolService$Stub.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$MonitorHealthResponseProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/Utils$Version.class": "Compiled from \"Utils.java\"\npublic final class org.apache.hadoop.io.file.tfile.Utils {\n  public static void writeVInt(java.io.DataOutput, int) throws java.io.IOException;\n  public static void writeVLong(java.io.DataOutput, long) throws java.io.IOException;\n  public static int readVInt(java.io.DataInput) throws java.io.IOException;\n  public static long readVLong(java.io.DataInput) throws java.io.IOException;\n  public static void writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static <T extends java/lang/Object> int lowerBound(java.util.List<? extends T>, T, java.util.Comparator<? super T>);\n  public static <T extends java/lang/Object> int upperBound(java.util.List<? extends T>, T, java.util.Comparator<? super T>);\n  public static <T extends java/lang/Object> int lowerBound(java.util.List<? extends java.lang.Comparable<? super T>>, T);\n  public static <T extends java/lang/Object> int upperBound(java.util.List<? extends java.lang.Comparable<? super T>>, T);\n}\n", 
  "org/apache/hadoop/io/ObjectWritable$NullInstance.class": "Compiled from \"ObjectWritable.java\"\npublic class org.apache.hadoop.io.ObjectWritable implements org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configurable {\n  public org.apache.hadoop.io.ObjectWritable();\n  public org.apache.hadoop.io.ObjectWritable(java.lang.Object);\n  public org.apache.hadoop.io.ObjectWritable(java.lang.Class, java.lang.Object);\n  public java.lang.Object get();\n  public java.lang.Class getDeclaredClass();\n  public void set(java.lang.Object);\n  public java.lang.String toString();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public static void writeObject(java.io.DataOutput, java.lang.Object, java.lang.Class, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void writeObject(java.io.DataOutput, java.lang.Object, java.lang.Class, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public static java.lang.Object readObject(java.io.DataInput, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.lang.Object readObject(java.io.DataInput, org.apache.hadoop.io.ObjectWritable, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static java.lang.reflect.Method getStaticProtobufMethod(java.lang.Class<?>, java.lang.String, java.lang.Class<?>...);\n  public static java.lang.Class<?> loadClass(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  static java.util.Map access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/DeflateCodec.class": "Compiled from \"DeflateCodec.java\"\npublic class org.apache.hadoop.io.compress.DeflateCodec extends org.apache.hadoop.io.compress.DefaultCodec {\n  public org.apache.hadoop.io.compress.DeflateCodec();\n}\n", 
  "org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.class": "Compiled from \"MD5MD5CRC32FileChecksum.java\"\npublic class org.apache.hadoop.fs.MD5MD5CRC32FileChecksum extends org.apache.hadoop.fs.FileChecksum {\n  public static final int LENGTH;\n  public org.apache.hadoop.fs.MD5MD5CRC32FileChecksum();\n  public org.apache.hadoop.fs.MD5MD5CRC32FileChecksum(int, long, org.apache.hadoop.io.MD5Hash);\n  public java.lang.String getAlgorithmName();\n  public static org.apache.hadoop.util.DataChecksum$Type getCrcTypeFromAlgorithmName(java.lang.String) throws java.io.IOException;\n  public int getLength();\n  public byte[] getBytes();\n  public org.apache.hadoop.util.DataChecksum$Type getCrcType();\n  public org.apache.hadoop.fs.Options$ChecksumOpt getChecksumOpt();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public static void write(org.znerd.xmlenc.XMLOutputter, org.apache.hadoop.fs.MD5MD5CRC32FileChecksum) throws java.io.IOException;\n  public static org.apache.hadoop.fs.MD5MD5CRC32FileChecksum valueOf(org.xml.sax.Attributes) throws org.xml.sax.SAXException;\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/tracing/SpanReceiverHost$1.class": "Compiled from \"SpanReceiverHost.java\"\npublic class org.apache.hadoop.tracing.SpanReceiverHost implements org.apache.hadoop.tracing.TraceAdminProtocol {\n  public static final java.lang.String SPAN_RECEIVERS_CONF_SUFFIX;\n  public static org.apache.hadoop.tracing.SpanReceiverHost get(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public synchronized void loadSpanReceivers(org.apache.hadoop.conf.Configuration);\n  public synchronized void closeReceivers();\n  public synchronized org.apache.hadoop.tracing.SpanReceiverInfo[] listSpanReceivers() throws java.io.IOException;\n  public synchronized long addSpanReceiver(org.apache.hadoop.tracing.SpanReceiverInfo) throws java.io.IOException;\n  public synchronized void removeSpanReceiver(long) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/NullContext.class": "Compiled from \"NullContext.java\"\npublic class org.apache.hadoop.metrics.spi.NullContext extends org.apache.hadoop.metrics.spi.AbstractMetricsContext {\n  public org.apache.hadoop.metrics.spi.NullContext();\n  public void startMonitoring();\n  protected void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord);\n  protected void update(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  protected void remove(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n}\n", 
  "org/apache/hadoop/fs/FileContext$32.class": "", 
  "org/apache/hadoop/io/retry/RetryPolicies$ExceptionDependentRetry.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$Reader$1.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$1.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer$QuotingInputFilter$RequestQuoter$1.class": "Compiled from \"HttpServer.java\"\npublic class org.apache.hadoop.http.HttpServer implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.Connector listener;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[]) throws java.io.IOException;\n  public org.mortbay.jetty.Connector createBaseListener(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean) throws java.io.IOException;\n  protected void addContext(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public void setThreads(int, int);\n  public void addSslListener(java.net.InetSocketAddress, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void addSslListener(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  protected void initSpnego(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void start() throws java.io.IOException;\n  void openListener() throws java.lang.Exception;\n  public java.net.InetSocketAddress getListenerAddress();\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  static org.apache.hadoop.security.ssl.SSLFactory access$000(org.apache.hadoop.http.HttpServer);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/PathIsNotEmptyDirectoryException.class": "Compiled from \"PathIsNotEmptyDirectoryException.java\"\npublic class org.apache.hadoop.fs.PathIsNotEmptyDirectoryException extends org.apache.hadoop.fs.PathExistsException {\n  public org.apache.hadoop.fs.PathIsNotEmptyDirectoryException(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/FileSystem$6.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/UserProvider$Factory.class": "Compiled from \"UserProvider.java\"\npublic class org.apache.hadoop.crypto.key.UserProvider extends org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public boolean isTransient();\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public synchronized void deleteKey(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public java.lang.String toString();\n  public synchronized void flush();\n  public synchronized java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public synchronized java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.crypto.key.UserProvider(org.apache.hadoop.conf.Configuration, org.apache.hadoop.crypto.key.UserProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Reader$LengthOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HARequestSource$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Options$IntegerOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/security/ssl/KeyStoresFactory.class": "Compiled from \"KeyStoresFactory.java\"\npublic interface org.apache.hadoop.security.ssl.KeyStoresFactory extends org.apache.hadoop.conf.Configurable {\n  public abstract void init(org.apache.hadoop.security.ssl.SSLFactory$Mode) throws java.io.IOException, java.security.GeneralSecurityException;\n  public abstract void destroy();\n  public abstract javax.net.ssl.KeyManager[] getKeyManagers();\n  public abstract javax.net.ssl.TrustManager[] getTrustManagers();\n}\n", 
  "org/apache/hadoop/util/JvmPauseMonitor$Monitor.class": "Compiled from \"JvmPauseMonitor.java\"\npublic class org.apache.hadoop.util.JvmPauseMonitor {\n  public org.apache.hadoop.util.JvmPauseMonitor(org.apache.hadoop.conf.Configuration);\n  public void start();\n  public void stop();\n  public boolean isStarted();\n  public long getNumGcWarnThreadholdExceeded();\n  public long getNumGcInfoThresholdExceeded();\n  public long getTotalGcExtraSleepTime();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static java.util.Map access$400(org.apache.hadoop.util.JvmPauseMonitor);\n  static boolean access$500(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$600(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$704(org.apache.hadoop.util.JvmPauseMonitor);\n  static java.lang.String access$800(org.apache.hadoop.util.JvmPauseMonitor, long, java.util.Map, java.util.Map);\n  static org.apache.commons.logging.Log access$900();\n  static long access$1000(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$1104(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$1214(org.apache.hadoop.util.JvmPauseMonitor, long);\n  static {};\n}\n", 
  "org/apache/hadoop/net/SocketInputStream.class": "Compiled from \"SocketInputStream.java\"\npublic class org.apache.hadoop.net.SocketInputStream extends java.io.InputStream implements java.nio.channels.ReadableByteChannel {\n  public org.apache.hadoop.net.SocketInputStream(java.nio.channels.ReadableByteChannel, long) throws java.io.IOException;\n  public org.apache.hadoop.net.SocketInputStream(java.net.Socket, long) throws java.io.IOException;\n  public org.apache.hadoop.net.SocketInputStream(java.net.Socket) throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public synchronized void close() throws java.io.IOException;\n  public java.nio.channels.ReadableByteChannel getChannel();\n  public boolean isOpen();\n  public int read(java.nio.ByteBuffer) throws java.io.IOException;\n  public void waitForReadable() throws java.io.IOException;\n  public void setTimeout(long);\n}\n", 
  "org/apache/hadoop/security/token/SecretManager$1.class": "Compiled from \"SecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.SecretManager<T extends org.apache.hadoop.security.token.TokenIdentifier> {\n  public org.apache.hadoop.security.token.SecretManager();\n  protected abstract byte[] createPassword(T);\n  public abstract byte[] retrievePassword(T) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public byte[] retriableRetrievePassword(T) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, org.apache.hadoop.ipc.StandbyException, org.apache.hadoop.ipc.RetriableException, java.io.IOException;\n  public abstract T createIdentifier();\n  public void checkAvailableForRead() throws org.apache.hadoop.ipc.StandbyException;\n  protected javax.crypto.SecretKey generateSecret();\n  protected static byte[] createPassword(byte[], javax.crypto.SecretKey);\n  protected static javax.crypto.SecretKey createSecretKey(byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Options$ProgressableOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/io/nativeio/NativeIO$CachedUid.class": "Compiled from \"NativeIO.java\"\npublic class org.apache.hadoop.io.nativeio.NativeIO {\n  public org.apache.hadoop.io.nativeio.NativeIO();\n  public static boolean isAvailable();\n  static long getMemlockLimit();\n  static long getOperatingSystemPageSize();\n  public static java.lang.String getOwner(java.io.FileDescriptor) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File) throws java.io.IOException;\n  public static java.io.FileInputStream getShareDeleteFileInputStream(java.io.File, long) throws java.io.IOException;\n  public static java.io.FileOutputStream getCreateForWriteFileOutputStream(java.io.File, int) throws java.io.IOException;\n  public static void renameTo(java.io.File, java.io.File) throws java.io.IOException;\n  public static void link(java.io.File, java.io.File) throws java.io.IOException;\n  public static void copyFileUnbuffered(java.io.File, java.io.File) throws java.io.IOException;\n  static boolean access$102(boolean);\n  static void access$200();\n  static java.lang.String access$300(java.lang.String);\n  static boolean access$802(boolean);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$25.class": "", 
  "org/apache/hadoop/conf/Reconfigurable.class": "Compiled from \"Reconfigurable.java\"\npublic interface org.apache.hadoop.conf.Reconfigurable extends org.apache.hadoop.conf.Configurable {\n  public abstract java.lang.String reconfigureProperty(java.lang.String, java.lang.String) throws org.apache.hadoop.conf.ReconfigurationException;\n  public abstract boolean isPropertyReconfigurable(java.lang.String);\n  public abstract java.util.Collection<java.lang.String> getReconfigurableProperties();\n}\n", 
  "org/apache/hadoop/ipc/Server$Listener.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolInfoService.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableRate.class": "Compiled from \"MutableRate.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableRate extends org.apache.hadoop.metrics2.lib.MutableStat {\n  org.apache.hadoop.metrics2.lib.MutableRate(java.lang.String, java.lang.String, boolean);\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider$KMSMetadata.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$CedeActiveResponseProto$Builder.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RPC$RpcKind.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslPlainServer$SaslPlainServerFactory.class": "Compiled from \"SaslPlainServer.java\"\npublic class org.apache.hadoop.security.SaslPlainServer implements javax.security.sasl.SaslServer {\n  org.apache.hadoop.security.SaslPlainServer(javax.security.auth.callback.CallbackHandler);\n  public java.lang.String getMechanismName();\n  public byte[] evaluateResponse(byte[]) throws javax.security.sasl.SaslException;\n  public boolean isComplete();\n  public java.lang.String getAuthorizationID();\n  public java.lang.Object getNegotiatedProperty(java.lang.String);\n  public byte[] wrap(byte[], int, int) throws javax.security.sasl.SaslException;\n  public byte[] unwrap(byte[], int, int) throws javax.security.sasl.SaslException;\n  public void dispose() throws javax.security.sasl.SaslException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$SpanReceiverListInfo$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Options$PathOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/net/AbstractDNSToSwitchMapping.class": "Compiled from \"AbstractDNSToSwitchMapping.java\"\npublic abstract class org.apache.hadoop.net.AbstractDNSToSwitchMapping implements org.apache.hadoop.net.DNSToSwitchMapping,org.apache.hadoop.conf.Configurable {\n  protected org.apache.hadoop.net.AbstractDNSToSwitchMapping();\n  protected org.apache.hadoop.net.AbstractDNSToSwitchMapping(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public boolean isSingleSwitch();\n  public java.util.Map<java.lang.String, java.lang.String> getSwitchMap();\n  public java.lang.String dumpTopology();\n  protected boolean isSingleSwitchByScriptPolicy();\n  public static boolean isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping);\n}\n", 
  "org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.class": "Compiled from \"ZKFCProtocolClientSideTranslatorPB.java\"\npublic class org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB implements org.apache.hadoop.ha.ZKFCProtocol,java.io.Closeable,org.apache.hadoop.ipc.ProtocolTranslator {\n  public org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public void cedeActive(int) throws java.io.IOException, org.apache.hadoop.security.AccessControlException;\n  public void gracefulFailover() throws java.io.IOException, org.apache.hadoop.security.AccessControlException;\n  public void close();\n  public java.lang.Object getUnderlyingProxyObject();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.class": "Compiled from \"RefreshCallQueueProtocolClientSideTranslatorPB.java\"\npublic class org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB implements org.apache.hadoop.ipc.ProtocolMetaInterface,org.apache.hadoop.ipc.RefreshCallQueueProtocol,java.io.Closeable {\n  public org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB(org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB);\n  public void close() throws java.io.IOException;\n  public void refreshCallQueue() throws java.io.IOException;\n  public boolean isMethodSupported(java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/ComparableVersion.class": "Compiled from \"ComparableVersion.java\"\npublic class org.apache.hadoop.util.ComparableVersion implements java.lang.Comparable<org.apache.hadoop.util.ComparableVersion> {\n  public org.apache.hadoop.util.ComparableVersion(java.lang.String);\n  public final void parseVersion(java.lang.String);\n  public int compareTo(org.apache.hadoop.util.ComparableVersion);\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/ipc/IpcException.class": "Compiled from \"IpcException.java\"\npublic class org.apache.hadoop.ipc.IpcException extends java.io.IOException {\n  final java.lang.String errMsg;\n  public org.apache.hadoop.ipc.IpcException(java.lang.String);\n}\n", 
  "org/apache/hadoop/security/alias/UserProvider$Factory.class": "Compiled from \"UserProvider.java\"\npublic class org.apache.hadoop.security.alias.UserProvider extends org.apache.hadoop.security.alias.CredentialProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public boolean isTransient();\n  public org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry getCredentialEntry(java.lang.String);\n  public org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry createCredentialEntry(java.lang.String, char[]) throws java.io.IOException;\n  public void deleteCredentialEntry(java.lang.String) throws java.io.IOException;\n  public java.lang.String toString();\n  public void flush();\n  public java.util.List<java.lang.String> getAliases() throws java.io.IOException;\n  org.apache.hadoop.security.alias.UserProvider(org.apache.hadoop.security.alias.UserProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts$Progress.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsConfigException.class": "Compiled from \"MetricsConfigException.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsConfigException extends org.apache.hadoop.metrics2.MetricsException {\n  org.apache.hadoop.metrics2.impl.MetricsConfigException(java.lang.String);\n  org.apache.hadoop.metrics2.impl.MetricsConfigException(java.lang.String, java.lang.Throwable);\n  org.apache.hadoop.metrics2.impl.MetricsConfigException(java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueProtocolService$BlockingInterface.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/LocalDirAllocator$1.class": "Compiled from \"LocalDirAllocator.java\"\npublic class org.apache.hadoop.fs.LocalDirAllocator {\n  public static final int SIZE_UNKNOWN;\n  public org.apache.hadoop.fs.LocalDirAllocator(java.lang.String);\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathToRead(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.Iterable<org.apache.hadoop.fs.Path> getAllLocalPathsToRead(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.io.File createTmpFileForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean isContextValid(java.lang.String);\n  public static void removeContext(java.lang.String);\n  public boolean ifExists(java.lang.String, org.apache.hadoop.conf.Configuration);\n  int getCurrentDirectoryIndex();\n  static {};\n}\n", 
  "org/apache/hadoop/util/FileBasedIPList.class": "Compiled from \"FileBasedIPList.java\"\npublic class org.apache.hadoop.util.FileBasedIPList implements org.apache.hadoop.util.IPList {\n  public org.apache.hadoop.util.FileBasedIPList(java.lang.String);\n  public org.apache.hadoop.util.FileBasedIPList reload();\n  public boolean isIn(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$GracefulFailoverResponseProto$Builder.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/CachingKeyProvider$1.class": "Compiled from \"CachingKeyProvider.java\"\npublic class org.apache.hadoop.crypto.key.CachingKeyProvider extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension> {\n  public org.apache.hadoop.crypto.key.CachingKeyProvider(org.apache.hadoop.crypto.key.KeyProvider, long, long);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolInfoService$Interface.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpRequestLog.class": "Compiled from \"HttpRequestLog.java\"\npublic class org.apache.hadoop.http.HttpRequestLog {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.http.HttpRequestLog();\n  public static org.mortbay.jetty.RequestLog getRequestLog(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/CopyCommands$Put.class": "Compiled from \"CopyCommands.java\"\nclass org.apache.hadoop.fs.shell.CopyCommands {\n  org.apache.hadoop.fs.shell.CopyCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/fs/permission/AclEntry$Builder.class": "Compiled from \"AclEntry.java\"\npublic class org.apache.hadoop.fs.permission.AclEntry {\n  public org.apache.hadoop.fs.permission.AclEntryType getType();\n  public java.lang.String getName();\n  public org.apache.hadoop.fs.permission.FsAction getPermission();\n  public org.apache.hadoop.fs.permission.AclEntryScope getScope();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public static java.util.List<org.apache.hadoop.fs.permission.AclEntry> parseAclSpec(java.lang.String, boolean);\n  public static org.apache.hadoop.fs.permission.AclEntry parseAclEntry(java.lang.String, boolean);\n  public static java.lang.String aclSpecToString(java.util.List<org.apache.hadoop.fs.permission.AclEntry>);\n  org.apache.hadoop.fs.permission.AclEntry(org.apache.hadoop.fs.permission.AclEntryType, java.lang.String, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.AclEntryScope, org.apache.hadoop.fs.permission.AclEntry$1);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolSignatureRequestProto$1.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController$5.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Client$Connection.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$FileOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/AclEntry.class": "Compiled from \"AclEntry.java\"\npublic class org.apache.hadoop.fs.permission.AclEntry {\n  public org.apache.hadoop.fs.permission.AclEntryType getType();\n  public java.lang.String getName();\n  public org.apache.hadoop.fs.permission.FsAction getPermission();\n  public org.apache.hadoop.fs.permission.AclEntryScope getScope();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public static java.util.List<org.apache.hadoop.fs.permission.AclEntry> parseAclSpec(java.lang.String, boolean);\n  public static org.apache.hadoop.fs.permission.AclEntry parseAclEntry(java.lang.String, boolean);\n  public static java.lang.String aclSpecToString(java.util.List<org.apache.hadoop.fs.permission.AclEntry>);\n  org.apache.hadoop.fs.permission.AclEntry(org.apache.hadoop.fs.permission.AclEntryType, java.lang.String, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.AclEntryScope, org.apache.hadoop.fs.permission.AclEntry$1);\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$CancelDelegationTokenResponseProtoOrBuilder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/SinkQueue.class": "Compiled from \"SinkQueue.java\"\nclass org.apache.hadoop.metrics2.impl.SinkQueue<T> {\n  org.apache.hadoop.metrics2.impl.SinkQueue(int);\n  synchronized boolean enqueue(T);\n  void consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer<T>) throws java.lang.InterruptedException;\n  void consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer<T>) throws java.lang.InterruptedException;\n  synchronized T dequeue() throws java.lang.InterruptedException;\n  synchronized T front();\n  synchronized T back();\n  synchronized void clear();\n  synchronized int size();\n  int capacity();\n}\n", 
  "org/apache/hadoop/net/ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency.class": "Compiled from \"ScriptBasedMappingWithDependency.java\"\npublic class org.apache.hadoop.net.ScriptBasedMappingWithDependency extends org.apache.hadoop.net.ScriptBasedMapping implements org.apache.hadoop.net.DNSToSwitchMappingWithDependency {\n  static final java.lang.String DEPENDENCY_SCRIPT_FILENAME_KEY;\n  public org.apache.hadoop.net.ScriptBasedMappingWithDependency();\n  public java.lang.String toString();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public java.util.List<java.lang.String> getDependency(java.lang.String);\n}\n", 
  "org/apache/hadoop/io/OutputBuffer$1.class": "Compiled from \"OutputBuffer.java\"\npublic class org.apache.hadoop.io.OutputBuffer extends java.io.FilterOutputStream {\n  public org.apache.hadoop.io.OutputBuffer();\n  public byte[] getData();\n  public int getLength();\n  public org.apache.hadoop.io.OutputBuffer reset();\n  public void write(java.io.InputStream, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HARequestSource.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/generated/Token.class": "Compiled from \"Token.java\"\npublic class org.apache.hadoop.record.compiler.generated.Token {\n  public int kind;\n  public int beginLine;\n  public int beginColumn;\n  public int endLine;\n  public int endColumn;\n  public java.lang.String image;\n  public org.apache.hadoop.record.compiler.generated.Token next;\n  public org.apache.hadoop.record.compiler.generated.Token specialToken;\n  public org.apache.hadoop.record.compiler.generated.Token();\n  public java.lang.String toString();\n  public static final org.apache.hadoop.record.compiler.generated.Token newToken(int);\n}\n", 
  "org/apache/hadoop/crypto/CipherSuite.class": "Compiled from \"CipherSuite.java\"\npublic final class org.apache.hadoop.crypto.CipherSuite extends java.lang.Enum<org.apache.hadoop.crypto.CipherSuite> {\n  public static final org.apache.hadoop.crypto.CipherSuite UNKNOWN;\n  public static final org.apache.hadoop.crypto.CipherSuite AES_CTR_NOPADDING;\n  public static org.apache.hadoop.crypto.CipherSuite[] values();\n  public static org.apache.hadoop.crypto.CipherSuite valueOf(java.lang.String);\n  public void setUnknownValue(int);\n  public int getUnknownValue();\n  public java.lang.String getName();\n  public int getAlgorithmBlockSize();\n  public java.lang.String toString();\n  public static org.apache.hadoop.crypto.CipherSuite convert(java.lang.String);\n  public java.lang.String getConfigSuffix();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAStateChangeRequestInfoProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtocolInfo.class": "Compiled from \"ProtocolInfo.java\"\npublic interface org.apache.hadoop.ipc.ProtocolInfo extends java.lang.annotation.Annotation {\n  public abstract java.lang.String protocolName();\n  public abstract long protocolVersion();\n}\n", 
  "org/apache/hadoop/log/LogLevel.class": "Compiled from \"LogLevel.java\"\npublic class org.apache.hadoop.log.LogLevel {\n  public static final java.lang.String USAGES;\n  static final java.lang.String MARKER;\n  static final java.util.regex.Pattern TAG;\n  public org.apache.hadoop.log.LogLevel();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/MoveCommands$MoveFromLocal.class": "Compiled from \"MoveCommands.java\"\nclass org.apache.hadoop.fs.shell.MoveCommands {\n  org.apache.hadoop.fs.shell.MoveCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ipc/RpcInvocationHandler.class": "Compiled from \"RpcInvocationHandler.java\"\npublic interface org.apache.hadoop.ipc.RpcInvocationHandler extends java.lang.reflect.InvocationHandler,java.io.Closeable {\n  public abstract org.apache.hadoop.ipc.Client$ConnectionId getConnectionId();\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Writer$MetaBlockRegister.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolSignatureProtoOrBuilder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$RetryLimited.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/security/HadoopKerberosName.class": "Compiled from \"HadoopKerberosName.java\"\npublic class org.apache.hadoop.security.HadoopKerberosName extends org.apache.hadoop.security.authentication.util.KerberosName {\n  public org.apache.hadoop.security.HadoopKerberosName(java.lang.String);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n", 
  "org/apache/hadoop/crypto/random/OsSecureRandom.class": "Compiled from \"OsSecureRandom.java\"\npublic class org.apache.hadoop.crypto.random.OsSecureRandom extends java.util.Random implements java.io.Closeable,org.apache.hadoop.conf.Configurable {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.crypto.random.OsSecureRandom();\n  public synchronized void setConf(org.apache.hadoop.conf.Configuration);\n  public synchronized org.apache.hadoop.conf.Configuration getConf();\n  public synchronized void nextBytes(byte[]);\n  protected synchronized int next(int);\n  public synchronized void close();\n  static {};\n}\n", 
  "org/apache/hadoop/io/ShortWritable$Comparator.class": "Compiled from \"ShortWritable.java\"\npublic class org.apache.hadoop.io.ShortWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.ShortWritable> {\n  public org.apache.hadoop.io.ShortWritable();\n  public org.apache.hadoop.io.ShortWritable(short);\n  public void set(short);\n  public short get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.ShortWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.class": "Compiled from \"DefaultFailoverProxyProvider.java\"\npublic class org.apache.hadoop.io.retry.DefaultFailoverProxyProvider<T> implements org.apache.hadoop.io.retry.FailoverProxyProvider<T> {\n  public org.apache.hadoop.io.retry.DefaultFailoverProxyProvider(java.lang.Class<T>, T);\n  public java.lang.Class<T> getInterface();\n  public org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo<T> getProxy();\n  public void performFailover(T);\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$ZKFCProtocolService$Interface.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$RetryUpToMaximumCountWithProportionalSleep.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$GracefulFailoverResponseProto.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos$RequestHeaderProto$1.class": "Compiled from \"ProtobufRpcEngineProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1102(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/http/NoCacheFilter.class": "Compiled from \"NoCacheFilter.java\"\npublic class org.apache.hadoop.http.NoCacheFilter implements javax.servlet.Filter {\n  public org.apache.hadoop.http.NoCacheFilter();\n  public void init(javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  public void doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain) throws java.io.IOException, javax.servlet.ServletException;\n  public void destroy();\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocket.class": "Compiled from \"DomainSocket.java\"\npublic class org.apache.hadoop.net.unix.DomainSocket implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final org.apache.hadoop.util.CloseableReferenceCount refCount;\n  final int fd;\n  public static final int SEND_BUFFER_SIZE;\n  public static final int RECEIVE_BUFFER_SIZE;\n  public static final int SEND_TIMEOUT;\n  public static final int RECEIVE_TIMEOUT;\n  static native void validateSocketPathSecurity0(java.lang.String, int) throws java.io.IOException;\n  public static java.lang.String getLoadingFailureReason();\n  public static void disableBindPathValidation();\n  public static java.lang.String getEffectivePath(java.lang.String, int);\n  public static org.apache.hadoop.net.unix.DomainSocket bindAndListen(java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.net.unix.DomainSocket[] socketpair() throws java.io.IOException;\n  public org.apache.hadoop.net.unix.DomainSocket accept() throws java.io.IOException;\n  public static org.apache.hadoop.net.unix.DomainSocket connect(java.lang.String) throws java.io.IOException;\n  public boolean isOpen();\n  public java.lang.String getPath();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainInputStream getInputStream();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream getOutputStream();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainChannel getChannel();\n  public void setAttribute(int, int) throws java.io.IOException;\n  public int getAttribute(int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void shutdown() throws java.io.IOException;\n  public void sendFileDescriptors(java.io.FileDescriptor[], byte[], int, int) throws java.io.IOException;\n  public int receiveFileDescriptors(java.io.FileDescriptor[], byte[], int, int) throws java.io.IOException;\n  public int recvFileInputStreams(java.io.FileInputStream[], byte[], int, int) throws java.io.IOException;\n  public java.lang.String toString();\n  static int access$000(int, byte[], int, int) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.net.unix.DomainSocket, boolean) throws java.nio.channels.ClosedChannelException;\n  static int access$200(int) throws java.io.IOException;\n  static void access$300(int, byte[], int, int) throws java.io.IOException;\n  static int access$400(int, java.nio.ByteBuffer, int, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/Text$Comparator.class": "Compiled from \"Text.java\"\npublic class org.apache.hadoop.io.Text extends org.apache.hadoop.io.BinaryComparable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.BinaryComparable> {\n  public static final int DEFAULT_MAX_LEN;\n  static final int[] bytesFromUTF8;\n  static final int[] offsetsFromUTF8;\n  public org.apache.hadoop.io.Text();\n  public org.apache.hadoop.io.Text(java.lang.String);\n  public org.apache.hadoop.io.Text(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text(byte[]);\n  public byte[] copyBytes();\n  public byte[] getBytes();\n  public int getLength();\n  public int charAt(int);\n  public int find(java.lang.String);\n  public int find(java.lang.String, int);\n  public void set(java.lang.String);\n  public void set(byte[]);\n  public void set(org.apache.hadoop.io.Text);\n  public void set(byte[], int, int);\n  public void append(byte[], int, int);\n  public void clear();\n  public java.lang.String toString();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void readFields(java.io.DataInput, int) throws java.io.IOException;\n  public static void skip(java.io.DataInput) throws java.io.IOException;\n  public void readWithKnownLength(java.io.DataInput, int) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void write(java.io.DataOutput, int) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static java.lang.String decode(byte[]) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String decode(byte[], int, int) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String decode(byte[], int, int, boolean) throws java.nio.charset.CharacterCodingException;\n  public static java.nio.ByteBuffer encode(java.lang.String) throws java.nio.charset.CharacterCodingException;\n  public static java.nio.ByteBuffer encode(java.lang.String, boolean) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput, int) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String, int) throws java.io.IOException;\n  public static void validateUTF8(byte[]) throws java.nio.charset.MalformedInputException;\n  public static void validateUTF8(byte[], int, int) throws java.nio.charset.MalformedInputException;\n  public static int bytesToCodePoint(java.nio.ByteBuffer);\n  public static int utf8Length(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsBase.class": "Compiled from \"MetricsBase.java\"\npublic abstract class org.apache.hadoop.metrics.util.MetricsBase {\n  public static final java.lang.String NO_DESCRIPTION;\n  protected org.apache.hadoop.metrics.util.MetricsBase(java.lang.String);\n  protected org.apache.hadoop.metrics.util.MetricsBase(java.lang.String, java.lang.String);\n  public abstract void pushMetric(org.apache.hadoop.metrics.MetricsRecord);\n  public java.lang.String getName();\n  public java.lang.String getDescription();\n}\n", 
  "org/apache/hadoop/fs/shell/Delete$Rmr.class": "Compiled from \"Delete.java\"\nclass org.apache.hadoop.fs.shell.Delete {\n  org.apache.hadoop.fs.shell.Delete();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/util/Waitable.class": "Compiled from \"Waitable.java\"\npublic class org.apache.hadoop.util.Waitable<T> {\n  public org.apache.hadoop.util.Waitable(java.util.concurrent.locks.Condition);\n  public T await() throws java.lang.InterruptedException;\n  public void provide(T);\n  public boolean hasVal();\n  public T getVal();\n}\n", 
  "org/apache/hadoop/service/LoggingStateChangeListener.class": "Compiled from \"LoggingStateChangeListener.java\"\npublic class org.apache.hadoop.service.LoggingStateChangeListener implements org.apache.hadoop.service.ServiceStateChangeListener {\n  public org.apache.hadoop.service.LoggingStateChangeListener(org.apache.commons.logging.Log);\n  public org.apache.hadoop.service.LoggingStateChangeListener();\n  public void stateChanged(org.apache.hadoop.service.Service);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtocolSignature$1.class": "Compiled from \"ProtocolSignature.java\"\npublic class org.apache.hadoop.ipc.ProtocolSignature implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.ipc.ProtocolSignature();\n  public org.apache.hadoop.ipc.ProtocolSignature(long, int[]);\n  public long getVersion();\n  public int[] getMethods();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  static int getFingerprint(java.lang.reflect.Method);\n  static int getFingerprint(java.lang.reflect.Method[]);\n  static int getFingerprint(int[]);\n  public static void resetCache();\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(int, long, java.lang.Class<? extends org.apache.hadoop.ipc.VersionedProtocol>);\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(java.lang.String, long) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol, java.lang.String, long, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$MonitorHealthRequestProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Sorter$RawKeyValueIterator.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns$Tags$1.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/security/ssl/SSLFactory$Mode.class": "Compiled from \"SSLFactory.java\"\npublic class org.apache.hadoop.security.ssl.SSLFactory implements org.apache.hadoop.security.authentication.client.ConnectionConfigurator {\n  public static final java.lang.String SSL_REQUIRE_CLIENT_CERT_KEY;\n  public static final java.lang.String SSL_HOSTNAME_VERIFIER_KEY;\n  public static final java.lang.String SSL_CLIENT_CONF_KEY;\n  public static final java.lang.String SSL_SERVER_CONF_KEY;\n  public static final java.lang.String SSLCERTIFICATE;\n  public static final boolean DEFAULT_SSL_REQUIRE_CLIENT_CERT;\n  public static final java.lang.String KEYSTORES_FACTORY_CLASS_KEY;\n  public static final java.lang.String SSL_ENABLED_PROTOCOLS;\n  public static final java.lang.String DEFAULT_SSL_ENABLED_PROTOCOLS;\n  public org.apache.hadoop.security.ssl.SSLFactory(org.apache.hadoop.security.ssl.SSLFactory$Mode, org.apache.hadoop.conf.Configuration);\n  public void init() throws java.security.GeneralSecurityException, java.io.IOException;\n  public static javax.net.ssl.HostnameVerifier getHostnameVerifier(java.lang.String) throws java.security.GeneralSecurityException, java.io.IOException;\n  public void destroy();\n  public org.apache.hadoop.security.ssl.KeyStoresFactory getKeystoresFactory();\n  public javax.net.ssl.SSLEngine createSSLEngine() throws java.security.GeneralSecurityException, java.io.IOException;\n  public javax.net.ssl.SSLServerSocketFactory createSSLServerSocketFactory() throws java.security.GeneralSecurityException, java.io.IOException;\n  public javax.net.ssl.SSLSocketFactory createSSLSocketFactory() throws java.security.GeneralSecurityException, java.io.IOException;\n  public javax.net.ssl.HostnameVerifier getHostnameVerifier();\n  public boolean isClientCertRequired();\n  public java.net.HttpURLConnection configure(java.net.HttpURLConnection) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/UserProvider.class": "Compiled from \"UserProvider.java\"\npublic class org.apache.hadoop.security.alias.UserProvider extends org.apache.hadoop.security.alias.CredentialProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public boolean isTransient();\n  public org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry getCredentialEntry(java.lang.String);\n  public org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry createCredentialEntry(java.lang.String, char[]) throws java.io.IOException;\n  public void deleteCredentialEntry(java.lang.String) throws java.io.IOException;\n  public java.lang.String toString();\n  public void flush();\n  public java.util.List<java.lang.String> getAliases() throws java.io.IOException;\n  org.apache.hadoop.security.alias.UserProvider(org.apache.hadoop.security.alias.UserProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/Client$1.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/security/ssl/SSLFactory.class": "Compiled from \"SSLFactory.java\"\npublic class org.apache.hadoop.security.ssl.SSLFactory implements org.apache.hadoop.security.authentication.client.ConnectionConfigurator {\n  public static final java.lang.String SSL_REQUIRE_CLIENT_CERT_KEY;\n  public static final java.lang.String SSL_HOSTNAME_VERIFIER_KEY;\n  public static final java.lang.String SSL_CLIENT_CONF_KEY;\n  public static final java.lang.String SSL_SERVER_CONF_KEY;\n  public static final java.lang.String SSLCERTIFICATE;\n  public static final boolean DEFAULT_SSL_REQUIRE_CLIENT_CERT;\n  public static final java.lang.String KEYSTORES_FACTORY_CLASS_KEY;\n  public static final java.lang.String SSL_ENABLED_PROTOCOLS;\n  public static final java.lang.String DEFAULT_SSL_ENABLED_PROTOCOLS;\n  public org.apache.hadoop.security.ssl.SSLFactory(org.apache.hadoop.security.ssl.SSLFactory$Mode, org.apache.hadoop.conf.Configuration);\n  public void init() throws java.security.GeneralSecurityException, java.io.IOException;\n  public static javax.net.ssl.HostnameVerifier getHostnameVerifier(java.lang.String) throws java.security.GeneralSecurityException, java.io.IOException;\n  public void destroy();\n  public org.apache.hadoop.security.ssl.KeyStoresFactory getKeystoresFactory();\n  public javax.net.ssl.SSLEngine createSSLEngine() throws java.security.GeneralSecurityException, java.io.IOException;\n  public javax.net.ssl.SSLServerSocketFactory createSSLServerSocketFactory() throws java.security.GeneralSecurityException, java.io.IOException;\n  public javax.net.ssl.SSLSocketFactory createSSLSocketFactory() throws java.security.GeneralSecurityException, java.io.IOException;\n  public javax.net.ssl.HostnameVerifier getHostnameVerifier();\n  public boolean isClientCertRequired();\n  public java.net.HttpURLConnection configure(java.net.HttpURLConnection) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/Tool.class": "Compiled from \"Tool.java\"\npublic interface org.apache.hadoop.util.Tool extends org.apache.hadoop.conf.Configurable {\n  public abstract int run(java.lang.String[]) throws java.lang.Exception;\n}\n", 
  "org/apache/hadoop/security/SaslOutputStream.class": "Compiled from \"SaslOutputStream.java\"\npublic class org.apache.hadoop.security.SaslOutputStream extends java.io.OutputStream {\n  public org.apache.hadoop.security.SaslOutputStream(java.io.OutputStream, javax.security.sasl.SaslServer);\n  public org.apache.hadoop.security.SaslOutputStream(java.io.OutputStream, javax.security.sasl.SaslClient);\n  public void write(int) throws java.io.IOException;\n  public void write(byte[]) throws java.io.IOException;\n  public void write(byte[], int, int) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/sink/GraphiteSink$Graphite.class": "Compiled from \"GraphiteSink.java\"\npublic class org.apache.hadoop.metrics2.sink.GraphiteSink implements org.apache.hadoop.metrics2.MetricsSink,java.io.Closeable {\n  public org.apache.hadoop.metrics2.sink.GraphiteSink();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  public void putMetrics(org.apache.hadoop.metrics2.MetricsRecord);\n  public void flush();\n  public void close() throws java.io.IOException;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$1.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/BatchedRemoteIterator$BatchedListEntries.class": "Compiled from \"BatchedRemoteIterator.java\"\npublic abstract class org.apache.hadoop.fs.BatchedRemoteIterator<K, E> implements org.apache.hadoop.fs.RemoteIterator<E> {\n  public org.apache.hadoop.fs.BatchedRemoteIterator(K);\n  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<E> makeRequest(K) throws java.io.IOException;\n  public boolean hasNext() throws java.io.IOException;\n  public abstract K elementToPrevKey(E);\n  public E next() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/FileContext$19.class": "", 
  "org/apache/hadoop/record/compiler/CGenerator.class": "Compiled from \"CGenerator.java\"\nclass org.apache.hadoop.record.compiler.CGenerator extends org.apache.hadoop.record.compiler.CodeGenerator {\n  org.apache.hadoop.record.compiler.CGenerator();\n  void genCode(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JFile>, java.util.ArrayList<org.apache.hadoop.record.compiler.JRecord>, java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto$SaslState.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticator.class": "Compiled from \"KerberosDelegationTokenAuthenticator.java\"\npublic class org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator extends org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator {\n  public org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcResponseHeaderProto$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/MetricsServlet.class": "Compiled from \"MetricsServlet.java\"\npublic class org.apache.hadoop.metrics.MetricsServlet extends javax.servlet.http.HttpServlet {\n  public org.apache.hadoop.metrics.MetricsServlet();\n  java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.util.List<org.apache.hadoop.metrics.MetricsServlet$TagsMetricsPair>>> makeMap(java.util.Collection<org.apache.hadoop.metrics.MetricsContext>) throws java.io.IOException;\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException;\n  void printMap(java.io.PrintWriter, java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.util.List<org.apache.hadoop.metrics.MetricsServlet$TagsMetricsPair>>>);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolVersionProtoOrBuilder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/StandardSocketFactory.class": "Compiled from \"StandardSocketFactory.java\"\npublic class org.apache.hadoop.net.StandardSocketFactory extends javax.net.SocketFactory {\n  public org.apache.hadoop.net.StandardSocketFactory();\n  public java.net.Socket createSocket() throws java.io.IOException;\n  public java.net.Socket createSocket(java.net.InetAddress, int) throws java.io.IOException;\n  public java.net.Socket createSocket(java.net.InetAddress, int, java.net.InetAddress, int) throws java.io.IOException;\n  public java.net.Socket createSocket(java.lang.String, int) throws java.io.IOException, java.net.UnknownHostException;\n  public java.net.Socket createSocket(java.lang.String, int, java.net.InetAddress, int) throws java.io.IOException, java.net.UnknownHostException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n", 
  "org/apache/hadoop/fs/FileContext$36.class": "", 
  "org/apache/hadoop/io/compress/GzipCodec$GzipOutputStream$ResetableGZIPOutputStream.class": "Compiled from \"GzipCodec.java\"\npublic class org.apache.hadoop.io.compress.GzipCodec extends org.apache.hadoop.io.compress.DefaultCodec {\n  public org.apache.hadoop.io.compress.GzipCodec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.DirectDecompressor createDirectDecompressor();\n  public java.lang.String getDefaultExtension();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$IpcConnectionContextProto$1.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RPC$Server$ProtoClassProtoImpl.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JByte$JavaByte.class": "Compiled from \"JByte.java\"\npublic class org.apache.hadoop.record.compiler.JByte extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JByte();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/util/bloom/RemoveScheme.class": "Compiled from \"RemoveScheme.java\"\npublic interface org.apache.hadoop.util.bloom.RemoveScheme {\n  public static final short RANDOM;\n  public static final short MINIMUM_FN;\n  public static final short MAXIMUM_FP;\n  public static final short RATIO;\n}\n", 
  "org/apache/hadoop/metrics2/util/SampleQuantiles.class": "Compiled from \"SampleQuantiles.java\"\npublic class org.apache.hadoop.metrics2.util.SampleQuantiles {\n  public org.apache.hadoop.metrics2.util.SampleQuantiles(org.apache.hadoop.metrics2.util.Quantile[]);\n  public synchronized void insert(long);\n  public synchronized java.util.Map<org.apache.hadoop.metrics2.util.Quantile, java.lang.Long> snapshot();\n  public synchronized long getCount();\n  public synchronized int getSampleCount();\n  public synchronized void clear();\n  public synchronized java.lang.String toString();\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSystemImpl$5.class": "Compiled from \"MetricsSystemImpl.java\"\npublic class org.apache.hadoop.metrics2.impl.MetricsSystemImpl extends org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSource {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String MS_NAME;\n  static final java.lang.String MS_STATS_NAME;\n  static final java.lang.String MS_STATS_DESC;\n  static final java.lang.String MS_CONTROL_NAME;\n  static final java.lang.String MS_INIT_MODE_KEY;\n  org.apache.hadoop.metrics2.lib.MutableStat snapshotStat;\n  org.apache.hadoop.metrics2.lib.MutableStat publishStat;\n  org.apache.hadoop.metrics2.lib.MutableCounterLong droppedPubAll;\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl(java.lang.String);\n  public org.apache.hadoop.metrics2.impl.MetricsSystemImpl();\n  public synchronized org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public synchronized void unregisterSource(java.lang.String);\n  synchronized void registerSource(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSource);\n  public synchronized <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  synchronized void registerSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink);\n  public synchronized void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public synchronized void startMetricsMBeans();\n  public synchronized void stopMetricsMBeans();\n  public synchronized java.lang.String currentConfig();\n  synchronized void onTimerEvent();\n  public synchronized void publishMetricsNow();\n  synchronized org.apache.hadoop.metrics2.impl.MetricsBuffer sampleMetrics();\n  synchronized void publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, boolean);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static org.apache.hadoop.metrics2.impl.MetricsSinkAdapter newSink(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig);\n  static java.lang.String getHostname();\n  public synchronized void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n  public synchronized boolean shutdown();\n  public org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  org.apache.hadoop.metrics2.impl.MetricsSourceAdapter getSourceAdapter(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/RawLocalFileSystem$LocalFSFileOutputStream.class": "Compiled from \"RawLocalFileSystem.java\"\npublic class org.apache.hadoop.fs.RawLocalFileSystem extends org.apache.hadoop.fs.FileSystem {\n  static final java.net.URI NAME;\n  public static void useStatIfAvailable();\n  public org.apache.hadoop.fs.RawLocalFileSystem();\n  public java.io.File pathToFile(org.apache.hadoop.fs.Path);\n  public java.net.URI getUri();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStream(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStreamWithMode(org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean mkOneDir(java.io.File) throws java.io.IOException;\n  protected boolean mkOneDirWithMode(org.apache.hadoop.fs.Path, java.io.File, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableCounter.class": "Compiled from \"MutableCounter.java\"\npublic abstract class org.apache.hadoop.metrics2.lib.MutableCounter extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  protected org.apache.hadoop.metrics2.lib.MutableCounter(org.apache.hadoop.metrics2.MetricsInfo);\n  protected org.apache.hadoop.metrics2.MetricsInfo info();\n  public abstract void incr();\n}\n", 
  "org/apache/hadoop/metrics2/lib/MethodMetric$4.class": "Compiled from \"MethodMetric.java\"\nclass org.apache.hadoop.metrics2.lib.MethodMetric extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  org.apache.hadoop.metrics2.lib.MethodMetric(java.lang.Object, java.lang.reflect.Method, org.apache.hadoop.metrics2.MetricsInfo, org.apache.hadoop.metrics2.annotation.Metric$Type);\n  org.apache.hadoop.metrics2.lib.MutableMetric newCounter(java.lang.Class<?>);\n  static boolean isInt(java.lang.Class<?>);\n  static boolean isLong(java.lang.Class<?>);\n  static boolean isFloat(java.lang.Class<?>);\n  static boolean isDouble(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newGauge(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newTag(java.lang.Class<?>);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  static org.apache.hadoop.metrics2.MetricsInfo metricInfo(java.lang.reflect.Method);\n  static java.lang.String nameFrom(java.lang.reflect.Method);\n  static java.lang.Object access$000(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static java.lang.reflect.Method access$100(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.hadoop.metrics2.MetricsInfo access$200(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.commons.logging.Log access$300();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/TrashPolicyDefault$1.class": "Compiled from \"TrashPolicyDefault.java\"\npublic class org.apache.hadoop.fs.TrashPolicyDefault extends org.apache.hadoop.fs.TrashPolicy {\n  public org.apache.hadoop.fs.TrashPolicyDefault();\n  public void initialize(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public boolean isEnabled();\n  public boolean moveToTrash(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void createCheckpoint() throws java.io.IOException;\n  public void deleteCheckpoint() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getCurrentTrashDir();\n  public java.lang.Runnable getEmptier() throws java.io.IOException;\n  static org.apache.commons.logging.Log access$000();\n  static org.apache.hadoop.fs.Path access$100(org.apache.hadoop.fs.TrashPolicyDefault);\n  org.apache.hadoop.fs.TrashPolicyDefault(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.TrashPolicyDefault$1) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/MetaBlockDoesNotExist.class": "Compiled from \"MetaBlockDoesNotExist.java\"\npublic class org.apache.hadoop.io.file.tfile.MetaBlockDoesNotExist extends java.io.IOException {\n  org.apache.hadoop.io.file.tfile.MetaBlockDoesNotExist(java.lang.String);\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$TFileIndexEntry.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter$1$1.class": "Compiled from \"DelegationTokenAuthenticationFilter.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter extends org.apache.hadoop.security.authentication.server.AuthenticationFilter {\n  public static final java.lang.String DELEGATION_TOKEN_SECRET_MANAGER_ATTR;\n  public static final java.lang.String PROXYUSER_PREFIX;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter();\n  protected java.util.Properties getConfiguration(java.lang.String, javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void setAuthHandlerClass(java.util.Properties) throws javax.servlet.ServletException;\n  protected org.apache.hadoop.conf.Configuration getProxyuserConfiguration(javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  public void init(javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void initializeAuthHandler(java.lang.String, javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void setHandlerAuthMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  static java.lang.String getDoAs(javax.servlet.http.HttpServletRequest);\n  static org.apache.hadoop.security.UserGroupInformation getHttpUserGroupInformationInContext();\n  protected void doFilter(javax.servlet.FilterChain, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n  static org.apache.hadoop.security.SaslRpcServer$AuthMethod access$000(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SortedMapWritable.class": "Compiled from \"SortedMapWritable.java\"\npublic class org.apache.hadoop.io.SortedMapWritable extends org.apache.hadoop.io.AbstractMapWritable implements java.util.SortedMap<org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.Writable> {\n  public org.apache.hadoop.io.SortedMapWritable();\n  public org.apache.hadoop.io.SortedMapWritable(org.apache.hadoop.io.SortedMapWritable);\n  public java.util.Comparator<? super org.apache.hadoop.io.WritableComparable> comparator();\n  public org.apache.hadoop.io.WritableComparable firstKey();\n  public java.util.SortedMap<org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.Writable> headMap(org.apache.hadoop.io.WritableComparable);\n  public org.apache.hadoop.io.WritableComparable lastKey();\n  public java.util.SortedMap<org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.Writable> subMap(org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.WritableComparable);\n  public java.util.SortedMap<org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.Writable> tailMap(org.apache.hadoop.io.WritableComparable);\n  public void clear();\n  public boolean containsKey(java.lang.Object);\n  public boolean containsValue(java.lang.Object);\n  public java.util.Set<java.util.Map$Entry<org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.Writable>> entrySet();\n  public org.apache.hadoop.io.Writable get(java.lang.Object);\n  public boolean isEmpty();\n  public java.util.Set<org.apache.hadoop.io.WritableComparable> keySet();\n  public org.apache.hadoop.io.Writable put(org.apache.hadoop.io.WritableComparable, org.apache.hadoop.io.Writable);\n  public void putAll(java.util.Map<? extends org.apache.hadoop.io.WritableComparable, ? extends org.apache.hadoop.io.Writable>);\n  public org.apache.hadoop.io.Writable remove(java.lang.Object);\n  public int size();\n  public java.util.Collection<org.apache.hadoop.io.Writable> values();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.Object lastKey();\n  public java.lang.Object firstKey();\n  public java.util.SortedMap tailMap(java.lang.Object);\n  public java.util.SortedMap headMap(java.lang.Object);\n  public java.util.SortedMap subMap(java.lang.Object, java.lang.Object);\n  public java.lang.Object remove(java.lang.Object);\n  public java.lang.Object put(java.lang.Object, java.lang.Object);\n  public java.lang.Object get(java.lang.Object);\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/CRC.class": "Compiled from \"CRC.java\"\nfinal class org.apache.hadoop.io.compress.bzip2.CRC {\n  static final int[] crc32Table;\n  int globalCrc;\n  org.apache.hadoop.io.compress.bzip2.CRC();\n  void initialiseCRC();\n  int getFinalCRC();\n  int getGlobalCRC();\n  void setGlobalCRC(int);\n  void updateCRC(int);\n  void updateCRC(int, int);\n  static {};\n}\n", 
  "org/apache/hadoop/record/Utils.class": "Compiled from \"Utils.java\"\npublic class org.apache.hadoop.record.Utils {\n  public static final char[] hexchars;\n  static java.lang.String toXMLString(java.lang.String);\n  static java.lang.String fromXMLString(java.lang.String);\n  static java.lang.String toCSVString(java.lang.String);\n  static java.lang.String fromCSVString(java.lang.String) throws java.io.IOException;\n  static java.lang.String toXMLBuffer(org.apache.hadoop.record.Buffer);\n  static org.apache.hadoop.record.Buffer fromXMLBuffer(java.lang.String) throws java.io.IOException;\n  static java.lang.String toCSVBuffer(org.apache.hadoop.record.Buffer);\n  static org.apache.hadoop.record.Buffer fromCSVBuffer(java.lang.String) throws java.io.IOException;\n  static void toBinaryString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  static boolean isValidCodePoint(int);\n  static java.lang.String fromBinaryString(java.io.DataInput) throws java.io.IOException;\n  public static float readFloat(byte[], int);\n  public static double readDouble(byte[], int);\n  public static long readVLong(byte[], int) throws java.io.IOException;\n  public static int readVInt(byte[], int) throws java.io.IOException;\n  public static long readVLong(java.io.DataInput) throws java.io.IOException;\n  public static int readVInt(java.io.DataInput) throws java.io.IOException;\n  public static int getVIntSize(long);\n  public static void writeVLong(java.io.DataOutput, long) throws java.io.IOException;\n  public static void writeVInt(java.io.DataOutput, int) throws java.io.IOException;\n  public static int compareBytes(byte[], int, int, byte[], int, int);\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer$1.class": "Compiled from \"HttpServer.java\"\npublic class org.apache.hadoop.http.HttpServer implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.Connector listener;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[]) throws java.io.IOException;\n  public org.mortbay.jetty.Connector createBaseListener(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean) throws java.io.IOException;\n  protected void addContext(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public void setThreads(int, int);\n  public void addSslListener(java.net.InetSocketAddress, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void addSslListener(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  protected void initSpnego(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void start() throws java.io.IOException;\n  void openListener() throws java.lang.Exception;\n  public java.net.InetSocketAddress getListenerAddress();\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  static org.apache.hadoop.security.ssl.SSLFactory access$000(org.apache.hadoop.http.HttpServer);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$7.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToActiveRequestProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslRpcServer$AuthMethod.class": "Compiled from \"SaslRpcServer.java\"\npublic class org.apache.hadoop.security.SaslRpcServer {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String SASL_DEFAULT_REALM;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod authMethod;\n  public java.lang.String mechanism;\n  public java.lang.String protocol;\n  public java.lang.String serverId;\n  public org.apache.hadoop.security.SaslRpcServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod) throws java.io.IOException;\n  public javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection, java.util.Map<java.lang.String, ?>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void init(org.apache.hadoop.conf.Configuration);\n  static java.lang.String encodeIdentifier(byte[]);\n  static byte[] decodeIdentifier(java.lang.String);\n  public static <T extends org/apache/hadoop/security/token/TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  static char[] encodePassword(byte[]);\n  public static java.lang.String[] splitKerberosName(java.lang.String);\n  static javax.security.sasl.SaslServerFactory access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/io/DataOutputBuffer.class": "Compiled from \"DataOutputBuffer.java\"\npublic class org.apache.hadoop.io.DataOutputBuffer extends java.io.DataOutputStream {\n  public org.apache.hadoop.io.DataOutputBuffer();\n  public org.apache.hadoop.io.DataOutputBuffer(int);\n  public byte[] getData();\n  public int getLength();\n  public org.apache.hadoop.io.DataOutputBuffer reset();\n  public void write(java.io.DataInput, int) throws java.io.IOException;\n  public void writeTo(java.io.OutputStream) throws java.io.IOException;\n  public void writeInt(int, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocketWatcher$2.class": "Compiled from \"DomainSocketWatcher.java\"\npublic final class org.apache.hadoop.net.unix.DomainSocketWatcher implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final java.lang.Thread watcherThread;\n  static final boolean $assertionsDisabled;\n  public static java.lang.String getLoadingFailureReason();\n  public org.apache.hadoop.net.unix.DomainSocketWatcher(int, java.lang.String) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void add(org.apache.hadoop.net.unix.DomainSocket, org.apache.hadoop.net.unix.DomainSocketWatcher$Handler);\n  public void remove(org.apache.hadoop.net.unix.DomainSocket);\n  public java.lang.String toString();\n  static java.util.concurrent.locks.ReentrantLock access$000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$102(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static boolean access$202(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static int access$300(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static void access$400(org.apache.hadoop.net.unix.DomainSocketWatcher, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet);\n  static void access$500(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static java.util.LinkedList access$600(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.TreeMap access$700(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.concurrent.locks.Condition access$800(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$200(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static int access$900(int, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet) throws java.io.IOException;\n  static void access$1000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$1100(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolInfoService$1.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ProgramDriver.class": "Compiled from \"ProgramDriver.java\"\npublic class org.apache.hadoop.util.ProgramDriver {\n  java.util.Map<java.lang.String, org.apache.hadoop.util.ProgramDriver$ProgramDescription> programs;\n  public org.apache.hadoop.util.ProgramDriver();\n  public void addClass(java.lang.String, java.lang.Class<?>, java.lang.String) throws java.lang.Throwable;\n  public int run(java.lang.String[]) throws java.lang.Throwable;\n  public void driver(java.lang.String[]) throws java.lang.Throwable;\n}\n", 
  "org/apache/hadoop/fs/PathOperationException.class": "Compiled from \"PathOperationException.java\"\npublic class org.apache.hadoop.fs.PathOperationException extends org.apache.hadoop.fs.PathExistsException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.PathOperationException(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/FileUtil.class": "Compiled from \"FileUtil.java\"\npublic class org.apache.hadoop.fs.FileUtil {\n  public static final int SYMLINK_NO_PRIVILEGE;\n  public org.apache.hadoop.fs.FileUtil();\n  public static org.apache.hadoop.fs.Path[] stat2Paths(org.apache.hadoop.fs.FileStatus[]);\n  public static org.apache.hadoop.fs.Path[] stat2Paths(org.apache.hadoop.fs.FileStatus[], org.apache.hadoop.fs.Path);\n  public static boolean fullyDelete(java.io.File);\n  public static boolean fullyDelete(java.io.File, boolean);\n  public static java.lang.String readLink(java.io.File);\n  public static boolean fullyDeleteContents(java.io.File);\n  public static boolean fullyDeleteContents(java.io.File, boolean);\n  public static void fullyDelete(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copyMerge(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public static boolean copy(java.io.File, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.io.File, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.lang.String makeShellPath(java.lang.String) throws java.io.IOException;\n  public static java.lang.String makeShellPath(java.io.File) throws java.io.IOException;\n  public static java.lang.String makeShellPath(java.io.File, boolean) throws java.io.IOException;\n  public static long getDU(java.io.File);\n  public static void unZip(java.io.File, java.io.File) throws java.io.IOException;\n  public static void unTar(java.io.File, java.io.File) throws java.io.IOException;\n  public static int symLink(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static int chmod(java.lang.String, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static int chmod(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public static void setOwner(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static boolean setReadable(java.io.File, boolean);\n  public static boolean setWritable(java.io.File, boolean);\n  public static boolean setExecutable(java.io.File, boolean);\n  public static boolean canRead(java.io.File);\n  public static boolean canWrite(java.io.File);\n  public static boolean canExecute(java.io.File);\n  public static void setPermission(java.io.File, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  static java.lang.String execCommand(java.io.File, java.lang.String...) throws java.io.IOException;\n  public static final java.io.File createLocalTempFile(java.io.File, java.lang.String, boolean) throws java.io.IOException;\n  public static void replaceFile(java.io.File, java.io.File) throws java.io.IOException;\n  public static java.io.File[] listFiles(java.io.File) throws java.io.IOException;\n  public static java.lang.String[] list(java.io.File) throws java.io.IOException;\n  public static java.lang.String[] createJarWithClassPath(java.lang.String, org.apache.hadoop.fs.Path, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public static java.lang.String[] createJarWithClassPath(java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configurable.class": "Compiled from \"Configurable.java\"\npublic interface org.apache.hadoop.conf.Configurable {\n  public abstract void setConf(org.apache.hadoop.conf.Configuration);\n  public abstract org.apache.hadoop.conf.Configuration getConf();\n}\n", 
  "org/apache/hadoop/net/ScriptBasedMapping$RawScriptBasedMapping.class": "Compiled from \"ScriptBasedMapping.java\"\npublic class org.apache.hadoop.net.ScriptBasedMapping extends org.apache.hadoop.net.CachedDNSToSwitchMapping {\n  static final int MIN_ALLOWABLE_ARGS;\n  static final int DEFAULT_ARG_COUNT;\n  static final java.lang.String SCRIPT_FILENAME_KEY;\n  static final java.lang.String SCRIPT_ARG_COUNT_KEY;\n  public static final java.lang.String NO_SCRIPT;\n  public org.apache.hadoop.net.ScriptBasedMapping();\n  public org.apache.hadoop.net.ScriptBasedMapping(org.apache.hadoop.net.DNSToSwitchMapping);\n  public org.apache.hadoop.net.ScriptBasedMapping(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.lang.String toString();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshProtocolService$Interface.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/TrashPolicy.class": "Compiled from \"TrashPolicy.java\"\npublic abstract class org.apache.hadoop.fs.TrashPolicy extends org.apache.hadoop.conf.Configured {\n  protected org.apache.hadoop.fs.FileSystem fs;\n  protected org.apache.hadoop.fs.Path trash;\n  protected long deletionInterval;\n  public org.apache.hadoop.fs.TrashPolicy();\n  public abstract void initialize(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public abstract boolean isEnabled();\n  public abstract boolean moveToTrash(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract void createCheckpoint() throws java.io.IOException;\n  public abstract void deleteCheckpoint() throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.Path getCurrentTrashDir();\n  public abstract java.lang.Runnable getEmptier() throws java.io.IOException;\n  public static org.apache.hadoop.fs.TrashPolicy getInstance(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto$Builder.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto$Builder.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Options$Rename.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/fs/shell/SnapshotCommands$DeleteSnapshot.class": "Compiled from \"SnapshotCommands.java\"\nclass org.apache.hadoop.fs.shell.SnapshotCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.SnapshotCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolVersionProto$1.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/DelegationTokenRenewer$1.class": "Compiled from \"DelegationTokenRenewer.java\"\npublic class org.apache.hadoop.fs.DelegationTokenRenewer extends java.lang.Thread {\n  public static long renewCycle;\n  protected int getRenewQueueLength();\n  public static synchronized org.apache.hadoop.fs.DelegationTokenRenewer getInstance();\n  static synchronized void reset();\n  public <T extends org/apache/hadoop/fs/FileSystem & org/apache/hadoop/fs/DelegationTokenRenewer$Renewable> org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction<T> addRenewAction(T);\n  public <T extends org/apache/hadoop/fs/FileSystem & org/apache/hadoop/fs/DelegationTokenRenewer$Renewable> void removeRenewAction(T) throws java.io.IOException;\n  public void run();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/AbstractMetricsContext$RecordMap.class": "Compiled from \"AbstractMetricsContext.java\"\npublic abstract class org.apache.hadoop.metrics.spi.AbstractMetricsContext implements org.apache.hadoop.metrics.MetricsContext {\n  protected org.apache.hadoop.metrics.spi.AbstractMetricsContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  protected java.lang.String getAttribute(java.lang.String);\n  protected java.util.Map<java.lang.String, java.lang.String> getAttributeTable(java.lang.String);\n  public java.lang.String getContextName();\n  public org.apache.hadoop.metrics.ContextFactory getContextFactory();\n  public synchronized void startMonitoring() throws java.io.IOException;\n  public synchronized void stopMonitoring();\n  public boolean isMonitoring();\n  public synchronized void close();\n  public final synchronized org.apache.hadoop.metrics.MetricsRecord createRecord(java.lang.String);\n  protected org.apache.hadoop.metrics.MetricsRecord newRecord(java.lang.String);\n  public synchronized void registerUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized void unregisterUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized java.util.Map<java.lang.String, java.util.Collection<org.apache.hadoop.metrics.spi.OutputRecord>> getAllRecords();\n  protected abstract void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord) throws java.io.IOException;\n  protected void flush() throws java.io.IOException;\n  protected void update(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  protected void remove(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  public int getPeriod();\n  protected void setPeriod(int);\n  protected void parseAndSetPeriod(java.lang.String);\n  static void access$000(org.apache.hadoop.metrics.spi.AbstractMetricsContext) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.class": "Compiled from \"DelegationTokenManager.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenManager {\n  public static final java.lang.String ENABLE_ZK_KEY;\n  public static final java.lang.String PREFIX;\n  public static final java.lang.String UPDATE_INTERVAL;\n  public static final long UPDATE_INTERVAL_DEFAULT;\n  public static final java.lang.String MAX_LIFETIME;\n  public static final long MAX_LIFETIME_DEFAULT;\n  public static final java.lang.String RENEW_INTERVAL;\n  public static final long RENEW_INTERVAL_DEFAULT;\n  public static final java.lang.String REMOVAL_SCAN_INTERVAL;\n  public static final long REMOVAL_SCAN_INTERVAL_DEFAULT;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.Text);\n  public void setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  public void init();\n  public void destroy();\n  public org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> createToken(org.apache.hadoop.security.UserGroupInformation, java.lang.String);\n  public long renewToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public void cancelToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.UserGroupInformation verifyToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier>) throws java.io.IOException;\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager getDelegationTokenSecretManager();\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier access$000(org.apache.hadoop.security.token.Token, org.apache.hadoop.io.Text) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserMappingsProtocolService.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsConfig$1.class": "Compiled from \"MetricsConfig.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsConfig extends org.apache.commons.configuration.SubsetConfiguration {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String DEFAULT_FILE_NAME;\n  static final java.lang.String PREFIX_DEFAULT;\n  static final java.lang.String PERIOD_KEY;\n  static final int PERIOD_DEFAULT;\n  static final java.lang.String QUEUE_CAPACITY_KEY;\n  static final int QUEUE_CAPACITY_DEFAULT;\n  static final java.lang.String RETRY_DELAY_KEY;\n  static final int RETRY_DELAY_DEFAULT;\n  static final java.lang.String RETRY_BACKOFF_KEY;\n  static final int RETRY_BACKOFF_DEFAULT;\n  static final java.lang.String RETRY_COUNT_KEY;\n  static final int RETRY_COUNT_DEFAULT;\n  static final java.lang.String JMX_CACHE_TTL_KEY;\n  static final java.lang.String START_MBEANS_KEY;\n  static final java.lang.String PLUGIN_URLS_KEY;\n  static final java.lang.String CONTEXT_KEY;\n  static final java.lang.String NAME_KEY;\n  static final java.lang.String DESC_KEY;\n  static final java.lang.String SOURCE_KEY;\n  static final java.lang.String SINK_KEY;\n  static final java.lang.String METRIC_FILTER_KEY;\n  static final java.lang.String RECORD_FILTER_KEY;\n  static final java.lang.String SOURCE_FILTER_KEY;\n  static final java.util.regex.Pattern INSTANCE_REGEX;\n  static final com.google.common.base.Splitter SPLITTER;\n  org.apache.hadoop.metrics2.impl.MetricsConfig(org.apache.commons.configuration.Configuration, java.lang.String);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig create(java.lang.String);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig create(java.lang.String, java.lang.String...);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig loadFirst(java.lang.String, java.lang.String...);\n  public org.apache.hadoop.metrics2.impl.MetricsConfig subset(java.lang.String);\n  java.util.Map<java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig> getInstanceConfigs(java.lang.String);\n  java.lang.Iterable<java.lang.String> keys();\n  public java.lang.Object getProperty(java.lang.String);\n  <T extends org/apache/hadoop/metrics2/MetricsPlugin> T getPlugin(java.lang.String);\n  java.lang.String getClassName(java.lang.String);\n  java.lang.ClassLoader getPluginLoader();\n  public void clear();\n  org.apache.hadoop.metrics2.MetricsFilter getFilter(java.lang.String);\n  public java.lang.String toString();\n  static java.lang.String toString(org.apache.commons.configuration.Configuration);\n  public org.apache.commons.configuration.Configuration subset(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/ByteWritable$Comparator.class": "Compiled from \"ByteWritable.java\"\npublic class org.apache.hadoop.io.ByteWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.ByteWritable> {\n  public org.apache.hadoop.io.ByteWritable();\n  public org.apache.hadoop.io.ByteWritable(byte);\n  public void set(byte);\n  public byte get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.ByteWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController$ServiceStateCallBacks.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FSDataOutputStream.class": "Compiled from \"FSDataOutputStream.java\"\npublic class org.apache.hadoop.fs.FSDataOutputStream extends java.io.DataOutputStream implements org.apache.hadoop.fs.Syncable,org.apache.hadoop.fs.CanSetDropBehind {\n  public org.apache.hadoop.fs.FSDataOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream(java.io.OutputStream, org.apache.hadoop.fs.FileSystem$Statistics) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream(java.io.OutputStream, org.apache.hadoop.fs.FileSystem$Statistics, long) throws java.io.IOException;\n  public long getPos() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public java.io.OutputStream getWrappedStream();\n  public void sync() throws java.io.IOException;\n  public void hflush() throws java.io.IOException;\n  public void hsync() throws java.io.IOException;\n  public void setDropBehind(java.lang.Boolean) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/CompositeGroupsMapping.class": "Compiled from \"CompositeGroupsMapping.java\"\npublic class org.apache.hadoop.security.CompositeGroupsMapping implements org.apache.hadoop.security.GroupMappingServiceProvider,org.apache.hadoop.conf.Configurable {\n  public static final java.lang.String MAPPING_PROVIDERS_CONFIG_KEY;\n  public static final java.lang.String MAPPING_PROVIDERS_COMBINED_CONFIG_KEY;\n  public static final java.lang.String MAPPING_PROVIDER_CONFIG_PREFIX;\n  public org.apache.hadoop.security.CompositeGroupsMapping();\n  public synchronized java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void cacheGroupsRefresh() throws java.io.IOException;\n  public void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n  public synchronized org.apache.hadoop.conf.Configuration getConf();\n  public synchronized void setConf(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/UserProvider.class": "Compiled from \"UserProvider.java\"\npublic class org.apache.hadoop.crypto.key.UserProvider extends org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public boolean isTransient();\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public synchronized void deleteKey(java.lang.String) throws java.io.IOException;\n  public synchronized org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public java.lang.String toString();\n  public synchronized void flush();\n  public synchronized java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public synchronized java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.crypto.key.UserProvider(org.apache.hadoop.conf.Configuration, org.apache.hadoop.crypto.key.UserProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$StatisticsAggregator.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/FailoverFailedException.class": "Compiled from \"FailoverFailedException.java\"\npublic class org.apache.hadoop.ha.FailoverFailedException extends java.lang.Exception {\n  public org.apache.hadoop.ha.FailoverFailedException(java.lang.String);\n  public org.apache.hadoop.ha.FailoverFailedException(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/fs/FsUrlStreamHandlerFactory.class": "Compiled from \"FsUrlStreamHandlerFactory.java\"\npublic class org.apache.hadoop.fs.FsUrlStreamHandlerFactory implements java.net.URLStreamHandlerFactory {\n  public org.apache.hadoop.fs.FsUrlStreamHandlerFactory();\n  public org.apache.hadoop.fs.FsUrlStreamHandlerFactory(org.apache.hadoop.conf.Configuration);\n  public java.net.URLStreamHandler createURLStreamHandler(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/InvalidPathException.class": "Compiled from \"InvalidPathException.java\"\npublic class org.apache.hadoop.fs.InvalidPathException extends org.apache.hadoop.HadoopIllegalArgumentException {\n  public org.apache.hadoop.fs.InvalidPathException(java.lang.String);\n  public org.apache.hadoop.fs.InvalidPathException(java.lang.String, java.lang.String);\n}\n", 
  "org/apache/hadoop/util/Shell$CommandExecutor.class": "Compiled from \"Shell.java\"\npublic abstract class org.apache.hadoop.util.Shell {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int WINDOWS_MAX_SHELL_LENGHT;\n  public static final java.lang.String USER_NAME_COMMAND;\n  public static final java.lang.Object WindowsProcessLaunchLock;\n  public static final org.apache.hadoop.util.Shell$OSType osType;\n  public static final boolean WINDOWS;\n  public static final boolean SOLARIS;\n  public static final boolean MAC;\n  public static final boolean FREEBSD;\n  public static final boolean LINUX;\n  public static final boolean OTHER;\n  public static final boolean PPC_64;\n  public static final java.lang.String SET_PERMISSION_COMMAND;\n  public static final java.lang.String SET_OWNER_COMMAND;\n  public static final java.lang.String SET_GROUP_COMMAND;\n  public static final java.lang.String LINK_COMMAND;\n  public static final java.lang.String READ_LINK_COMMAND;\n  protected long timeOutInterval;\n  public static final java.lang.String WINUTILS;\n  public static final boolean isSetsidAvailable;\n  public static final java.lang.String TOKEN_SEPARATOR_REGEX;\n  public static boolean isJava7OrAbove();\n  public static void checkWindowsCommandLineLength(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String[] getGroupsCommand();\n  public static java.lang.String[] getGroupsForUserCommand(java.lang.String);\n  public static java.lang.String[] getUsersForNetgroupCommand(java.lang.String);\n  public static java.lang.String[] getGetPermissionCommand();\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean);\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean, java.lang.String);\n  public static java.lang.String[] getSetOwnerCommand(java.lang.String);\n  public static java.lang.String[] getSymlinkCommand(java.lang.String, java.lang.String);\n  public static java.lang.String[] getReadlinkCommand(java.lang.String);\n  public static java.lang.String[] getCheckProcessIsAliveCommand(java.lang.String);\n  public static java.lang.String[] getSignalKillCommand(int, java.lang.String);\n  public static java.lang.String getEnvironmentVariableRegex();\n  public static java.io.File appendScriptExtension(java.io.File, java.lang.String);\n  public static java.lang.String appendScriptExtension(java.lang.String);\n  public static java.lang.String[] getRunScriptCommand(java.io.File);\n  public static final java.lang.String getHadoopHome() throws java.io.IOException;\n  public static final java.lang.String getQualifiedBinPath(java.lang.String) throws java.io.IOException;\n  public static final java.lang.String getWinUtilsPath();\n  public org.apache.hadoop.util.Shell();\n  public org.apache.hadoop.util.Shell(long);\n  public org.apache.hadoop.util.Shell(long, boolean);\n  protected void setEnvironment(java.util.Map<java.lang.String, java.lang.String>);\n  protected void setWorkingDirectory(java.io.File);\n  protected void run() throws java.io.IOException;\n  protected abstract java.lang.String[] getExecString();\n  protected abstract void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public java.lang.String getEnvironment(java.lang.String);\n  public java.lang.Process getProcess();\n  public int getExitCode();\n  public boolean isTimedOut();\n  public static java.lang.String execCommand(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String[], long) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String...) throws java.io.IOException;\n  static java.util.concurrent.atomic.AtomicBoolean access$000(org.apache.hadoop.util.Shell);\n  static void access$100(org.apache.hadoop.util.Shell);\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/avro/AvroReflectSerializable.class": "Compiled from \"AvroReflectSerializable.java\"\npublic interface org.apache.hadoop.io.serializer.avro.AvroReflectSerializable {\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProtoOrBuilder.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation$1.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshAuthorizationPolicyProtocolService.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocketWatcher$FdSet.class": "Compiled from \"DomainSocketWatcher.java\"\npublic final class org.apache.hadoop.net.unix.DomainSocketWatcher implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final java.lang.Thread watcherThread;\n  static final boolean $assertionsDisabled;\n  public static java.lang.String getLoadingFailureReason();\n  public org.apache.hadoop.net.unix.DomainSocketWatcher(int, java.lang.String) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void add(org.apache.hadoop.net.unix.DomainSocket, org.apache.hadoop.net.unix.DomainSocketWatcher$Handler);\n  public void remove(org.apache.hadoop.net.unix.DomainSocket);\n  public java.lang.String toString();\n  static java.util.concurrent.locks.ReentrantLock access$000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$102(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static boolean access$202(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static int access$300(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static void access$400(org.apache.hadoop.net.unix.DomainSocketWatcher, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet);\n  static void access$500(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static java.util.LinkedList access$600(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.TreeMap access$700(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.concurrent.locks.Condition access$800(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$200(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static int access$900(int, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet) throws java.io.IOException;\n  static void access$1000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$1100(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/And.class": "Compiled from \"And.java\"\nfinal class org.apache.hadoop.fs.shell.find.And extends org.apache.hadoop.fs.shell.find.BaseExpression {\n  public static void registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.And();\n  public org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  public boolean isOperator();\n  public int getPrecedence();\n  public void addChildren(java.util.Deque<org.apache.hadoop.fs.shell.find.Expression>);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$30.class": "", 
  "org/apache/hadoop/io/SequenceFile$Sorter$SortPass$SeqFileComparator.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$BlockRegion.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JString$JavaString.class": "Compiled from \"JString.java\"\npublic class org.apache.hadoop.record.compiler.JString extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JString();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserMappingsProtocolService$BlockingInterface.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/CredentialShell$DeleteCommand.class": "Compiled from \"CredentialShell.java\"\npublic class org.apache.hadoop.security.alias.CredentialShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.security.alias.CredentialShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int init(java.lang.String[]) throws java.io.IOException;\n  protected char[] promptForCredential() throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialShell$PasswordReader getPasswordReader();\n  public void setPasswordReader(org.apache.hadoop.security.alias.CredentialShell$PasswordReader);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.security.alias.CredentialShell);\n  static boolean access$300(org.apache.hadoop.security.alias.CredentialShell);\n  static java.lang.String access$400(org.apache.hadoop.security.alias.CredentialShell);\n}\n", 
  "org/apache/hadoop/io/file/tfile/CompareUtils$BytesComparator.class": "Compiled from \"CompareUtils.java\"\nclass org.apache.hadoop.io.file.tfile.CompareUtils {\n}\n", 
  "org/apache/hadoop/ha/NodeFencer$FenceMethodWithArg.class": "Compiled from \"NodeFencer.java\"\npublic class org.apache.hadoop.ha.NodeFencer {\n  org.apache.hadoop.ha.NodeFencer(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public static org.apache.hadoop.ha.NodeFencer create(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public boolean fence(org.apache.hadoop.ha.HAServiceTarget);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ListSpanReceiversRequestProto.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Client$ClientExecutorServiceFactory.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/util/LightWeightCache.class": "Compiled from \"LightWeightCache.java\"\npublic class org.apache.hadoop.util.LightWeightCache<K, E extends K> extends org.apache.hadoop.util.LightWeightGSet<K, E> {\n  public org.apache.hadoop.util.LightWeightCache(int, int, long, long);\n  org.apache.hadoop.util.LightWeightCache(int, int, long, long, org.apache.hadoop.util.LightWeightCache$Clock);\n  void setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  boolean isExpired(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  public E get(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto$Builder.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto$1.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/InodeTree$MountPoint.class": "Compiled from \"InodeTree.java\"\nabstract class org.apache.hadoop.fs.viewfs.InodeTree<T> {\n  static final org.apache.hadoop.fs.Path SlashPath;\n  final org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T> root;\n  final java.lang.String homedirPrefix;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> mountPoints;\n  static final boolean $assertionsDisabled;\n  static java.lang.String[] breakIntoPathComponents(java.lang.String);\n  protected abstract T getTargetFileSystem(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, java.io.IOException;\n  protected abstract T getTargetFileSystem(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T>) throws java.net.URISyntaxException;\n  protected abstract T getTargetFileSystem(java.net.URI[]) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException;\n  protected org.apache.hadoop.fs.viewfs.InodeTree(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.IOException;\n  org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult<T> resolve(java.lang.String, boolean) throws java.io.FileNotFoundException;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> getMountPoints();\n  java.lang.String getHomeDirPrefixValue();\n  static {};\n}\n", 
  "org/apache/hadoop/http/FilterContainer.class": "Compiled from \"FilterContainer.java\"\npublic interface org.apache.hadoop.http.FilterContainer {\n  public abstract void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public abstract void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n}\n", 
  "org/apache/hadoop/fs/LocalDirAllocator$AllocatorPerContext.class": "Compiled from \"LocalDirAllocator.java\"\npublic class org.apache.hadoop.fs.LocalDirAllocator {\n  public static final int SIZE_UNKNOWN;\n  public org.apache.hadoop.fs.LocalDirAllocator(java.lang.String);\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLocalPathToRead(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.Iterable<org.apache.hadoop.fs.Path> getAllLocalPathsToRead(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.io.File createTmpFileForWrite(java.lang.String, long, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean isContextValid(java.lang.String);\n  public static void removeContext(java.lang.String);\n  public boolean ifExists(java.lang.String, org.apache.hadoop.conf.Configuration);\n  int getCurrentDirectoryIndex();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyShell$1.class": "Compiled from \"KeyShell.java\"\npublic class org.apache.hadoop.crypto.key.KeyShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.crypto.key.KeyShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.crypto.key.KeyShell);\n  static boolean access$300(org.apache.hadoop.crypto.key.KeyShell);\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$RemoveSpanReceiverRequestProto.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToStandbyRequestProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Shell$1.class": "Compiled from \"Shell.java\"\npublic abstract class org.apache.hadoop.util.Shell {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int WINDOWS_MAX_SHELL_LENGHT;\n  public static final java.lang.String USER_NAME_COMMAND;\n  public static final java.lang.Object WindowsProcessLaunchLock;\n  public static final org.apache.hadoop.util.Shell$OSType osType;\n  public static final boolean WINDOWS;\n  public static final boolean SOLARIS;\n  public static final boolean MAC;\n  public static final boolean FREEBSD;\n  public static final boolean LINUX;\n  public static final boolean OTHER;\n  public static final boolean PPC_64;\n  public static final java.lang.String SET_PERMISSION_COMMAND;\n  public static final java.lang.String SET_OWNER_COMMAND;\n  public static final java.lang.String SET_GROUP_COMMAND;\n  public static final java.lang.String LINK_COMMAND;\n  public static final java.lang.String READ_LINK_COMMAND;\n  protected long timeOutInterval;\n  public static final java.lang.String WINUTILS;\n  public static final boolean isSetsidAvailable;\n  public static final java.lang.String TOKEN_SEPARATOR_REGEX;\n  public static boolean isJava7OrAbove();\n  public static void checkWindowsCommandLineLength(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String[] getGroupsCommand();\n  public static java.lang.String[] getGroupsForUserCommand(java.lang.String);\n  public static java.lang.String[] getUsersForNetgroupCommand(java.lang.String);\n  public static java.lang.String[] getGetPermissionCommand();\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean);\n  public static java.lang.String[] getSetPermissionCommand(java.lang.String, boolean, java.lang.String);\n  public static java.lang.String[] getSetOwnerCommand(java.lang.String);\n  public static java.lang.String[] getSymlinkCommand(java.lang.String, java.lang.String);\n  public static java.lang.String[] getReadlinkCommand(java.lang.String);\n  public static java.lang.String[] getCheckProcessIsAliveCommand(java.lang.String);\n  public static java.lang.String[] getSignalKillCommand(int, java.lang.String);\n  public static java.lang.String getEnvironmentVariableRegex();\n  public static java.io.File appendScriptExtension(java.io.File, java.lang.String);\n  public static java.lang.String appendScriptExtension(java.lang.String);\n  public static java.lang.String[] getRunScriptCommand(java.io.File);\n  public static final java.lang.String getHadoopHome() throws java.io.IOException;\n  public static final java.lang.String getQualifiedBinPath(java.lang.String) throws java.io.IOException;\n  public static final java.lang.String getWinUtilsPath();\n  public org.apache.hadoop.util.Shell();\n  public org.apache.hadoop.util.Shell(long);\n  public org.apache.hadoop.util.Shell(long, boolean);\n  protected void setEnvironment(java.util.Map<java.lang.String, java.lang.String>);\n  protected void setWorkingDirectory(java.io.File);\n  protected void run() throws java.io.IOException;\n  protected abstract java.lang.String[] getExecString();\n  protected abstract void parseExecResult(java.io.BufferedReader) throws java.io.IOException;\n  public java.lang.String getEnvironment(java.lang.String);\n  public java.lang.Process getProcess();\n  public int getExitCode();\n  public boolean isTimedOut();\n  public static java.lang.String execCommand(java.lang.String...) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String[], long) throws java.io.IOException;\n  public static java.lang.String execCommand(java.util.Map<java.lang.String, java.lang.String>, java.lang.String...) throws java.io.IOException;\n  static java.util.concurrent.atomic.AtomicBoolean access$000(org.apache.hadoop.util.Shell);\n  static void access$100(org.apache.hadoop.util.Shell);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RetryCache$CacheEntryWithPayload.class": "Compiled from \"RetryCache.java\"\npublic class org.apache.hadoop.ipc.RetryCache {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.RetryCache(java.lang.String, double, long);\n  public void lock();\n  public void unlock();\n  public org.apache.hadoop.util.LightWeightGSet<org.apache.hadoop.ipc.RetryCache$CacheEntry, org.apache.hadoop.ipc.RetryCache$CacheEntry> getCacheSet();\n  public org.apache.hadoop.ipc.metrics.RetryCacheMetrics getMetricsForTests();\n  public java.lang.String getCacheName();\n  public void addCacheEntry(byte[], int);\n  public void addCacheEntryWithPayload(byte[], int, java.lang.Object);\n  public static org.apache.hadoop.ipc.RetryCache$CacheEntry waitForCompletion(org.apache.hadoop.ipc.RetryCache);\n  public static org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload waitForCompletion(org.apache.hadoop.ipc.RetryCache, java.lang.Object);\n  public static void setState(org.apache.hadoop.ipc.RetryCache$CacheEntry, boolean);\n  public static void setState(org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload, boolean, java.lang.Object);\n  public static void clear(org.apache.hadoop.ipc.RetryCache);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/annotation/Metric$Type.class": "Compiled from \"Metric.java\"\npublic interface org.apache.hadoop.metrics2.annotation.Metric extends java.lang.annotation.Annotation {\n  public abstract java.lang.String[] value();\n  public abstract java.lang.String about();\n  public abstract java.lang.String sampleName();\n  public abstract java.lang.String valueName();\n  public abstract boolean always();\n  public abstract org.apache.hadoop.metrics2.annotation.Metric$Type type();\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$7.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshProtocolService.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SecurityUtil$QualifiedHostResolver.class": "Compiled from \"SecurityUtil.java\"\npublic class org.apache.hadoop.security.SecurityUtil {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String HOSTNAME_PATTERN;\n  public static final java.lang.String FAILED_TO_GET_UGI_MSG_HEADER;\n  static boolean useIpForTokenService;\n  static org.apache.hadoop.security.SecurityUtil$HostResolver hostResolver;\n  public org.apache.hadoop.security.SecurityUtil();\n  public static void setTokenServiceUseIp(boolean);\n  static boolean isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal);\n  protected static boolean isOriginalTGT(javax.security.auth.kerberos.KerberosTicket);\n  public static java.lang.String getServerPrincipal(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String getServerPrincipal(java.lang.String, java.net.InetAddress) throws java.io.IOException;\n  static java.lang.String getLocalHostName() throws java.net.UnknownHostException;\n  public static void login(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void login(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String buildDTServiceName(java.net.URI, int);\n  public static java.lang.String getHostFromPrincipal(java.lang.String);\n  public static void setSecurityInfoProviders(org.apache.hadoop.security.SecurityInfo...);\n  public static org.apache.hadoop.security.KerberosInfo getKerberosInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.security.token.TokenInfo getTokenInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static java.net.InetSocketAddress getTokenServiceAddr(org.apache.hadoop.security.token.Token<?>);\n  public static void setTokenService(org.apache.hadoop.security.token.Token<?>, java.net.InetSocketAddress);\n  public static org.apache.hadoop.io.Text buildTokenService(java.net.InetSocketAddress);\n  public static org.apache.hadoop.io.Text buildTokenService(java.net.URI);\n  public static <T extends java/lang/Object> T doAsLoginUserOrFatal(java.security.PrivilegedAction<T>);\n  public static <T extends java/lang/Object> T doAsLoginUser(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException;\n  public static <T extends java/lang/Object> T doAsCurrentUser(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException;\n  public static java.net.InetAddress getByName(java.lang.String) throws java.net.UnknownHostException;\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod(org.apache.hadoop.conf.Configuration);\n  public static void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod, org.apache.hadoop.conf.Configuration);\n  public static boolean isPrivilegedPort(int);\n  static {};\n}\n", 
  "org/apache/hadoop/security/RefreshUserMappingsProtocol.class": "Compiled from \"RefreshUserMappingsProtocol.java\"\npublic interface org.apache.hadoop.security.RefreshUserMappingsProtocol {\n  public static final long versionID;\n  public abstract void refreshUserToGroupsMappings() throws java.io.IOException;\n  public abstract void refreshSuperUserGroupsConfiguration() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/FastByteComparisons$LexicographicalComparerHolder$PureJavaComparer.class": "Compiled from \"FastByteComparisons.java\"\nabstract class org.apache.hadoop.io.FastByteComparisons {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.io.FastByteComparisons();\n  public static int compareTo(byte[], int, int, byte[], int, int);\n  static org.apache.hadoop.io.FastByteComparisons$Comparer access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JVector$JavaVector.class": "Compiled from \"JVector.java\"\npublic class org.apache.hadoop.record.compiler.JVector extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JVector(org.apache.hadoop.record.compiler.JType);\n  java.lang.String getSignature();\n  static void access$000();\n  static java.lang.String access$100(java.lang.String);\n  static void access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcResponseHeaderProto$Builder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/http/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.http.package-info {\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshProtocolService$BlockingInterface.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProtoOrBuilder.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$29.class": "", 
  "org/apache/hadoop/metrics2/impl/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics2.impl.package-info {\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAStateChangeRequestInfoProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/CryptoStreamUtils.class": "Compiled from \"CryptoStreamUtils.java\"\npublic class org.apache.hadoop.crypto.CryptoStreamUtils {\n  public org.apache.hadoop.crypto.CryptoStreamUtils();\n  public static void freeDB(java.nio.ByteBuffer);\n  public static int getBufferSize(org.apache.hadoop.conf.Configuration);\n  public static void checkCodec(org.apache.hadoop.crypto.CryptoCodec);\n  public static int checkBufferSize(org.apache.hadoop.crypto.CryptoCodec, int);\n  public static long getInputStreamOffset(java.io.InputStream) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetGroupsForUserResponseProtoOrBuilder.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FsShell.class": "Compiled from \"FsShell.java\"\npublic class org.apache.hadoop.fs.FsShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.fs.shell.CommandFactory commandFactory;\n  public org.apache.hadoop.fs.FsShell();\n  public org.apache.hadoop.fs.FsShell(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.fs.FileSystem getFS() throws java.io.IOException;\n  protected org.apache.hadoop.fs.Trash getTrash() throws java.io.IOException;\n  protected void init() throws java.io.IOException;\n  protected void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.Path getCurrentTrashDir() throws java.io.IOException;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public void close() throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  protected static org.apache.hadoop.fs.FsShell newShellInstance();\n  static void access$000(org.apache.hadoop.fs.FsShell, java.io.PrintStream);\n  static void access$100(org.apache.hadoop.fs.FsShell, java.io.PrintStream, java.lang.String);\n  static void access$200(org.apache.hadoop.fs.FsShell, java.io.PrintStream);\n  static void access$300(org.apache.hadoop.fs.FsShell, java.io.PrintStream, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$1.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/FsUsage$Du.class": "Compiled from \"FsUsage.java\"\nclass org.apache.hadoop.fs.shell.FsUsage extends org.apache.hadoop.fs.shell.FsCommand {\n  protected boolean humanReadable;\n  protected org.apache.hadoop.fs.shell.FsUsage$TableBuilder usagesTable;\n  org.apache.hadoop.fs.shell.FsUsage();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected java.lang.String formatSize(long);\n}\n", 
  "org/apache/hadoop/io/DoubleWritable$Comparator.class": "Compiled from \"DoubleWritable.java\"\npublic class org.apache.hadoop.io.DoubleWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.DoubleWritable> {\n  public org.apache.hadoop.io.DoubleWritable();\n  public org.apache.hadoop.io.DoubleWritable(double);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void set(double);\n  public double get();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.DoubleWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Writer$BlockRegister.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocket$DomainOutputStream.class": "Compiled from \"DomainSocket.java\"\npublic class org.apache.hadoop.net.unix.DomainSocket implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final org.apache.hadoop.util.CloseableReferenceCount refCount;\n  final int fd;\n  public static final int SEND_BUFFER_SIZE;\n  public static final int RECEIVE_BUFFER_SIZE;\n  public static final int SEND_TIMEOUT;\n  public static final int RECEIVE_TIMEOUT;\n  static native void validateSocketPathSecurity0(java.lang.String, int) throws java.io.IOException;\n  public static java.lang.String getLoadingFailureReason();\n  public static void disableBindPathValidation();\n  public static java.lang.String getEffectivePath(java.lang.String, int);\n  public static org.apache.hadoop.net.unix.DomainSocket bindAndListen(java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.net.unix.DomainSocket[] socketpair() throws java.io.IOException;\n  public org.apache.hadoop.net.unix.DomainSocket accept() throws java.io.IOException;\n  public static org.apache.hadoop.net.unix.DomainSocket connect(java.lang.String) throws java.io.IOException;\n  public boolean isOpen();\n  public java.lang.String getPath();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainInputStream getInputStream();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream getOutputStream();\n  public org.apache.hadoop.net.unix.DomainSocket$DomainChannel getChannel();\n  public void setAttribute(int, int) throws java.io.IOException;\n  public int getAttribute(int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void shutdown() throws java.io.IOException;\n  public void sendFileDescriptors(java.io.FileDescriptor[], byte[], int, int) throws java.io.IOException;\n  public int receiveFileDescriptors(java.io.FileDescriptor[], byte[], int, int) throws java.io.IOException;\n  public int recvFileInputStreams(java.io.FileInputStream[], byte[], int, int) throws java.io.IOException;\n  public java.lang.String toString();\n  static int access$000(int, byte[], int, int) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.net.unix.DomainSocket, boolean) throws java.nio.channels.ClosedChannelException;\n  static int access$200(int) throws java.io.IOException;\n  static void access$300(int, byte[], int, int) throws java.io.IOException;\n  static int access$400(int, java.nio.ByteBuffer, int, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos$RequestHeaderProto.class": "Compiled from \"ProtobufRpcEngineProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1102(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/ContextFactory.class": "Compiled from \"ContextFactory.java\"\npublic class org.apache.hadoop.metrics.ContextFactory {\n  protected org.apache.hadoop.metrics.ContextFactory();\n  public java.lang.Object getAttribute(java.lang.String);\n  public java.lang.String[] getAttributeNames();\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void removeAttribute(java.lang.String);\n  public synchronized org.apache.hadoop.metrics.MetricsContext getContext(java.lang.String, java.lang.String) throws java.io.IOException, java.lang.ClassNotFoundException, java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public synchronized org.apache.hadoop.metrics.MetricsContext getContext(java.lang.String) throws java.io.IOException, java.lang.ClassNotFoundException, java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public synchronized java.util.Collection<org.apache.hadoop.metrics.MetricsContext> getAllContexts();\n  public static synchronized org.apache.hadoop.metrics.MetricsContext getNullContext(java.lang.String);\n  public static synchronized org.apache.hadoop.metrics.ContextFactory getFactory() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/CachingKeyProvider$CacheExtension.class": "Compiled from \"CachingKeyProvider.java\"\npublic class org.apache.hadoop.crypto.key.CachingKeyProvider extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension> {\n  public org.apache.hadoop.crypto.key.CachingKeyProvider(org.apache.hadoop.crypto.key.KeyProvider, long, long);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/RPC$VersionMismatch.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$28.class": "", 
  "org/apache/hadoop/security/proto/SecurityProtos$CancelDelegationTokenRequestProtoOrBuilder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/SplitCompressionInputStream.class": "Compiled from \"SplitCompressionInputStream.java\"\npublic abstract class org.apache.hadoop.io.compress.SplitCompressionInputStream extends org.apache.hadoop.io.compress.CompressionInputStream {\n  public org.apache.hadoop.io.compress.SplitCompressionInputStream(java.io.InputStream, long, long) throws java.io.IOException;\n  protected void setStart(long);\n  protected void setEnd(long);\n  public long getAdjustedStart();\n  public long getAdjustedEnd();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolVersionsRequestProto$Builder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsTimeVaryingRate$Metrics.class": "Compiled from \"MetricsTimeVaryingRate.java\"\npublic class org.apache.hadoop.metrics.util.MetricsTimeVaryingRate extends org.apache.hadoop.metrics.util.MetricsBase {\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingRate(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry, java.lang.String);\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingRate(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry);\n  public synchronized void inc(int, long);\n  public synchronized void inc(long);\n  public synchronized void pushMetric(org.apache.hadoop.metrics.MetricsRecord);\n  public synchronized int getPreviousIntervalNumOps();\n  public synchronized long getPreviousIntervalAverageTime();\n  public synchronized long getMinTime();\n  public synchronized long getMaxTime();\n  public synchronized void resetMinMax();\n  static {};\n}\n", 
  "org/apache/hadoop/net/DNS.class": "Compiled from \"DNS.java\"\npublic class org.apache.hadoop.net.DNS {\n  public org.apache.hadoop.net.DNS();\n  public static java.lang.String reverseDns(java.net.InetAddress, java.lang.String) throws javax.naming.NamingException;\n  public static java.lang.String[] getIPs(java.lang.String) throws java.net.UnknownHostException;\n  public static java.lang.String[] getIPs(java.lang.String, boolean) throws java.net.UnknownHostException;\n  public static java.lang.String getDefaultIP(java.lang.String) throws java.net.UnknownHostException;\n  public static java.lang.String[] getHosts(java.lang.String, java.lang.String) throws java.net.UnknownHostException;\n  public static java.lang.String[] getHosts(java.lang.String) throws java.net.UnknownHostException;\n  public static java.lang.String getDefaultHost(java.lang.String, java.lang.String) throws java.net.UnknownHostException;\n  public static java.lang.String getDefaultHost(java.lang.String) throws java.net.UnknownHostException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HardLink$1.class": "Compiled from \"HardLink.java\"\npublic class org.apache.hadoop.fs.HardLink {\n  public final org.apache.hadoop.fs.HardLink$LinkStats linkStats;\n  public org.apache.hadoop.fs.HardLink();\n  public static void createHardLink(java.io.File, java.io.File) throws java.io.IOException;\n  public static void createHardLinkMult(java.io.File, java.lang.String[], java.io.File) throws java.io.IOException;\n  public static int getLinkCount(java.io.File) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToActiveResponseProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer.class": "Compiled from \"FastByteComparisons.java\"\nabstract class org.apache.hadoop.io.FastByteComparisons {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.io.FastByteComparisons();\n  public static int compareTo(byte[], int, int, byte[], int, int);\n  static org.apache.hadoop.io.FastByteComparisons$Comparer access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$39.class": "", 
  "org/apache/hadoop/ha/SshFenceByTcpPort$1.class": "Compiled from \"SshFenceByTcpPort.java\"\npublic class org.apache.hadoop.ha.SshFenceByTcpPort extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.ha.FenceMethod {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String CONF_CONNECT_TIMEOUT_KEY;\n  static final java.lang.String CONF_IDENTITIES_KEY;\n  public org.apache.hadoop.ha.SshFenceByTcpPort();\n  public void checkArgs(java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public boolean tryFence(org.apache.hadoop.ha.HAServiceTarget, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HardLink$LinkStats.class": "Compiled from \"HardLink.java\"\npublic class org.apache.hadoop.fs.HardLink {\n  public final org.apache.hadoop.fs.HardLink$LinkStats linkStats;\n  public org.apache.hadoop.fs.HardLink();\n  public static void createHardLink(java.io.File, java.io.File) throws java.io.IOException;\n  public static void createHardLinkMult(java.io.File, java.lang.String[], java.io.File) throws java.io.IOException;\n  public static int getLinkCount(java.io.File) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/Expression.class": "Compiled from \"Expression.java\"\npublic interface org.apache.hadoop.fs.shell.find.Expression {\n  public abstract void setOptions(org.apache.hadoop.fs.shell.find.FindOptions) throws java.io.IOException;\n  public abstract void prepare() throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  public abstract void finish() throws java.io.IOException;\n  public abstract java.lang.String[] getUsage();\n  public abstract java.lang.String[] getHelp();\n  public abstract boolean isAction();\n  public abstract boolean isOperator();\n  public abstract int getPrecedence();\n  public abstract void addChildren(java.util.Deque<org.apache.hadoop.fs.shell.find.Expression>);\n  public abstract void addArguments(java.util.Deque<java.lang.String>);\n}\n", 
  "org/apache/hadoop/fs/shell/FsCommand.class": "Compiled from \"FsCommand.java\"\npublic abstract class org.apache.hadoop.fs.shell.FsCommand extends org.apache.hadoop.fs.shell.Command {\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected org.apache.hadoop.fs.shell.FsCommand();\n  protected org.apache.hadoop.fs.shell.FsCommand(org.apache.hadoop.conf.Configuration);\n  public java.lang.String getCommandName();\n  protected void run(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public int runAll();\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableMetric.class": "Compiled from \"MutableMetric.java\"\npublic abstract class org.apache.hadoop.metrics2.lib.MutableMetric {\n  public org.apache.hadoop.metrics2.lib.MutableMetric();\n  public abstract void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder);\n  protected void setChanged();\n  protected void clearChanged();\n  public boolean changed();\n}\n", 
  "org/apache/hadoop/fs/Seekable.class": "Compiled from \"Seekable.java\"\npublic interface org.apache.hadoop.fs.Seekable {\n  public abstract void seek(long) throws java.io.IOException;\n  public abstract long getPos() throws java.io.IOException;\n  public abstract boolean seekToNewSource(long) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/RPC$Server$VerProtocolImpl.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/security/ShellBasedIdMapping$PassThroughMap.class": "Compiled from \"ShellBasedIdMapping.java\"\npublic class org.apache.hadoop.security.ShellBasedIdMapping implements org.apache.hadoop.security.IdMappingServiceProvider {\n  static final java.lang.String GET_ALL_USERS_CMD;\n  static final java.lang.String GET_ALL_GROUPS_CMD;\n  static final java.lang.String MAC_GET_ALL_USERS_CMD;\n  static final java.lang.String MAC_GET_ALL_GROUPS_CMD;\n  public org.apache.hadoop.security.ShellBasedIdMapping(org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.security.ShellBasedIdMapping(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public long getTimeout();\n  public com.google.common.collect.BiMap<java.lang.Integer, java.lang.String> getUidNameMap();\n  public com.google.common.collect.BiMap<java.lang.Integer, java.lang.String> getGidNameMap();\n  public synchronized void clearNameMaps();\n  public static boolean updateMapInternal(com.google.common.collect.BiMap<java.lang.Integer, java.lang.String>, java.lang.String, java.lang.String, java.lang.String, java.util.Map<java.lang.Integer, java.lang.Integer>) throws java.io.IOException;\n  public synchronized void updateMaps() throws java.io.IOException;\n  static org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping parseStaticMap(java.io.File) throws java.io.IOException;\n  public synchronized int getUid(java.lang.String) throws java.io.IOException;\n  public synchronized int getGid(java.lang.String) throws java.io.IOException;\n  public synchronized java.lang.String getUserName(int, java.lang.String);\n  public synchronized java.lang.String getGroupName(int, java.lang.String);\n  public int getUidAllowingUnknown(java.lang.String);\n  public int getGidAllowingUnknown(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/Bzip2Factory.class": "Compiled from \"Bzip2Factory.java\"\npublic class org.apache.hadoop.io.compress.bzip2.Bzip2Factory {\n  public org.apache.hadoop.io.compress.bzip2.Bzip2Factory();\n  public static boolean isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration);\n  public static java.lang.String getLibraryName(org.apache.hadoop.conf.Configuration);\n  public static java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getBzip2CompressorType(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.io.compress.Compressor getBzip2Compressor(org.apache.hadoop.conf.Configuration);\n  public static java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getBzip2DecompressorType(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.io.compress.Decompressor getBzip2Decompressor(org.apache.hadoop.conf.Configuration);\n  public static void setBlockSize(org.apache.hadoop.conf.Configuration, int);\n  public static int getBlockSize(org.apache.hadoop.conf.Configuration);\n  public static void setWorkFactor(org.apache.hadoop.conf.Configuration, int);\n  public static int getWorkFactor(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$BlockSizeOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JDouble$CppDouble.class": "Compiled from \"JDouble.java\"\npublic class org.apache.hadoop.record.compiler.JDouble extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JDouble();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/FileSystem$5.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsVisitor.class": "Compiled from \"MetricsVisitor.java\"\npublic interface org.apache.hadoop.metrics2.MetricsVisitor {\n  public abstract void gauge(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public abstract void gauge(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public abstract void gauge(org.apache.hadoop.metrics2.MetricsInfo, float);\n  public abstract void gauge(org.apache.hadoop.metrics2.MetricsInfo, double);\n  public abstract void counter(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public abstract void counter(org.apache.hadoop.metrics2.MetricsInfo, long);\n}\n", 
  "org/apache/hadoop/fs/FSDataInputStream.class": "Compiled from \"FSDataInputStream.java\"\npublic class org.apache.hadoop.fs.FSDataInputStream extends java.io.DataInputStream implements org.apache.hadoop.fs.Seekable,org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.ByteBufferReadable,org.apache.hadoop.fs.HasFileDescriptor,org.apache.hadoop.fs.CanSetDropBehind,org.apache.hadoop.fs.CanSetReadahead,org.apache.hadoop.fs.HasEnhancedByteBufferAccess,org.apache.hadoop.fs.CanUnbuffer {\n  public org.apache.hadoop.fs.FSDataInputStream(java.io.InputStream);\n  public void seek(long) throws java.io.IOException;\n  public long getPos() throws java.io.IOException;\n  public int read(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[]) throws java.io.IOException;\n  public boolean seekToNewSource(long) throws java.io.IOException;\n  public java.io.InputStream getWrappedStream();\n  public int read(java.nio.ByteBuffer) throws java.io.IOException;\n  public java.io.FileDescriptor getFileDescriptor() throws java.io.IOException;\n  public void setReadahead(java.lang.Long) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public void setDropBehind(java.lang.Boolean) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public java.nio.ByteBuffer read(org.apache.hadoop.io.ByteBufferPool, int, java.util.EnumSet<org.apache.hadoop.fs.ReadOption>) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public final java.nio.ByteBuffer read(org.apache.hadoop.io.ByteBufferPool, int) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public void releaseBuffer(java.nio.ByteBuffer);\n  public void unbuffer();\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/TokenRenewer.class": "Compiled from \"TokenRenewer.java\"\npublic abstract class org.apache.hadoop.security.token.TokenRenewer {\n  public org.apache.hadoop.security.token.TokenRenewer();\n  public abstract boolean handleKind(org.apache.hadoop.io.Text);\n  public abstract boolean isManaged(org.apache.hadoop.security.token.Token<?>) throws java.io.IOException;\n  public abstract long renew(org.apache.hadoop.security.token.Token<?>, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException;\n  public abstract void cancel(org.apache.hadoop.security.token.Token<?>, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.InterruptedException;\n}\n", 
  "org/apache/hadoop/security/ssl/SslSocketConnectorSecure.class": "Compiled from \"SslSocketConnectorSecure.java\"\npublic class org.apache.hadoop.security.ssl.SslSocketConnectorSecure extends org.mortbay.jetty.security.SslSocketConnector {\n  public org.apache.hadoop.security.ssl.SslSocketConnectorSecure();\n  protected java.net.ServerSocket newServerSocket(java.lang.String, int, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/shell/Delete$Rm.class": "Compiled from \"Delete.java\"\nclass org.apache.hadoop.fs.shell.Delete {\n  org.apache.hadoop.fs.shell.Delete();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/metrics/Updater.class": "Compiled from \"Updater.java\"\npublic interface org.apache.hadoop.metrics.Updater {\n  public abstract void doUpdates(org.apache.hadoop.metrics.MetricsContext);\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$1.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/util/IdentityHashStore.class": "Compiled from \"IdentityHashStore.java\"\npublic final class org.apache.hadoop.util.IdentityHashStore<K, V> {\n  public org.apache.hadoop.util.IdentityHashStore(int);\n  public void put(K, V);\n  public V get(K);\n  public V remove(K);\n  public boolean isEmpty();\n  public int numElements();\n  public int capacity();\n  public void visitAll(org.apache.hadoop.util.IdentityHashStore$Visitor<K, V>);\n}\n", 
  "org/apache/hadoop/fs/shell/FsUsage$TableBuilder.class": "Compiled from \"FsUsage.java\"\nclass org.apache.hadoop.fs.shell.FsUsage extends org.apache.hadoop.fs.shell.FsCommand {\n  protected boolean humanReadable;\n  protected org.apache.hadoop.fs.shell.FsUsage$TableBuilder usagesTable;\n  org.apache.hadoop.fs.shell.FsUsage();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected java.lang.String formatSize(long);\n}\n", 
  "org/apache/hadoop/fs/ChecksumFs$ChecksumFSOutputSummer.class": "Compiled from \"ChecksumFs.java\"\npublic abstract class org.apache.hadoop.fs.ChecksumFs extends org.apache.hadoop.fs.FilterFs {\n  public static double getApproxChkSumLength(long);\n  public org.apache.hadoop.fs.ChecksumFs(org.apache.hadoop.fs.AbstractFileSystem) throws java.io.IOException, java.net.URISyntaxException;\n  public void setVerifyChecksum(boolean);\n  public org.apache.hadoop.fs.AbstractFileSystem getRawFs();\n  public org.apache.hadoop.fs.Path getChecksumFile(org.apache.hadoop.fs.Path);\n  public static boolean isChecksumFile(org.apache.hadoop.fs.Path);\n  public long getChecksumFileLength(org.apache.hadoop.fs.Path, long);\n  public int getBytesPerSum();\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public static long getChecksumLength(long, int);\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  static int access$000(org.apache.hadoop.fs.ChecksumFs, int, int) throws java.io.IOException;\n  static byte[] access$100();\n  static boolean access$200(org.apache.hadoop.fs.ChecksumFs);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto$SaslAuth.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ByteBufferReadable.class": "Compiled from \"ByteBufferReadable.java\"\npublic interface org.apache.hadoop.fs.ByteBufferReadable {\n  public abstract int read(java.nio.ByteBuffer) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/AvroFSInput.class": "Compiled from \"AvroFSInput.java\"\npublic class org.apache.hadoop.fs.AvroFSInput implements java.io.Closeable,org.apache.avro.file.SeekableInput {\n  public org.apache.hadoop.fs.AvroFSInput(org.apache.hadoop.fs.FSDataInputStream, long);\n  public org.apache.hadoop.fs.AvroFSInput(org.apache.hadoop.fs.FileContext, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long length();\n  public int read(byte[], int, int) throws java.io.IOException;\n  public void seek(long) throws java.io.IOException;\n  public long tell() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/retry/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.io.retry.package-info {\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Cache$Key.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$GetServiceStatusResponseProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/generated/RccTokenManager.class": "Compiled from \"RccTokenManager.java\"\npublic class org.apache.hadoop.record.compiler.generated.RccTokenManager implements org.apache.hadoop.record.compiler.generated.RccConstants {\n  public java.io.PrintStream debugStream;\n  static final long[] jjbitVec0;\n  static final int[] jjnextStates;\n  public static final java.lang.String[] jjstrLiteralImages;\n  public static final java.lang.String[] lexStateNames;\n  public static final int[] jjnewLexState;\n  static final long[] jjtoToken;\n  static final long[] jjtoSkip;\n  static final long[] jjtoSpecial;\n  static final long[] jjtoMore;\n  protected org.apache.hadoop.record.compiler.generated.SimpleCharStream input_stream;\n  java.lang.StringBuffer image;\n  int jjimageLen;\n  int lengthOfMatch;\n  protected char curChar;\n  int curLexState;\n  int defaultLexState;\n  int jjnewStateCnt;\n  int jjround;\n  int jjmatchedPos;\n  int jjmatchedKind;\n  public void setDebugStream(java.io.PrintStream);\n  public org.apache.hadoop.record.compiler.generated.RccTokenManager(org.apache.hadoop.record.compiler.generated.SimpleCharStream);\n  public org.apache.hadoop.record.compiler.generated.RccTokenManager(org.apache.hadoop.record.compiler.generated.SimpleCharStream, int);\n  public void ReInit(org.apache.hadoop.record.compiler.generated.SimpleCharStream);\n  public void ReInit(org.apache.hadoop.record.compiler.generated.SimpleCharStream, int);\n  public void SwitchTo(int);\n  protected org.apache.hadoop.record.compiler.generated.Token jjFillToken();\n  public org.apache.hadoop.record.compiler.generated.Token getNextToken();\n  void SkipLexicalActions(org.apache.hadoop.record.compiler.generated.Token);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Options$ChecksumOpt.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.class": "Compiled from \"JniBasedUnixGroupsNetgroupMappingWithFallback.java\"\npublic class org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback implements org.apache.hadoop.security.GroupMappingServiceProvider {\n  public org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback();\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void cacheGroupsRefresh() throws java.io.IOException;\n  public void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$RenewDelegationTokenResponseProto$Builder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolSignatureProto$Builder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/FenceMethod.class": "Compiled from \"FenceMethod.java\"\npublic interface org.apache.hadoop.ha.FenceMethod {\n  public abstract void checkArgs(java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public abstract boolean tryFence(org.apache.hadoop.ha.HAServiceTarget, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdmin.class": "Compiled from \"TraceAdmin.java\"\npublic class org.apache.hadoop.tracing.TraceAdmin extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public org.apache.hadoop.tracing.TraceAdmin();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n", 
  "org/apache/hadoop/fs/shell/Delete.class": "Compiled from \"Delete.java\"\nclass org.apache.hadoop.fs.shell.Delete {\n  org.apache.hadoop.fs.shell.Delete();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/record/compiler/JType$CppType.class": "Compiled from \"JType.java\"\npublic abstract class org.apache.hadoop.record.compiler.JType {\n  org.apache.hadoop.record.compiler.JType$JavaType javaType;\n  org.apache.hadoop.record.compiler.JType$CppType cppType;\n  org.apache.hadoop.record.compiler.JType$CType cType;\n  public org.apache.hadoop.record.compiler.JType();\n  static java.lang.String toCamelCase(java.lang.String);\n  abstract java.lang.String getSignature();\n  void setJavaType(org.apache.hadoop.record.compiler.JType$JavaType);\n  org.apache.hadoop.record.compiler.JType$JavaType getJavaType();\n  void setCppType(org.apache.hadoop.record.compiler.JType$CppType);\n  org.apache.hadoop.record.compiler.JType$CppType getCppType();\n  void setCType(org.apache.hadoop.record.compiler.JType$CType);\n  org.apache.hadoop.record.compiler.JType$CType getCType();\n}\n", 
  "org/apache/hadoop/fs/permission/AccessControlException.class": "Compiled from \"AccessControlException.java\"\npublic class org.apache.hadoop.fs.permission.AccessControlException extends java.io.IOException {\n  public org.apache.hadoop.fs.permission.AccessControlException();\n  public org.apache.hadoop.fs.permission.AccessControlException(java.lang.String);\n  public org.apache.hadoop.fs.permission.AccessControlException(java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/record/meta/TypeID.class": "Compiled from \"TypeID.java\"\npublic class org.apache.hadoop.record.meta.TypeID {\n  public static final org.apache.hadoop.record.meta.TypeID BoolTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID BufferTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID ByteTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID DoubleTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID FloatTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID IntTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID LongTypeID;\n  public static final org.apache.hadoop.record.meta.TypeID StringTypeID;\n  protected byte typeVal;\n  org.apache.hadoop.record.meta.TypeID(byte);\n  public byte getTypeVal();\n  void write(org.apache.hadoop.record.RecordOutput, java.lang.String) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsIntValue.class": "Compiled from \"MetricsIntValue.java\"\npublic class org.apache.hadoop.metrics.util.MetricsIntValue extends org.apache.hadoop.metrics.util.MetricsBase {\n  public org.apache.hadoop.metrics.util.MetricsIntValue(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry, java.lang.String);\n  public org.apache.hadoop.metrics.util.MetricsIntValue(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry);\n  public synchronized void set(int);\n  public synchronized int get();\n  public synchronized void pushMetric(org.apache.hadoop.metrics.MetricsRecord);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/source/JvmMetrics.class": "Compiled from \"JvmMetrics.java\"\npublic class org.apache.hadoop.metrics2.source.JvmMetrics implements org.apache.hadoop.metrics2.MetricsSource {\n  static final float M;\n  final java.lang.management.MemoryMXBean memoryMXBean;\n  final java.util.List<java.lang.management.GarbageCollectorMXBean> gcBeans;\n  final java.lang.management.ThreadMXBean threadMXBean;\n  final java.lang.String processName;\n  final java.lang.String sessionId;\n  final java.util.concurrent.ConcurrentHashMap<java.lang.String, org.apache.hadoop.metrics2.MetricsInfo[]> gcInfoCache;\n  org.apache.hadoop.metrics2.source.JvmMetrics(java.lang.String, java.lang.String);\n  public void setPauseMonitor(org.apache.hadoop.util.JvmPauseMonitor);\n  public static org.apache.hadoop.metrics2.source.JvmMetrics create(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSystem);\n  public static org.apache.hadoop.metrics2.source.JvmMetrics initSingleton(java.lang.String, java.lang.String);\n  public void getMetrics(org.apache.hadoop.metrics2.MetricsCollector, boolean);\n}\n", 
  "org/apache/hadoop/fs/HardLink.class": "Compiled from \"HardLink.java\"\npublic class org.apache.hadoop.fs.HardLink {\n  public final org.apache.hadoop.fs.HardLink$LinkStats linkStats;\n  public org.apache.hadoop.fs.HardLink();\n  public static void createHardLink(java.io.File, java.io.File) throws java.io.IOException;\n  public static void createHardLinkMult(java.io.File, java.lang.String[], java.io.File) throws java.io.IOException;\n  public static int getLinkCount(java.io.File) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToActiveRequestProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFileSystem$InternalDirOfViewFs.class": "Compiled from \"ViewFileSystem.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFileSystem extends org.apache.hadoop.fs.FileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  java.net.URI myUri;\n  org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.FileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem() throws java.io.IOException;\n  public java.lang.String getScheme();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  org.apache.hadoop.fs.viewfs.ViewFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getTrashCanLocation(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException;\n  public java.net.URI getUri();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public long getDefaultBlockSize();\n  public short getDefaultReplication();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint[] getMountPoints();\n  static org.apache.hadoop.fs.Path access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.class": "Compiled from \"MetricsSinkAdapter.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsSinkAdapter implements org.apache.hadoop.metrics2.impl.SinkQueue$Consumer<org.apache.hadoop.metrics2.impl.MetricsBuffer> {\n  org.apache.hadoop.metrics2.impl.MetricsSinkAdapter(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, java.lang.String, org.apache.hadoop.metrics2.MetricsFilter, org.apache.hadoop.metrics2.MetricsFilter, org.apache.hadoop.metrics2.MetricsFilter, int, int, int, float, int);\n  boolean putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, long);\n  public boolean putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer);\n  void publishMetricsFromQueue();\n  public void consume(org.apache.hadoop.metrics2.impl.MetricsBuffer);\n  void start();\n  void stop();\n  java.lang.String name();\n  java.lang.String description();\n  void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  org.apache.hadoop.metrics2.MetricsSink sink();\n  public void consume(java.lang.Object) throws java.lang.InterruptedException;\n}\n", 
  "org/apache/hadoop/fs/shell/SnapshotCommands$CreateSnapshot.class": "Compiled from \"SnapshotCommands.java\"\nclass org.apache.hadoop.fs.shell.SnapshotCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.SnapshotCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$FailoverOnNetworkExceptionRetry.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/net/DNSToSwitchMappingWithDependency.class": "Compiled from \"DNSToSwitchMappingWithDependency.java\"\npublic interface org.apache.hadoop.net.DNSToSwitchMappingWithDependency extends org.apache.hadoop.net.DNSToSwitchMapping {\n  public abstract java.util.List<java.lang.String> getDependency(java.lang.String);\n}\n", 
  "org/apache/hadoop/util/LightWeightGSet$LinkedElement.class": "Compiled from \"LightWeightGSet.java\"\npublic class org.apache.hadoop.util.LightWeightGSet<K, E extends K> implements org.apache.hadoop.util.GSet<K, E> {\n  static final int MAX_ARRAY_LENGTH;\n  static final int MIN_ARRAY_LENGTH;\n  public org.apache.hadoop.util.LightWeightGSet(int);\n  public int size();\n  public E get(K);\n  public boolean contains(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  public java.lang.String toString();\n  public void printDetails(java.io.PrintStream);\n  public static int computeCapacity(double, java.lang.String);\n  static int computeCapacity(long, double, java.lang.String);\n  public void clear();\n  static int access$000(org.apache.hadoop.util.LightWeightGSet);\n  static org.apache.hadoop.util.LightWeightGSet$LinkedElement[] access$100(org.apache.hadoop.util.LightWeightGSet);\n  static java.lang.Object access$200(org.apache.hadoop.util.LightWeightGSet, org.apache.hadoop.util.LightWeightGSet$LinkedElement);\n}\n", 
  "org/apache/hadoop/fs/DelegateToFileSystem.class": "Compiled from \"DelegateToFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.DelegateToFileSystem extends org.apache.hadoop.fs.AbstractFileSystem {\n  protected final org.apache.hadoop.fs.FileSystem fsImpl;\n  protected org.apache.hadoop.fs.DelegateToFileSystem(java.net.URI, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, java.lang.String, boolean) throws java.io.IOException, java.net.URISyntaxException;\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean) throws java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/permission/AclEntry$1.class": "Compiled from \"AclEntry.java\"\npublic class org.apache.hadoop.fs.permission.AclEntry {\n  public org.apache.hadoop.fs.permission.AclEntryType getType();\n  public java.lang.String getName();\n  public org.apache.hadoop.fs.permission.FsAction getPermission();\n  public org.apache.hadoop.fs.permission.AclEntryScope getScope();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public static java.util.List<org.apache.hadoop.fs.permission.AclEntry> parseAclSpec(java.lang.String, boolean);\n  public static org.apache.hadoop.fs.permission.AclEntry parseAclEntry(java.lang.String, boolean);\n  public static java.lang.String aclSpecToString(java.util.List<org.apache.hadoop.fs.permission.AclEntry>);\n  org.apache.hadoop.fs.permission.AclEntry(org.apache.hadoop.fs.permission.AclEntryType, java.lang.String, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.AclEntryScope, org.apache.hadoop.fs.permission.AclEntry$1);\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/JavaSerialization$JavaSerializationDeserializer$1.class": "Compiled from \"JavaSerialization.java\"\npublic class org.apache.hadoop.io.serializer.JavaSerialization implements org.apache.hadoop.io.serializer.Serialization<java.io.Serializable> {\n  public org.apache.hadoop.io.serializer.JavaSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.hadoop.io.serializer.Deserializer<java.io.Serializable> getDeserializer(java.lang.Class<java.io.Serializable>);\n  public org.apache.hadoop.io.serializer.Serializer<java.io.Serializable> getSerializer(java.lang.Class<java.io.Serializable>);\n}\n", 
  "org/apache/hadoop/crypto/OpensslCipher.class": "Compiled from \"OpensslCipher.java\"\npublic final class org.apache.hadoop.crypto.OpensslCipher {\n  public static final int ENCRYPT_MODE;\n  public static final int DECRYPT_MODE;\n  public static java.lang.String getLoadingFailureReason();\n  public static final org.apache.hadoop.crypto.OpensslCipher getInstance(java.lang.String) throws java.security.NoSuchAlgorithmException, javax.crypto.NoSuchPaddingException;\n  public void init(int, byte[], byte[]);\n  public int update(java.nio.ByteBuffer, java.nio.ByteBuffer) throws javax.crypto.ShortBufferException;\n  public int doFinal(java.nio.ByteBuffer) throws javax.crypto.ShortBufferException, javax.crypto.IllegalBlockSizeException, javax.crypto.BadPaddingException;\n  public void clean();\n  protected void finalize() throws java.lang.Throwable;\n  public static native java.lang.String getLibraryName();\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer.class": "Compiled from \"HttpServer.java\"\npublic class org.apache.hadoop.http.HttpServer implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.Connector listener;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[]) throws java.io.IOException;\n  public org.mortbay.jetty.Connector createBaseListener(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean) throws java.io.IOException;\n  protected void addContext(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public void setThreads(int, int);\n  public void addSslListener(java.net.InetSocketAddress, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void addSslListener(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  protected void initSpnego(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void start() throws java.io.IOException;\n  void openListener() throws java.lang.Exception;\n  public java.net.InetSocketAddress getListenerAddress();\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  static org.apache.hadoop.security.ssl.SSLFactory access$000(org.apache.hadoop.http.HttpServer);\n  static {};\n}\n", 
  "org/apache/hadoop/tools/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.tools.package-info {\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableQuantiles$RolloverSample.class": "Compiled from \"MutableQuantiles.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableQuantiles extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  public static final org.apache.hadoop.metrics2.util.Quantile[] quantiles;\n  protected java.util.Map<org.apache.hadoop.metrics2.util.Quantile, java.lang.Long> previousSnapshot;\n  public org.apache.hadoop.metrics2.lib.MutableQuantiles(java.lang.String, java.lang.String, java.lang.String, java.lang.String, int);\n  public synchronized void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  public synchronized void add(long);\n  public int getInterval();\n  static long access$002(org.apache.hadoop.metrics2.lib.MutableQuantiles, long);\n  static org.apache.hadoop.metrics2.util.SampleQuantiles access$100(org.apache.hadoop.metrics2.lib.MutableQuantiles);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshResponseProto$1.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProtoOrBuilder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshResponseProtoOrBuilder.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Tail.class": "Compiled from \"Tail.java\"\nclass org.apache.hadoop.fs.shell.Tail extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  org.apache.hadoop.fs.shell.Tail();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected java.util.List<org.apache.hadoop.fs.shell.PathData> expandArgument(java.lang.String) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/proto/RefreshAuthorizationPolicyProtocolProtos$RefreshAuthorizationPolicyProtocolService$BlockingStub.class": "Compiled from \"RefreshAuthorizationPolicyProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/UmaskParser.class": "Compiled from \"UmaskParser.java\"\nclass org.apache.hadoop.fs.permission.UmaskParser extends org.apache.hadoop.fs.permission.PermissionParser {\n  final short umaskMode;\n  public org.apache.hadoop.fs.permission.UmaskParser(java.lang.String) throws java.lang.IllegalArgumentException;\n  public short getUMask();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToStandbyResponseProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JVector$CppVector.class": "Compiled from \"JVector.java\"\npublic class org.apache.hadoop.record.compiler.JVector extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JVector(org.apache.hadoop.record.compiler.JType);\n  java.lang.String getSignature();\n  static void access$000();\n  static java.lang.String access$100(java.lang.String);\n  static void access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.class": "Compiled from \"GetUserMappingsProtocolClientSideTranslatorPB.java\"\npublic class org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB implements org.apache.hadoop.ipc.ProtocolMetaInterface,org.apache.hadoop.tools.GetUserMappingsProtocol,java.io.Closeable {\n  public org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB(org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB);\n  public void close() throws java.io.IOException;\n  public java.lang.String[] getGroupsForUser(java.lang.String) throws java.io.IOException;\n  public boolean isMethodSupported(java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToStandbyRequestProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/Text$2.class": "Compiled from \"Text.java\"\npublic class org.apache.hadoop.io.Text extends org.apache.hadoop.io.BinaryComparable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.BinaryComparable> {\n  public static final int DEFAULT_MAX_LEN;\n  static final int[] bytesFromUTF8;\n  static final int[] offsetsFromUTF8;\n  public org.apache.hadoop.io.Text();\n  public org.apache.hadoop.io.Text(java.lang.String);\n  public org.apache.hadoop.io.Text(org.apache.hadoop.io.Text);\n  public org.apache.hadoop.io.Text(byte[]);\n  public byte[] copyBytes();\n  public byte[] getBytes();\n  public int getLength();\n  public int charAt(int);\n  public int find(java.lang.String);\n  public int find(java.lang.String, int);\n  public void set(java.lang.String);\n  public void set(byte[]);\n  public void set(org.apache.hadoop.io.Text);\n  public void set(byte[], int, int);\n  public void append(byte[], int, int);\n  public void clear();\n  public java.lang.String toString();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void readFields(java.io.DataInput, int) throws java.io.IOException;\n  public static void skip(java.io.DataInput) throws java.io.IOException;\n  public void readWithKnownLength(java.io.DataInput, int) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void write(java.io.DataOutput, int) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static java.lang.String decode(byte[]) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String decode(byte[], int, int) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String decode(byte[], int, int, boolean) throws java.nio.charset.CharacterCodingException;\n  public static java.nio.ByteBuffer encode(java.lang.String) throws java.nio.charset.CharacterCodingException;\n  public static java.nio.ByteBuffer encode(java.lang.String, boolean) throws java.nio.charset.CharacterCodingException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput, int) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String, int) throws java.io.IOException;\n  public static void validateUTF8(byte[]) throws java.nio.charset.MalformedInputException;\n  public static void validateUTF8(byte[], int, int) throws java.nio.charset.MalformedInputException;\n  public static int bytesToCodePoint(java.nio.ByteBuffer);\n  public static int utf8Length(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$1.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/protocolPB/HAServiceProtocolPB.class": "Compiled from \"HAServiceProtocolPB.java\"\npublic interface org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB extends org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$BlockingInterface,org.apache.hadoop.ipc.VersionedProtocol {\n}\n", 
  "org/apache/hadoop/util/bloom/Key.class": "Compiled from \"Key.java\"\npublic class org.apache.hadoop.util.bloom.Key implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.util.bloom.Key> {\n  byte[] bytes;\n  double weight;\n  public org.apache.hadoop.util.bloom.Key();\n  public org.apache.hadoop.util.bloom.Key(byte[]);\n  public org.apache.hadoop.util.bloom.Key(byte[], double);\n  public void set(byte[], double);\n  public byte[] getBytes();\n  public double getWeight();\n  public void incrementWeight(double);\n  public void incrementWeight();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public int compareTo(org.apache.hadoop.util.bloom.Key);\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/security/ssl/SSLHostnameVerifier$3.class": "Compiled from \"SSLHostnameVerifier.java\"\npublic interface org.apache.hadoop.security.ssl.SSLHostnameVerifier extends javax.net.ssl.HostnameVerifier {\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT_AND_LOCALHOST;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT_IE6;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier ALLOW_ALL;\n  public abstract boolean verify(java.lang.String, javax.net.ssl.SSLSession);\n  public abstract void check(java.lang.String, javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String, java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String, java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String[], java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/ZKUtil$BadAuthFormatException.class": "Compiled from \"ZKUtil.java\"\npublic class org.apache.hadoop.util.ZKUtil {\n  public org.apache.hadoop.util.ZKUtil();\n  public static int removeSpecificPerms(int, int);\n  public static java.util.List<org.apache.zookeeper.data.ACL> parseACLs(java.lang.String) throws org.apache.hadoop.util.ZKUtil$BadAclFormatException;\n  public static java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo> parseAuth(java.lang.String) throws org.apache.hadoop.util.ZKUtil$BadAuthFormatException;\n  public static java.lang.String resolveConfIndirection(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController$4.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/net/SocketInputStream$Reader.class": "Compiled from \"SocketInputStream.java\"\npublic class org.apache.hadoop.net.SocketInputStream extends java.io.InputStream implements java.nio.channels.ReadableByteChannel {\n  public org.apache.hadoop.net.SocketInputStream(java.nio.channels.ReadableByteChannel, long) throws java.io.IOException;\n  public org.apache.hadoop.net.SocketInputStream(java.net.Socket, long) throws java.io.IOException;\n  public org.apache.hadoop.net.SocketInputStream(java.net.Socket) throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public synchronized void close() throws java.io.IOException;\n  public java.nio.channels.ReadableByteChannel getChannel();\n  public boolean isOpen();\n  public int read(java.nio.ByteBuffer) throws java.io.IOException;\n  public void waitForReadable() throws java.io.IOException;\n  public void setTimeout(long);\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$RemoveSpanReceiverRequestProto$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/BinaryComparable.class": "Compiled from \"BinaryComparable.java\"\npublic abstract class org.apache.hadoop.io.BinaryComparable implements java.lang.Comparable<org.apache.hadoop.io.BinaryComparable> {\n  public org.apache.hadoop.io.BinaryComparable();\n  public abstract int getLength();\n  public abstract byte[] getBytes();\n  public int compareTo(org.apache.hadoop.io.BinaryComparable);\n  public int compareTo(byte[], int, int);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$RemoveSpanReceiverResponseProto$1.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/MapFile$Reader$Option.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolSignatureResponseProto.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/SocketIOWithTimeout$SelectorPool$SelectorInfo.class": "Compiled from \"SocketIOWithTimeout.java\"\nabstract class org.apache.hadoop.net.SocketIOWithTimeout {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.net.SocketIOWithTimeout(java.nio.channels.SelectableChannel, long) throws java.io.IOException;\n  void close();\n  boolean isOpen();\n  java.nio.channels.SelectableChannel getChannel();\n  static void checkChannelValidity(java.lang.Object) throws java.io.IOException;\n  abstract int performIO(java.nio.ByteBuffer) throws java.io.IOException;\n  int doIO(java.nio.ByteBuffer, int) throws java.io.IOException;\n  static void connect(java.nio.channels.SocketChannel, java.net.SocketAddress, int) throws java.io.IOException;\n  void waitForIO(int) throws java.io.IOException;\n  public void setTimeout(long);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ReflectionUtils$1.class": "Compiled from \"ReflectionUtils.java\"\npublic class org.apache.hadoop.util.ReflectionUtils {\n  public org.apache.hadoop.util.ReflectionUtils();\n  public static void setConf(java.lang.Object, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T newInstance(java.lang.Class<T>, org.apache.hadoop.conf.Configuration);\n  public static void setContentionTracing(boolean);\n  public static synchronized void printThreadInfo(java.io.PrintStream, java.lang.String);\n  public static void logThreadInfo(org.apache.commons.logging.Log, java.lang.String, long);\n  public static <T extends java/lang/Object> java.lang.Class<T> getClass(T);\n  static void clearCache();\n  static int getCacheSize();\n  public static <T extends java/lang/Object> T copy(org.apache.hadoop.conf.Configuration, T, T) throws java.io.IOException;\n  public static void cloneWritableInto(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable) throws java.io.IOException;\n  public static java.util.List<java.lang.reflect.Field> getDeclaredFieldsIncludingInherited(java.lang.Class<?>);\n  public static java.util.List<java.lang.reflect.Method> getDeclaredMethodsIncludingInherited(java.lang.Class<?>);\n  static {};\n}\n", 
  "org/apache/hadoop/util/DiskChecker$DiskOutOfSpaceException.class": "Compiled from \"DiskChecker.java\"\npublic class org.apache.hadoop.util.DiskChecker {\n  public org.apache.hadoop.util.DiskChecker();\n  public static boolean mkdirsWithExistsCheck(java.io.File);\n  public static void checkDirs(java.io.File) throws org.apache.hadoop.util.DiskChecker$DiskErrorException;\n  public static void checkDir(java.io.File) throws org.apache.hadoop.util.DiskChecker$DiskErrorException;\n  public static void mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static void checkDir(org.apache.hadoop.fs.LocalFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.util.DiskChecker$DiskErrorException, java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolInfoService$BlockingInterface.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.class": "Compiled from \"FileBasedKeyStoresFactory.java\"\npublic class org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory implements org.apache.hadoop.security.ssl.KeyStoresFactory {\n  public static final java.lang.String SSL_KEYSTORE_LOCATION_TPL_KEY;\n  public static final java.lang.String SSL_KEYSTORE_PASSWORD_TPL_KEY;\n  public static final java.lang.String SSL_KEYSTORE_KEYPASSWORD_TPL_KEY;\n  public static final java.lang.String SSL_KEYSTORE_TYPE_TPL_KEY;\n  public static final java.lang.String SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY;\n  public static final java.lang.String SSL_TRUSTSTORE_LOCATION_TPL_KEY;\n  public static final java.lang.String SSL_TRUSTSTORE_PASSWORD_TPL_KEY;\n  public static final java.lang.String SSL_TRUSTSTORE_TYPE_TPL_KEY;\n  public static final java.lang.String DEFAULT_KEYSTORE_TYPE;\n  public static final int DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL;\n  public org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory();\n  public static java.lang.String resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode, java.lang.String);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void init(org.apache.hadoop.security.ssl.SSLFactory$Mode) throws java.io.IOException, java.security.GeneralSecurityException;\n  java.lang.String getPassword(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String);\n  public synchronized void destroy();\n  public javax.net.ssl.KeyManager[] getKeyManagers();\n  public javax.net.ssl.TrustManager[] getTrustManagers();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$1.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$GetDelegationTokenResponseProto$Builder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/CompareUtils$Scalar.class": "Compiled from \"CompareUtils.java\"\nclass org.apache.hadoop.io.file.tfile.CompareUtils {\n}\n", 
  "org/apache/hadoop/util/LightWeightGSet.class": "Compiled from \"LightWeightGSet.java\"\npublic class org.apache.hadoop.util.LightWeightGSet<K, E extends K> implements org.apache.hadoop.util.GSet<K, E> {\n  static final int MAX_ARRAY_LENGTH;\n  static final int MIN_ARRAY_LENGTH;\n  public org.apache.hadoop.util.LightWeightGSet(int);\n  public int size();\n  public E get(K);\n  public boolean contains(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  public java.lang.String toString();\n  public void printDetails(java.io.PrintStream);\n  public static int computeCapacity(double, java.lang.String);\n  static int computeCapacity(long, double, java.lang.String);\n  public void clear();\n  static int access$000(org.apache.hadoop.util.LightWeightGSet);\n  static org.apache.hadoop.util.LightWeightGSet$LinkedElement[] access$100(org.apache.hadoop.util.LightWeightGSet);\n  static java.lang.Object access$200(org.apache.hadoop.util.LightWeightGSet, org.apache.hadoop.util.LightWeightGSet$LinkedElement);\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$4.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsSystem.class": "Compiled from \"MetricsSystem.java\"\npublic abstract class org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSystemMXBean {\n  public org.apache.hadoop.metrics2.MetricsSystem();\n  public abstract org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public abstract <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public abstract void unregisterSource(java.lang.String);\n  public <T extends java/lang/Object> T register(T);\n  public abstract org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  public abstract <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  public abstract void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public abstract void publishMetricsNow();\n  public abstract boolean shutdown();\n}\n", 
  "org/apache/hadoop/fs/HarFileSystem$LruCache.class": "Compiled from \"HarFileSystem.java\"\npublic class org.apache.hadoop.fs.HarFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final java.lang.String METADATA_CACHE_ENTRIES_KEY;\n  public static final int METADATA_CACHE_ENTRIES_DEFAULT;\n  public static final int VERSION;\n  public org.apache.hadoop.fs.HarFileSystem();\n  public java.lang.String getScheme();\n  public org.apache.hadoop.fs.HarFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public int getHarVersion() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  public java.net.URI getUri();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  static org.apache.hadoop.fs.BlockLocation[] fixBlockLocations(org.apache.hadoop.fs.BlockLocation[], long, long, long);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public static int getHarHash(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long);\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  org.apache.hadoop.fs.HarFileSystem$HarMetaData getMetadata();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  static java.lang.String access$200(org.apache.hadoop.fs.HarFileSystem, java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.hadoop.fs.HarFileSystem$HarMetaData access$300(org.apache.hadoop.fs.HarFileSystem);\n  static java.lang.String access$400(java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.commons.logging.Log access$500();\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserMappingsProtocolService$Interface.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$Call.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/snappy/SnappyDecompressor$SnappyDirectDecompressor.class": "Compiled from \"SnappyDecompressor.java\"\npublic class org.apache.hadoop.io.compress.snappy.SnappyDecompressor implements org.apache.hadoop.io.compress.Decompressor {\n  static final boolean $assertionsDisabled;\n  public static boolean isNativeCodeLoaded();\n  public org.apache.hadoop.io.compress.snappy.SnappyDecompressor(int);\n  public org.apache.hadoop.io.compress.snappy.SnappyDecompressor();\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public boolean needsDictionary();\n  public boolean finished();\n  public int decompress(byte[], int, int) throws java.io.IOException;\n  public int getRemaining();\n  public void reset();\n  public void end();\n  int decompressDirect(java.nio.ByteBuffer, java.nio.ByteBuffer) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/authorize/ProxyServers.class": "Compiled from \"ProxyServers.java\"\npublic class org.apache.hadoop.security.authorize.ProxyServers {\n  public static final java.lang.String CONF_HADOOP_PROXYSERVERS;\n  public org.apache.hadoop.security.authorize.ProxyServers();\n  public static void refresh();\n  public static void refresh(org.apache.hadoop.conf.Configuration);\n  public static boolean isProxyServer(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/DUHelper.class": "Compiled from \"DUHelper.java\"\npublic class org.apache.hadoop.fs.DUHelper {\n  public static long getFolderUsage(java.lang.String);\n  public java.lang.String check(java.lang.String);\n  public long getFileCount();\n  public double getUsage();\n  public static void main(java.lang.String[]);\n}\n", 
  "org/apache/hadoop/io/SequenceFile$BlockCompressWriter.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$RpcRequestWrapper.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/io/InputBuffer.class": "Compiled from \"InputBuffer.java\"\npublic class org.apache.hadoop.io.InputBuffer extends java.io.FilterInputStream {\n  public org.apache.hadoop.io.InputBuffer();\n  public void reset(byte[], int);\n  public void reset(byte[], int, int);\n  public int getPosition();\n  public int getLength();\n}\n", 
  "org/apache/hadoop/security/authorize/PolicyProvider.class": "Compiled from \"PolicyProvider.java\"\npublic abstract class org.apache.hadoop.security.authorize.PolicyProvider {\n  public static final java.lang.String POLICY_PROVIDER_CONFIG;\n  public static final org.apache.hadoop.security.authorize.PolicyProvider DEFAULT_POLICY_PROVIDER;\n  public org.apache.hadoop.security.authorize.PolicyProvider();\n  public abstract org.apache.hadoop.security.authorize.Service[] getServices();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$StatisticsData.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/security/Credentials.class": "Compiled from \"Credentials.java\"\npublic class org.apache.hadoop.security.Credentials implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.security.Credentials();\n  public org.apache.hadoop.security.Credentials(org.apache.hadoop.security.Credentials);\n  public org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier> getToken(org.apache.hadoop.io.Text);\n  public void addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getAllTokens();\n  public int numberOfTokens();\n  public byte[] getSecretKey(org.apache.hadoop.io.Text);\n  public int numberOfSecretKeys();\n  public void addSecretKey(org.apache.hadoop.io.Text, byte[]);\n  public void removeSecretKey(org.apache.hadoop.io.Text);\n  public java.util.List<org.apache.hadoop.io.Text> getAllSecretKeys();\n  public static org.apache.hadoop.security.Credentials readTokenStorageFile(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.security.Credentials readTokenStorageFile(java.io.File, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void readTokenStorageStream(java.io.DataInputStream) throws java.io.IOException;\n  public void writeTokenStorageToStream(java.io.DataOutputStream) throws java.io.IOException;\n  public void writeTokenStorageFile(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void addAll(org.apache.hadoop.security.Credentials);\n  public void mergeAll(org.apache.hadoop.security.Credentials);\n  static {};\n}\n", 
  "org/apache/hadoop/io/NullWritable$Comparator.class": "Compiled from \"NullWritable.java\"\npublic class org.apache.hadoop.io.NullWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.NullWritable> {\n  public static org.apache.hadoop.io.NullWritable get();\n  public java.lang.String toString();\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.NullWritable);\n  public boolean equals(java.lang.Object);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ChecksumFileSystem$ChecksumFSInputChecker.class": "Compiled from \"ChecksumFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.ChecksumFileSystem extends org.apache.hadoop.fs.FilterFileSystem {\n  public static double getApproxChkSumLength(long);\n  public org.apache.hadoop.fs.ChecksumFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem getRawFileSystem();\n  public org.apache.hadoop.fs.Path getChecksumFile(org.apache.hadoop.fs.Path);\n  public static boolean isChecksumFile(org.apache.hadoop.fs.Path);\n  public long getChecksumFileLength(org.apache.hadoop.fs.Path, long);\n  public int getBytesPerSum();\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public static long getChecksumLength(long, int);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  static int access$000(org.apache.hadoop.fs.ChecksumFileSystem, int, int);\n  static byte[] access$100();\n  static boolean access$200(org.apache.hadoop.fs.ChecksumFileSystem);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$ParsedTimeDuration.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/IdMappingServiceProvider.class": "Compiled from \"IdMappingServiceProvider.java\"\npublic interface org.apache.hadoop.security.IdMappingServiceProvider {\n  public abstract int getUid(java.lang.String) throws java.io.IOException;\n  public abstract int getGid(java.lang.String) throws java.io.IOException;\n  public abstract java.lang.String getUserName(int, java.lang.String);\n  public abstract java.lang.String getGroupName(int, java.lang.String);\n  public abstract int getUidAllowingUnknown(java.lang.String);\n  public abstract int getGidAllowingUnknown(java.lang.String);\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceProtocolService$2.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ShutdownHookManager$HookEntry.class": "Compiled from \"ShutdownHookManager.java\"\npublic class org.apache.hadoop.util.ShutdownHookManager {\n  public static org.apache.hadoop.util.ShutdownHookManager get();\n  java.util.List<java.lang.Runnable> getShutdownHooksInOrder();\n  public void addShutdownHook(java.lang.Runnable, int);\n  public boolean removeShutdownHook(java.lang.Runnable);\n  public boolean hasShutdownHook(java.lang.Runnable);\n  public boolean isShutdownInProgress();\n  static org.apache.hadoop.util.ShutdownHookManager access$000();\n  static java.util.concurrent.atomic.AtomicBoolean access$100(org.apache.hadoop.util.ShutdownHookManager);\n  static org.apache.commons.logging.Log access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/net/NetworkTopologyWithNodeGroup.class": "Compiled from \"NetworkTopologyWithNodeGroup.java\"\npublic class org.apache.hadoop.net.NetworkTopologyWithNodeGroup extends org.apache.hadoop.net.NetworkTopology {\n  public static final java.lang.String DEFAULT_NODEGROUP;\n  public org.apache.hadoop.net.NetworkTopologyWithNodeGroup();\n  protected org.apache.hadoop.net.Node getNodeForNetworkLocation(org.apache.hadoop.net.Node);\n  public java.lang.String getRack(java.lang.String);\n  public java.lang.String getNodeGroup(java.lang.String);\n  public boolean isOnSameRack(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isOnSameNodeGroup(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isNodeGroupAware();\n  public void add(org.apache.hadoop.net.Node);\n  public void remove(org.apache.hadoop.net.Node);\n  protected int getWeight(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public void sortByDistance(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node[], int);\n}\n", 
  "org/apache/hadoop/record/XmlRecordInput$XMLParser.class": "Compiled from \"XmlRecordInput.java\"\npublic class org.apache.hadoop.record.XmlRecordInput implements org.apache.hadoop.record.RecordInput {\n  public org.apache.hadoop.record.XmlRecordInput(java.io.InputStream);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  static int access$000(org.apache.hadoop.record.XmlRecordInput);\n  static java.util.ArrayList access$100(org.apache.hadoop.record.XmlRecordInput);\n  static int access$008(org.apache.hadoop.record.XmlRecordInput);\n}\n", 
  "org/apache/hadoop/metrics2/MetricsSystem$Callback.class": "Compiled from \"MetricsSystem.java\"\npublic abstract class org.apache.hadoop.metrics2.MetricsSystem implements org.apache.hadoop.metrics2.MetricsSystemMXBean {\n  public org.apache.hadoop.metrics2.MetricsSystem();\n  public abstract org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String);\n  public abstract <T extends java/lang/Object> T register(java.lang.String, java.lang.String, T);\n  public abstract void unregisterSource(java.lang.String);\n  public <T extends java/lang/Object> T register(T);\n  public abstract org.apache.hadoop.metrics2.MetricsSource getSource(java.lang.String);\n  public abstract <T extends org/apache/hadoop/metrics2/MetricsSink> T register(java.lang.String, java.lang.String, T);\n  public abstract void register(org.apache.hadoop.metrics2.MetricsSystem$Callback);\n  public abstract void publishMetricsNow();\n  public abstract boolean shutdown();\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$1.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/util/ShutdownThreadsHelper.class": "Compiled from \"ShutdownThreadsHelper.java\"\npublic class org.apache.hadoop.util.ShutdownThreadsHelper {\n  static final int SHUTDOWN_WAIT_MS;\n  public org.apache.hadoop.util.ShutdownThreadsHelper();\n  public static boolean shutdownThread(java.lang.Thread);\n  public static boolean shutdownThread(java.lang.Thread, long);\n  public static boolean shutdownExecutorService(java.util.concurrent.ExecutorService) throws java.lang.InterruptedException;\n  public static boolean shutdownExecutorService(java.util.concurrent.ExecutorService, long) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderFactory.class": "Compiled from \"KeyProviderFactory.java\"\npublic abstract class org.apache.hadoop.crypto.key.KeyProviderFactory {\n  public static final java.lang.String KEY_PROVIDER_PATH;\n  public org.apache.hadoop.crypto.key.KeyProviderFactory();\n  public abstract org.apache.hadoop.crypto.key.KeyProvider createProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.crypto.key.KeyProvider> getProviders(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.crypto.key.KeyProvider get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/local/RawLocalFs.class": "Compiled from \"RawLocalFs.java\"\npublic class org.apache.hadoop.fs.local.RawLocalFs extends org.apache.hadoop.fs.DelegateToFileSystem {\n  org.apache.hadoop.fs.local.RawLocalFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  org.apache.hadoop.fs.local.RawLocalFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public boolean isValidName(java.lang.String);\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToStandbyResponseProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension$DelegationTokenExtension.class": "Compiled from \"KeyProviderDelegationTokenExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension> {\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public static org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/AbstractMetricsContext$MetricMap.class": "Compiled from \"AbstractMetricsContext.java\"\npublic abstract class org.apache.hadoop.metrics.spi.AbstractMetricsContext implements org.apache.hadoop.metrics.MetricsContext {\n  protected org.apache.hadoop.metrics.spi.AbstractMetricsContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  protected java.lang.String getAttribute(java.lang.String);\n  protected java.util.Map<java.lang.String, java.lang.String> getAttributeTable(java.lang.String);\n  public java.lang.String getContextName();\n  public org.apache.hadoop.metrics.ContextFactory getContextFactory();\n  public synchronized void startMonitoring() throws java.io.IOException;\n  public synchronized void stopMonitoring();\n  public boolean isMonitoring();\n  public synchronized void close();\n  public final synchronized org.apache.hadoop.metrics.MetricsRecord createRecord(java.lang.String);\n  protected org.apache.hadoop.metrics.MetricsRecord newRecord(java.lang.String);\n  public synchronized void registerUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized void unregisterUpdater(org.apache.hadoop.metrics.Updater);\n  public synchronized java.util.Map<java.lang.String, java.util.Collection<org.apache.hadoop.metrics.spi.OutputRecord>> getAllRecords();\n  protected abstract void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord) throws java.io.IOException;\n  protected void flush() throws java.io.IOException;\n  protected void update(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  protected void remove(org.apache.hadoop.metrics.spi.MetricsRecordImpl);\n  public int getPeriod();\n  protected void setPeriod(int);\n  protected void parseAndSetPeriod(java.lang.String);\n  static void access$000(org.apache.hadoop.metrics.spi.AbstractMetricsContext) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcRequestHeaderProto$OperationProto$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$TFileMeta.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HarFileSystem$HarFSDataInputStream$HarFsInputStream.class": "Compiled from \"HarFileSystem.java\"\npublic class org.apache.hadoop.fs.HarFileSystem extends org.apache.hadoop.fs.FileSystem {\n  public static final java.lang.String METADATA_CACHE_ENTRIES_KEY;\n  public static final int METADATA_CACHE_ENTRIES_DEFAULT;\n  public static final int VERSION;\n  public org.apache.hadoop.fs.HarFileSystem();\n  public java.lang.String getScheme();\n  public org.apache.hadoop.fs.HarFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConf();\n  public int getHarVersion() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  public java.net.URI getUri();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  static org.apache.hadoop.fs.BlockLocation[] fixBlockLocations(org.apache.hadoop.fs.BlockLocation[], long, long, long);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public static int getHarHash(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long);\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  org.apache.hadoop.fs.HarFileSystem$HarMetaData getMetadata();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  static java.lang.String access$200(org.apache.hadoop.fs.HarFileSystem, java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.hadoop.fs.HarFileSystem$HarMetaData access$300(org.apache.hadoop.fs.HarFileSystem);\n  static java.lang.String access$400(java.lang.String) throws java.io.UnsupportedEncodingException;\n  static org.apache.commons.logging.Log access$500();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RPC$Builder.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JVector.class": "Compiled from \"JVector.java\"\npublic class org.apache.hadoop.record.compiler.JVector extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JVector(org.apache.hadoop.record.compiler.JType);\n  java.lang.String getSignature();\n  static void access$000();\n  static java.lang.String access$100(java.lang.String);\n  static void access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/util/ApplicationClassLoader$1.class": "Compiled from \"ApplicationClassLoader.java\"\npublic class org.apache.hadoop.util.ApplicationClassLoader extends java.net.URLClassLoader {\n  public static final java.lang.String SYSTEM_CLASSES_DEFAULT;\n  public org.apache.hadoop.util.ApplicationClassLoader(java.net.URL[], java.lang.ClassLoader, java.util.List<java.lang.String>);\n  public org.apache.hadoop.util.ApplicationClassLoader(java.lang.String, java.lang.ClassLoader, java.util.List<java.lang.String>) throws java.net.MalformedURLException;\n  static java.net.URL[] constructUrlsFromClasspath(java.lang.String) throws java.net.MalformedURLException;\n  public java.net.URL getResource(java.lang.String);\n  public java.lang.Class<?> loadClass(java.lang.String) throws java.lang.ClassNotFoundException;\n  protected synchronized java.lang.Class<?> loadClass(java.lang.String, boolean) throws java.lang.ClassNotFoundException;\n  public static boolean isSystemClass(java.lang.String, java.util.List<java.lang.String>);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsDynamicMBeanBase.class": "Compiled from \"MetricsDynamicMBeanBase.java\"\npublic abstract class org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase implements javax.management.DynamicMBean {\n  protected org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase(org.apache.hadoop.metrics.util.MetricsRegistry, java.lang.String);\n  public java.lang.Object getAttribute(java.lang.String) throws javax.management.AttributeNotFoundException, javax.management.MBeanException, javax.management.ReflectionException;\n  public javax.management.AttributeList getAttributes(java.lang.String[]);\n  public javax.management.MBeanInfo getMBeanInfo();\n  public java.lang.Object invoke(java.lang.String, java.lang.Object[], java.lang.String[]) throws javax.management.MBeanException, javax.management.ReflectionException;\n  public void setAttribute(javax.management.Attribute) throws javax.management.AttributeNotFoundException, javax.management.InvalidAttributeValueException, javax.management.MBeanException, javax.management.ReflectionException;\n  public javax.management.AttributeList setAttributes(javax.management.AttributeList);\n}\n", 
  "org/apache/hadoop/security/GroupMappingServiceProvider.class": "Compiled from \"GroupMappingServiceProvider.java\"\npublic interface org.apache.hadoop.security.GroupMappingServiceProvider {\n  public static final java.lang.String GROUP_MAPPING_CONFIG_PREFIX;\n  public abstract java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public abstract void cacheGroupsRefresh() throws java.io.IOException;\n  public abstract void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToActiveResponseProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolPB.class": "Compiled from \"GenericRefreshProtocolPB.java\"\npublic interface org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB extends org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshProtocolService$BlockingInterface {\n}\n", 
  "org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.class": "Compiled from \"RpcDetailedMetrics.java\"\npublic class org.apache.hadoop.ipc.metrics.RpcDetailedMetrics {\n  org.apache.hadoop.metrics2.lib.MutableRates rates;\n  static final org.apache.commons.logging.Log LOG;\n  final org.apache.hadoop.metrics2.lib.MetricsRegistry registry;\n  final java.lang.String name;\n  org.apache.hadoop.ipc.metrics.RpcDetailedMetrics(int);\n  public java.lang.String name();\n  public static org.apache.hadoop.ipc.metrics.RpcDetailedMetrics create(int);\n  public void init(java.lang.Class<?>);\n  public void addProcessingTime(java.lang.String, int);\n  public void shutdown();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsTimeVaryingLong.class": "Compiled from \"MetricsTimeVaryingLong.java\"\npublic class org.apache.hadoop.metrics.util.MetricsTimeVaryingLong extends org.apache.hadoop.metrics.util.MetricsBase {\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingLong(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry, java.lang.String);\n  public org.apache.hadoop.metrics.util.MetricsTimeVaryingLong(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry);\n  public synchronized void inc(long);\n  public synchronized void inc();\n  public synchronized void pushMetric(org.apache.hadoop.metrics.MetricsRecord);\n  public synchronized long getPreviousIntervalValue();\n  public synchronized long getCurrentIntervalValue();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/AbstractFileSystem$1.class": "Compiled from \"AbstractFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.AbstractFileSystem {\n  static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  static final java.lang.String NO_ABSTRACT_FS_ERROR;\n  public org.apache.hadoop.fs.FileSystem$Statistics getStatistics();\n  public boolean isValidName(java.lang.String);\n  static <T extends java/lang/Object> T newInstance(java.lang.Class<T>, java.net.URI, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.fs.AbstractFileSystem createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  protected static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics();\n  protected static synchronized java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static org.apache.hadoop.fs.AbstractFileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem(java.net.URI, java.lang.String, boolean, int) throws java.net.URISyntaxException;\n  public void checkScheme(java.net.URI, java.lang.String);\n  public abstract int getUriDefaultPort();\n  public java.net.URI getUri();\n  public void checkPath(org.apache.hadoop.fs.Path);\n  public java.lang.String getUriPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public final org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public final void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FsStatus getFsStatus() throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract void setVerifyChecksum(boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ProtoUtil.class": "Compiled from \"ProtoUtil.java\"\npublic abstract class org.apache.hadoop.util.ProtoUtil {\n  public org.apache.hadoop.util.ProtoUtil();\n  public static int readRawVarint32(java.io.DataInput) throws java.io.IOException;\n  public static org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto makeIpcConnectionContext(java.lang.String, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto);\n  public static org.apache.hadoop.security.UserGroupInformation getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto convert(org.apache.hadoop.ipc.RPC$RpcKind);\n  public static org.apache.hadoop.ipc.RPC$RpcKind convert(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto, int, int, byte[]);\n}\n", 
  "org/apache/hadoop/fs/ParentNotDirectoryException.class": "Compiled from \"ParentNotDirectoryException.java\"\npublic class org.apache.hadoop.fs.ParentNotDirectoryException extends java.io.IOException {\n  public org.apache.hadoop.fs.ParentNotDirectoryException();\n  public org.apache.hadoop.fs.ParentNotDirectoryException(java.lang.String);\n}\n", 
  "org/apache/hadoop/record/compiler/JType.class": "Compiled from \"JType.java\"\npublic abstract class org.apache.hadoop.record.compiler.JType {\n  org.apache.hadoop.record.compiler.JType$JavaType javaType;\n  org.apache.hadoop.record.compiler.JType$CppType cppType;\n  org.apache.hadoop.record.compiler.JType$CType cType;\n  public org.apache.hadoop.record.compiler.JType();\n  static java.lang.String toCamelCase(java.lang.String);\n  abstract java.lang.String getSignature();\n  void setJavaType(org.apache.hadoop.record.compiler.JType$JavaType);\n  org.apache.hadoop.record.compiler.JType$JavaType getJavaType();\n  void setCppType(org.apache.hadoop.record.compiler.JType$CppType);\n  org.apache.hadoop.record.compiler.JType$CppType getCppType();\n  void setCType(org.apache.hadoop.record.compiler.JType$CType);\n  org.apache.hadoop.record.compiler.JType$CType getCType();\n}\n", 
  "org/apache/hadoop/fs/UnsupportedFileSystemException.class": "Compiled from \"UnsupportedFileSystemException.java\"\npublic class org.apache.hadoop.fs.UnsupportedFileSystemException extends java.io.IOException {\n  public org.apache.hadoop.fs.UnsupportedFileSystemException(java.lang.String);\n}\n", 
  "org/apache/hadoop/security/token/delegation/DelegationKey.class": "Compiled from \"DelegationKey.java\"\npublic class org.apache.hadoop.security.token.delegation.DelegationKey implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.security.token.delegation.DelegationKey();\n  public org.apache.hadoop.security.token.delegation.DelegationKey(int, long, javax.crypto.SecretKey);\n  public org.apache.hadoop.security.token.delegation.DelegationKey(int, long, byte[]);\n  public int getKeyId();\n  public long getExpiryDate();\n  public javax.crypto.SecretKey getKey();\n  public byte[] getEncodedKey();\n  public void setExpiryDate(long);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n}\n", 
  "org/apache/hadoop/security/JniBasedUnixGroupsMapping.class": "Compiled from \"JniBasedUnixGroupsMapping.java\"\npublic class org.apache.hadoop.security.JniBasedUnixGroupsMapping implements org.apache.hadoop.security.GroupMappingServiceProvider {\n  public org.apache.hadoop.security.JniBasedUnixGroupsMapping();\n  static native void anchorNative();\n  static native java.lang.String[] getGroupsForUser(java.lang.String);\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void cacheGroupsRefresh() throws java.io.IOException;\n  public void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableGaugeInt.class": "Compiled from \"MutableGaugeInt.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableGaugeInt extends org.apache.hadoop.metrics2.lib.MutableGauge {\n  org.apache.hadoop.metrics2.lib.MutableGaugeInt(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public int value();\n  public void incr();\n  public void incr(int);\n  public void decr();\n  public void decr(int);\n  public void set(int);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n}\n", 
  "org/apache/hadoop/util/JvmPauseMonitor$GcTimes.class": "Compiled from \"JvmPauseMonitor.java\"\npublic class org.apache.hadoop.util.JvmPauseMonitor {\n  public org.apache.hadoop.util.JvmPauseMonitor(org.apache.hadoop.conf.Configuration);\n  public void start();\n  public void stop();\n  public boolean isStarted();\n  public long getNumGcWarnThreadholdExceeded();\n  public long getNumGcInfoThresholdExceeded();\n  public long getTotalGcExtraSleepTime();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static java.util.Map access$400(org.apache.hadoop.util.JvmPauseMonitor);\n  static boolean access$500(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$600(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$704(org.apache.hadoop.util.JvmPauseMonitor);\n  static java.lang.String access$800(org.apache.hadoop.util.JvmPauseMonitor, long, java.util.Map, java.util.Map);\n  static org.apache.commons.logging.Log access$900();\n  static long access$1000(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$1104(org.apache.hadoop.util.JvmPauseMonitor);\n  static long access$1214(org.apache.hadoop.util.JvmPauseMonitor, long);\n  static {};\n}\n", 
  "org/apache/hadoop/io/DoubleWritable.class": "Compiled from \"DoubleWritable.java\"\npublic class org.apache.hadoop.io.DoubleWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.DoubleWritable> {\n  public org.apache.hadoop.io.DoubleWritable();\n  public org.apache.hadoop.io.DoubleWritable(double);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void set(double);\n  public double get();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.DoubleWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ChecksumFileSystem$1.class": "Compiled from \"ChecksumFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.ChecksumFileSystem extends org.apache.hadoop.fs.FilterFileSystem {\n  public static double getApproxChkSumLength(long);\n  public org.apache.hadoop.fs.ChecksumFileSystem(org.apache.hadoop.fs.FileSystem);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FileSystem getRawFileSystem();\n  public org.apache.hadoop.fs.Path getChecksumFile(org.apache.hadoop.fs.Path);\n  public static boolean isChecksumFile(org.apache.hadoop.fs.Path);\n  public long getChecksumFileLength(org.apache.hadoop.fs.Path, long);\n  public int getBytesPerSum();\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public static long getChecksumLength(long, int);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  static int access$000(org.apache.hadoop.fs.ChecksumFileSystem, int, int);\n  static byte[] access$100();\n  static boolean access$200(org.apache.hadoop.fs.ChecksumFileSystem);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ContentSummary$Builder.class": "Compiled from \"ContentSummary.java\"\npublic class org.apache.hadoop.fs.ContentSummary implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.fs.ContentSummary();\n  public org.apache.hadoop.fs.ContentSummary(long, long, long);\n  public org.apache.hadoop.fs.ContentSummary(long, long, long, long, long, long);\n  public long getLength();\n  public long getDirectoryCount();\n  public long getFileCount();\n  public long getQuota();\n  public long getSpaceConsumed();\n  public long getSpaceQuota();\n  public long getTypeQuota(org.apache.hadoop.fs.StorageType);\n  public long getTypeConsumed(org.apache.hadoop.fs.StorageType);\n  public boolean isTypeQuotaSet();\n  public boolean isTypeConsumedAvailable();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static java.lang.String getHeader(boolean);\n  public java.lang.String toString();\n  public java.lang.String toString(boolean);\n  public java.lang.String toString(boolean, boolean);\n  org.apache.hadoop.fs.ContentSummary(long, long, long, long, long, long, long[], long[], org.apache.hadoop.fs.ContentSummary$1);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RPCTraceInfoProto$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Daemon$DaemonFactory.class": "Compiled from \"Daemon.java\"\npublic class org.apache.hadoop.util.Daemon extends java.lang.Thread {\n  java.lang.Runnable runnable;\n  public org.apache.hadoop.util.Daemon();\n  public org.apache.hadoop.util.Daemon(java.lang.Runnable);\n  public org.apache.hadoop.util.Daemon(java.lang.ThreadGroup, java.lang.Runnable);\n  public java.lang.Runnable getRunnable();\n}\n", 
  "org/apache/hadoop/ipc/UnexpectedServerException.class": "Compiled from \"UnexpectedServerException.java\"\npublic class org.apache.hadoop.ipc.UnexpectedServerException extends org.apache.hadoop.ipc.RpcException {\n  org.apache.hadoop.ipc.UnexpectedServerException(java.lang.String);\n  org.apache.hadoop.ipc.UnexpectedServerException(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/fs/ftp/FTPInputStream.class": "Compiled from \"FTPInputStream.java\"\npublic class org.apache.hadoop.fs.ftp.FTPInputStream extends org.apache.hadoop.fs.FSInputStream {\n  java.io.InputStream wrappedStream;\n  org.apache.commons.net.ftp.FTPClient client;\n  org.apache.hadoop.fs.FileSystem$Statistics stats;\n  boolean closed;\n  long pos;\n  public org.apache.hadoop.fs.ftp.FTPInputStream(java.io.InputStream, org.apache.commons.net.ftp.FTPClient, org.apache.hadoop.fs.FileSystem$Statistics);\n  public long getPos() throws java.io.IOException;\n  public void seek(long) throws java.io.IOException;\n  public boolean seekToNewSource(long) throws java.io.IOException;\n  public synchronized int read() throws java.io.IOException;\n  public synchronized int read(byte[], int, int) throws java.io.IOException;\n  public synchronized void close() throws java.io.IOException;\n  public boolean markSupported();\n  public void mark(int);\n  public void reset() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/ContentSummary.class": "Compiled from \"ContentSummary.java\"\npublic class org.apache.hadoop.fs.ContentSummary implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.fs.ContentSummary();\n  public org.apache.hadoop.fs.ContentSummary(long, long, long);\n  public org.apache.hadoop.fs.ContentSummary(long, long, long, long, long, long);\n  public long getLength();\n  public long getDirectoryCount();\n  public long getFileCount();\n  public long getQuota();\n  public long getSpaceConsumed();\n  public long getSpaceQuota();\n  public long getTypeQuota(org.apache.hadoop.fs.StorageType);\n  public long getTypeConsumed(org.apache.hadoop.fs.StorageType);\n  public boolean isTypeQuotaSet();\n  public boolean isTypeConsumedAvailable();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static java.lang.String getHeader(boolean);\n  public java.lang.String toString();\n  public java.lang.String toString(boolean);\n  public java.lang.String toString(boolean, boolean);\n  org.apache.hadoop.fs.ContentSummary(long, long, long, long, long, long, long[], long[], org.apache.hadoop.fs.ContentSummary$1);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Reader$BlockReader.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryInvocationHandler.class": "Compiled from \"RetryInvocationHandler.java\"\npublic class org.apache.hadoop.io.retry.RetryInvocationHandler<T> implements org.apache.hadoop.ipc.RpcInvocationHandler {\n  public static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.io.retry.RetryInvocationHandler(org.apache.hadoop.io.retry.FailoverProxyProvider<T>, org.apache.hadoop.io.retry.RetryPolicy);\n  protected org.apache.hadoop.io.retry.RetryInvocationHandler(org.apache.hadoop.io.retry.FailoverProxyProvider<T>, org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.String, org.apache.hadoop.io.retry.RetryPolicy>);\n  public java.lang.Object invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[]) throws java.lang.Throwable;\n  protected java.lang.Object invokeMethod(java.lang.reflect.Method, java.lang.Object[]) throws java.lang.Throwable;\n  static boolean isRpcInvocation(java.lang.Object);\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.ipc.Client$ConnectionId getConnectionId();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$ExceptionsHandler.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceProtocolService$BlockingInterface.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/JavaKeyStoreProvider$Factory.class": "Compiled from \"JavaKeyStoreProvider.java\"\npublic class org.apache.hadoop.security.alias.JavaKeyStoreProvider extends org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider {\n  public static final java.lang.String SCHEME_NAME;\n  protected java.lang.String getSchemeName();\n  protected java.io.OutputStream getOutputStreamForKeystore() throws java.io.IOException;\n  protected boolean keystoreExists() throws java.io.IOException;\n  protected java.io.InputStream getInputStreamForFile() throws java.io.IOException;\n  protected void createPermissions(java.lang.String);\n  protected void stashOriginalFilePermissions() throws java.io.IOException;\n  protected void initFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  org.apache.hadoop.security.alias.JavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.alias.JavaKeyStoreProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/WritableRpcEngine$Invoker.class": "Compiled from \"WritableRpcEngine.java\"\npublic class org.apache.hadoop.ipc.WritableRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final long writableRpcVersion;\n  public org.apache.hadoop.ipc.WritableRpcEngine();\n  public static synchronized void ensureInitialized();\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$000();\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/DirectoryListingStartAfterNotFoundException.class": "Compiled from \"DirectoryListingStartAfterNotFoundException.java\"\npublic class org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException extends java.io.IOException {\n  public org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException();\n  public org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/permission/AclUtil.class": "Compiled from \"AclUtil.java\"\npublic final class org.apache.hadoop.fs.permission.AclUtil {\n  public static java.util.List<org.apache.hadoop.fs.permission.AclEntry> getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission, java.util.List<org.apache.hadoop.fs.permission.AclEntry>);\n  public static java.util.List<org.apache.hadoop.fs.permission.AclEntry> getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission);\n  public static boolean isMinimalAcl(java.util.List<org.apache.hadoop.fs.permission.AclEntry>);\n}\n", 
  "org/apache/hadoop/ha/NodeFencer.class": "Compiled from \"NodeFencer.java\"\npublic class org.apache.hadoop.ha.NodeFencer {\n  org.apache.hadoop.ha.NodeFencer(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public static org.apache.hadoop.ha.NodeFencer create(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public boolean fence(org.apache.hadoop.ha.HAServiceTarget);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolVersionsResponseProto$1.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.class": "Compiled from \"AbstractDelegationTokenSecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager<TokenIdent extends org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier> extends org.apache.hadoop.security.token.SecretManager<TokenIdent> {\n  protected final java.util.Map<TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation> currentTokens;\n  protected int delegationTokenSequenceNumber;\n  protected final java.util.Map<java.lang.Integer, org.apache.hadoop.security.token.delegation.DelegationKey> allKeys;\n  protected int currentId;\n  protected boolean storeTokenTrackingId;\n  protected volatile boolean running;\n  protected java.lang.Object noInterruptsLock;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager(long, long, long, long);\n  public void startThreads() throws java.io.IOException;\n  public synchronized void reset();\n  public synchronized void addKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  public synchronized org.apache.hadoop.security.token.delegation.DelegationKey[] getAllKeys();\n  protected void logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void logExpireToken(TokenIdent) throws java.io.IOException;\n  protected void storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey);\n  protected void storeNewToken(TokenIdent, long) throws java.io.IOException;\n  protected void removeStoredToken(TokenIdent) throws java.io.IOException;\n  protected void updateStoredToken(TokenIdent, long) throws java.io.IOException;\n  protected synchronized int getCurrentKeyId();\n  protected synchronized int incrementCurrentKeyId();\n  protected synchronized void setCurrentKeyId(int);\n  protected synchronized int getDelegationTokenSeqNum();\n  protected synchronized int incrementDelegationTokenSeqNum();\n  protected synchronized void setDelegationTokenSeqNum(int);\n  protected org.apache.hadoop.security.token.delegation.DelegationKey getDelegationKey(int);\n  protected void storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected void updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey) throws java.io.IOException;\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getTokenInfo(TokenIdent);\n  protected void storeToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  protected void updateToken(TokenIdent, org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation) throws java.io.IOException;\n  public synchronized void addPersistedDelegationToken(TokenIdent, long) throws java.io.IOException;\n  void rollMasterKey() throws java.io.IOException;\n  protected synchronized byte[] createPassword(TokenIdent);\n  protected org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation checkToken(TokenIdent) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public synchronized byte[] retrievePassword(TokenIdent) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  protected java.lang.String getTrackingIdIfEnabled(TokenIdent);\n  public synchronized java.lang.String getTokenTrackingId(TokenIdent);\n  public synchronized void verifyToken(TokenIdent, byte[]) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public synchronized long renewToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, java.io.IOException;\n  public synchronized TokenIdent cancelToken(org.apache.hadoop.security.token.Token<TokenIdent>, java.lang.String) throws java.io.IOException;\n  public static javax.crypto.SecretKey createSecretKey(byte[]);\n  public void stopThreads();\n  public synchronized boolean isRunning();\n  public TokenIdent decodeTokenIdentifier(org.apache.hadoop.security.token.Token<TokenIdent>) throws java.io.IOException;\n  public byte[] retrievePassword(org.apache.hadoop.security.token.TokenIdentifier) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  protected byte[] createPassword(org.apache.hadoop.security.token.TokenIdentifier);\n  static long access$100(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  static org.apache.commons.logging.Log access$200();\n  static long access$300(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager);\n  static void access$400(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ClientId.class": "Compiled from \"ClientId.java\"\npublic class org.apache.hadoop.ipc.ClientId {\n  public static final int BYTE_LENGTH;\n  public org.apache.hadoop.ipc.ClientId();\n  public static byte[] getClientId();\n  public static java.lang.String toString(byte[]);\n  public static long getMsb(byte[]);\n  public static long getLsb(byte[]);\n  public static byte[] toBytes(java.lang.String);\n}\n", 
  "org/apache/hadoop/record/compiler/CodeBuffer.class": "Compiled from \"CodeBuffer.java\"\npublic class org.apache.hadoop.record.compiler.CodeBuffer {\n  static void addMarkers(char, char);\n  org.apache.hadoop.record.compiler.CodeBuffer();\n  org.apache.hadoop.record.compiler.CodeBuffer(java.lang.String);\n  org.apache.hadoop.record.compiler.CodeBuffer(int, java.lang.String);\n  void append(java.lang.String);\n  void append(char);\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/InodeTree.class": "Compiled from \"InodeTree.java\"\nabstract class org.apache.hadoop.fs.viewfs.InodeTree<T> {\n  static final org.apache.hadoop.fs.Path SlashPath;\n  final org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T> root;\n  final java.lang.String homedirPrefix;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> mountPoints;\n  static final boolean $assertionsDisabled;\n  static java.lang.String[] breakIntoPathComponents(java.lang.String);\n  protected abstract T getTargetFileSystem(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, java.io.IOException;\n  protected abstract T getTargetFileSystem(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T>) throws java.net.URISyntaxException;\n  protected abstract T getTargetFileSystem(java.net.URI[]) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException;\n  protected org.apache.hadoop.fs.viewfs.InodeTree(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.IOException;\n  org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult<T> resolve(java.lang.String, boolean) throws java.io.FileNotFoundException;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> getMountPoints();\n  java.lang.String getHomeDirPrefixValue();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HealthMonitor$MonitorDaemon$1.class": "Compiled from \"HealthMonitor.java\"\npublic class org.apache.hadoop.ha.HealthMonitor {\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.HealthMonitor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  public void addCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public void removeCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public synchronized void addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public synchronized void removeServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public void shutdown();\n  public synchronized org.apache.hadoop.ha.HAServiceProtocol getProxy();\n  protected org.apache.hadoop.ha.HAServiceProtocol createProxy() throws java.io.IOException;\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getHealthState();\n  synchronized org.apache.hadoop.ha.HAServiceStatus getLastServiceStatus();\n  boolean isAlive();\n  void join() throws java.lang.InterruptedException;\n  void start();\n  static org.apache.hadoop.ha.HAServiceTarget access$100(org.apache.hadoop.ha.HealthMonitor);\n  static org.apache.commons.logging.Log access$200();\n  static void access$300(org.apache.hadoop.ha.HealthMonitor, org.apache.hadoop.ha.HealthMonitor$State);\n  static boolean access$400(org.apache.hadoop.ha.HealthMonitor);\n  static void access$500(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static void access$600(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/DataChecksum$ChecksumNull.class": "Compiled from \"DataChecksum.java\"\npublic class org.apache.hadoop.util.DataChecksum implements java.util.zip.Checksum {\n  public static final int CHECKSUM_NULL;\n  public static final int CHECKSUM_CRC32;\n  public static final int CHECKSUM_CRC32C;\n  public static final int CHECKSUM_DEFAULT;\n  public static final int CHECKSUM_MIXED;\n  public static final int SIZE_OF_INTEGER;\n  public static java.util.zip.Checksum newCrc32();\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(org.apache.hadoop.util.DataChecksum$Type, int);\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(byte[], int);\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(java.io.DataInputStream) throws java.io.IOException;\n  public void writeHeader(java.io.DataOutputStream) throws java.io.IOException;\n  public byte[] getHeader();\n  public int writeValue(java.io.DataOutputStream, boolean) throws java.io.IOException;\n  public int writeValue(byte[], int, boolean) throws java.io.IOException;\n  public boolean compare(byte[], int);\n  public org.apache.hadoop.util.DataChecksum$Type getChecksumType();\n  public int getChecksumSize();\n  public int getChecksumSize(int);\n  public int getBytesPerChecksum();\n  public int getNumBytesInSum();\n  public static int getChecksumHeaderSize();\n  public long getValue();\n  public void reset();\n  public void update(byte[], int, int);\n  public void update(int);\n  public void verifyChunkedSums(java.nio.ByteBuffer, java.nio.ByteBuffer, java.lang.String, long) throws org.apache.hadoop.fs.ChecksumException;\n  public void calculateChunkedSums(java.nio.ByteBuffer, java.nio.ByteBuffer);\n  public void calculateChunkedSums(byte[], int, int, byte[], int);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/record/compiler/JBoolean$CppBoolean.class": "Compiled from \"JBoolean.java\"\npublic class org.apache.hadoop.record.compiler.JBoolean extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JBoolean();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$Writer$State.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ShutdownHookManager$1.class": "Compiled from \"ShutdownHookManager.java\"\npublic class org.apache.hadoop.util.ShutdownHookManager {\n  public static org.apache.hadoop.util.ShutdownHookManager get();\n  java.util.List<java.lang.Runnable> getShutdownHooksInOrder();\n  public void addShutdownHook(java.lang.Runnable, int);\n  public boolean removeShutdownHook(java.lang.Runnable);\n  public boolean hasShutdownHook(java.lang.Runnable);\n  public boolean isShutdownInProgress();\n  static org.apache.hadoop.util.ShutdownHookManager access$000();\n  static java.util.concurrent.atomic.AtomicBoolean access$100(org.apache.hadoop.util.ShutdownHookManager);\n  static org.apache.commons.logging.Log access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/ChmodParser.class": "Compiled from \"ChmodParser.java\"\npublic class org.apache.hadoop.fs.permission.ChmodParser extends org.apache.hadoop.fs.permission.PermissionParser {\n  public org.apache.hadoop.fs.permission.ChmodParser(java.lang.String) throws java.lang.IllegalArgumentException;\n  public short applyNewPermission(org.apache.hadoop.fs.FileStatus);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/annotation/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics2.annotation.package-info {\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsSinkAdapter$WaitableMetricsBuffer.class": "Compiled from \"MetricsSinkAdapter.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsSinkAdapter implements org.apache.hadoop.metrics2.impl.SinkQueue$Consumer<org.apache.hadoop.metrics2.impl.MetricsBuffer> {\n  org.apache.hadoop.metrics2.impl.MetricsSinkAdapter(java.lang.String, java.lang.String, org.apache.hadoop.metrics2.MetricsSink, java.lang.String, org.apache.hadoop.metrics2.MetricsFilter, org.apache.hadoop.metrics2.MetricsFilter, org.apache.hadoop.metrics2.MetricsFilter, int, int, int, float, int);\n  boolean putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer, long);\n  public boolean putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer);\n  void publishMetricsFromQueue();\n  public void consume(org.apache.hadoop.metrics2.impl.MetricsBuffer);\n  void start();\n  void stop();\n  java.lang.String name();\n  java.lang.String description();\n  void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  org.apache.hadoop.metrics2.MetricsSink sink();\n  public void consume(java.lang.Object) throws java.lang.InterruptedException;\n}\n", 
  "org/apache/hadoop/fs/FileContext$2.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableGaugeLong.class": "Compiled from \"MutableGaugeLong.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableGaugeLong extends org.apache.hadoop.metrics2.lib.MutableGauge {\n  org.apache.hadoop.metrics2.lib.MutableGaugeLong(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public long value();\n  public void incr();\n  public void incr(long);\n  public void decr();\n  public void decr(long);\n  public void set(long);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n}\n", 
  "org/apache/hadoop/io/IOUtils$NullOutputStream.class": "Compiled from \"IOUtils.java\"\npublic class org.apache.hadoop.io.IOUtils {\n  public org.apache.hadoop.io.IOUtils();\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, int, boolean) throws java.io.IOException;\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, int) throws java.io.IOException;\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public static void copyBytes(java.io.InputStream, java.io.OutputStream, long, boolean) throws java.io.IOException;\n  public static int wrappedReadForCompressedData(java.io.InputStream, byte[], int, int) throws java.io.IOException;\n  public static void readFully(java.io.InputStream, byte[], int, int) throws java.io.IOException;\n  public static void skipFully(java.io.InputStream, long) throws java.io.IOException;\n  public static void cleanup(org.apache.commons.logging.Log, java.io.Closeable...);\n  public static void closeStream(java.io.Closeable);\n  public static void closeSocket(java.net.Socket);\n  public static void writeFully(java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  public static void writeFully(java.nio.channels.FileChannel, java.nio.ByteBuffer, long) throws java.io.IOException;\n  public static java.util.List<java.lang.String> listDirectory(java.io.File, java.io.FilenameFilter) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/crypto/key/CachingKeyProvider.class": "Compiled from \"CachingKeyProvider.java\"\npublic class org.apache.hadoop.crypto.key.CachingKeyProvider extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension> {\n  public org.apache.hadoop.crypto.key.CachingKeyProvider(org.apache.hadoop.crypto.key.KeyProvider, long, long);\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/SaslInputStream.class": "Compiled from \"SaslInputStream.java\"\npublic class org.apache.hadoop.security.SaslInputStream extends java.io.InputStream implements java.nio.channels.ReadableByteChannel {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.SaslInputStream(java.io.InputStream, javax.security.sasl.SaslServer);\n  public org.apache.hadoop.security.SaslInputStream(java.io.InputStream, javax.security.sasl.SaslClient);\n  public int read() throws java.io.IOException;\n  public int read(byte[]) throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public long skip(long) throws java.io.IOException;\n  public int available() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean markSupported();\n  public boolean isOpen();\n  public int read(java.nio.ByteBuffer) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/GzipCodec$GzipOutputStream.class": "Compiled from \"GzipCodec.java\"\npublic class org.apache.hadoop.io.compress.GzipCodec extends org.apache.hadoop.io.compress.DefaultCodec {\n  public org.apache.hadoop.io.compress.GzipCodec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.DirectDecompressor createDirectDecompressor();\n  public java.lang.String getDefaultExtension();\n}\n", 
  "org/apache/hadoop/fs/FSInputStream.class": "Compiled from \"FSInputStream.java\"\npublic abstract class org.apache.hadoop.fs.FSInputStream extends java.io.InputStream implements org.apache.hadoop.fs.Seekable,org.apache.hadoop.fs.PositionedReadable {\n  public org.apache.hadoop.fs.FSInputStream();\n  public abstract void seek(long) throws java.io.IOException;\n  public abstract long getPos() throws java.io.IOException;\n  public abstract boolean seekToNewSource(long) throws java.io.IOException;\n  public int read(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[]) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/shell/find/Find$2.class": "Compiled from \"Find.java\"\npublic class org.apache.hadoop.fs.shell.find.Find extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.shell.find.Find();\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  void setRootExpression(org.apache.hadoop.fs.shell.find.Expression);\n  org.apache.hadoop.fs.shell.find.Expression getRootExpression();\n  org.apache.hadoop.fs.shell.find.FindOptions getOptions();\n  protected void recursePath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected boolean isPathRecursable(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void postProcessPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$CedeActiveRequestProto$Builder.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/HarFs.class": "Compiled from \"HarFs.java\"\npublic class org.apache.hadoop.fs.HarFs extends org.apache.hadoop.fs.DelegateToFileSystem {\n  org.apache.hadoop.fs.HarFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  public int getUriDefaultPort();\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$CedeActiveRequestProto$1.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslRpcClient$WrappedOutputStream.class": "Compiled from \"SaslRpcClient.java\"\npublic class org.apache.hadoop.security.SaslRpcClient {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.SaslRpcClient(org.apache.hadoop.security.UserGroupInformation, java.lang.Class<?>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration);\n  public java.lang.Object getNegotiatedProperty(java.lang.String);\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod getAuthMethod();\n  java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth) throws java.io.IOException;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod saslConnect(java.io.InputStream, java.io.OutputStream) throws java.io.IOException;\n  public java.io.InputStream getInputStream(java.io.InputStream) throws java.io.IOException;\n  public java.io.OutputStream getOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public void dispose() throws javax.security.sasl.SaslException;\n  static javax.security.sasl.SaslClient access$000(org.apache.hadoop.security.SaslRpcClient);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/security/AnnotatedSecurityInfo.class": "Compiled from \"AnnotatedSecurityInfo.java\"\npublic class org.apache.hadoop.security.AnnotatedSecurityInfo extends org.apache.hadoop.security.SecurityInfo {\n  public org.apache.hadoop.security.AnnotatedSecurityInfo();\n  public org.apache.hadoop.security.KerberosInfo getKerberosInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.security.token.TokenInfo getTokenInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/fs/shell/find/FindOptions.class": "Compiled from \"FindOptions.java\"\npublic class org.apache.hadoop.fs.shell.find.FindOptions {\n  public org.apache.hadoop.fs.shell.find.FindOptions();\n  public void setOut(java.io.PrintStream);\n  public java.io.PrintStream getOut();\n  public void setErr(java.io.PrintStream);\n  public java.io.PrintStream getErr();\n  public void setIn(java.io.InputStream);\n  public java.io.InputStream getIn();\n  public void setDepthFirst(boolean);\n  public boolean isDepthFirst();\n  public void setFollowLink(boolean);\n  public boolean isFollowLink();\n  public void setFollowArgLink(boolean);\n  public boolean isFollowArgLink();\n  public long getStartTime();\n  public void setStartTime(long);\n  public int getMinDepth();\n  public void setMinDepth(int);\n  public int getMaxDepth();\n  public void setMaxDepth(int);\n  public void setCommandFactory(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.shell.CommandFactory getCommandFactory();\n  public void setConfiguration(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n}\n", 
  "org/apache/hadoop/fs/shell/FsUsage$Df.class": "Compiled from \"FsUsage.java\"\nclass org.apache.hadoop.fs.shell.FsUsage extends org.apache.hadoop.fs.shell.FsCommand {\n  protected boolean humanReadable;\n  protected org.apache.hadoop.fs.shell.FsUsage$TableBuilder usagesTable;\n  org.apache.hadoop.fs.shell.FsUsage();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected java.lang.String formatSize(long);\n}\n", 
  "org/apache/hadoop/io/SequenceFile$CompressionType.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RPC.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/util/NativeCrc32.class": "Compiled from \"NativeCrc32.java\"\nclass org.apache.hadoop.util.NativeCrc32 {\n  public static final int CHECKSUM_CRC32;\n  public static final int CHECKSUM_CRC32C;\n  org.apache.hadoop.util.NativeCrc32();\n  public static boolean isAvailable();\n  public static void verifyChunkedSums(int, int, java.nio.ByteBuffer, java.nio.ByteBuffer, java.lang.String, long) throws org.apache.hadoop.fs.ChecksumException;\n  public static void verifyChunkedSumsByteArray(int, int, byte[], int, byte[], int, int, java.lang.String, long) throws org.apache.hadoop.fs.ChecksumException;\n  public static void calculateChunkedSums(int, int, java.nio.ByteBuffer, java.nio.ByteBuffer);\n  public static void calculateChunkedSumsByteArray(int, int, byte[], int, byte[], int, int);\n  static native void nativeVerifyChunkedSums(int, int, java.nio.ByteBuffer, int, java.nio.ByteBuffer, int, int, java.lang.String, long) throws org.apache.hadoop.fs.ChecksumException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos$1.class": "Compiled from \"ProtobufRpcEngineProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1102(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RefreshHandler.class": "Compiled from \"RefreshHandler.java\"\npublic interface org.apache.hadoop.ipc.RefreshHandler {\n  public abstract org.apache.hadoop.ipc.RefreshResponse handleRefresh(java.lang.String, java.lang.String[]);\n}\n", 
  "org/apache/hadoop/net/NodeBase.class": "Compiled from \"NodeBase.java\"\npublic class org.apache.hadoop.net.NodeBase implements org.apache.hadoop.net.Node {\n  public static final char PATH_SEPARATOR;\n  public static final java.lang.String PATH_SEPARATOR_STR;\n  public static final java.lang.String ROOT;\n  protected java.lang.String name;\n  protected java.lang.String location;\n  protected int level;\n  protected org.apache.hadoop.net.Node parent;\n  public org.apache.hadoop.net.NodeBase();\n  public org.apache.hadoop.net.NodeBase(java.lang.String);\n  public org.apache.hadoop.net.NodeBase(java.lang.String, java.lang.String);\n  public org.apache.hadoop.net.NodeBase(java.lang.String, java.lang.String, org.apache.hadoop.net.Node, int);\n  public java.lang.String getName();\n  public java.lang.String getNetworkLocation();\n  public void setNetworkLocation(java.lang.String);\n  public static java.lang.String getPath(org.apache.hadoop.net.Node);\n  public java.lang.String toString();\n  public static java.lang.String normalize(java.lang.String);\n  public org.apache.hadoop.net.Node getParent();\n  public void setParent(org.apache.hadoop.net.Node);\n  public int getLevel();\n  public void setLevel(int);\n  public static int locationToDepth(java.lang.String);\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$Invoker.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsRecordFiltered$1$1.class": "Compiled from \"MetricsRecordFiltered.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsRecordFiltered extends org.apache.hadoop.metrics2.impl.AbstractMetricsRecord {\n  org.apache.hadoop.metrics2.impl.MetricsRecordFiltered(org.apache.hadoop.metrics2.MetricsRecord, org.apache.hadoop.metrics2.MetricsFilter);\n  public long timestamp();\n  public java.lang.String name();\n  public java.lang.String description();\n  public java.lang.String context();\n  public java.util.Collection<org.apache.hadoop.metrics2.MetricsTag> tags();\n  public java.lang.Iterable<org.apache.hadoop.metrics2.AbstractMetric> metrics();\n  static org.apache.hadoop.metrics2.MetricsRecord access$000(org.apache.hadoop.metrics2.impl.MetricsRecordFiltered);\n  static org.apache.hadoop.metrics2.MetricsFilter access$100(org.apache.hadoop.metrics2.impl.MetricsRecordFiltered);\n}\n", 
  "org/apache/hadoop/fs/FSOutputSummer.class": "Compiled from \"FSOutputSummer.java\"\npublic abstract class org.apache.hadoop.fs.FSOutputSummer extends java.io.OutputStream {\n  protected org.apache.hadoop.fs.FSOutputSummer(org.apache.hadoop.util.DataChecksum);\n  protected abstract void writeChunk(byte[], int, int, byte[], int, int) throws java.io.IOException;\n  protected abstract void checkClosed() throws java.io.IOException;\n  public synchronized void write(int) throws java.io.IOException;\n  public synchronized void write(byte[], int, int) throws java.io.IOException;\n  protected synchronized void flushBuffer() throws java.io.IOException;\n  protected synchronized int flushBuffer(boolean, boolean) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  protected synchronized int getBufferedDataSize();\n  protected int getChecksumSize();\n  public static byte[] convertToByteStream(java.util.zip.Checksum, int);\n  static byte[] int2byte(int, byte[]);\n  protected synchronized void setChecksumBufSize(int);\n  protected synchronized void resetChecksumBufSize();\n}\n", 
  "org/apache/hadoop/fs/shell/PathData$1.class": "Compiled from \"PathData.java\"\npublic class org.apache.hadoop.fs.shell.PathData implements java.lang.Comparable<org.apache.hadoop.fs.shell.PathData> {\n  protected final java.net.URI uri;\n  public final org.apache.hadoop.fs.FileSystem fs;\n  public final org.apache.hadoop.fs.Path path;\n  public org.apache.hadoop.fs.FileStatus stat;\n  public boolean exists;\n  public org.apache.hadoop.fs.shell.PathData(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus refreshStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData suffix(java.lang.String) throws java.io.IOException;\n  public boolean parentExists() throws java.io.IOException;\n  public boolean representsDirectory();\n  public org.apache.hadoop.fs.shell.PathData[] getDirectoryContents() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.PathData getPathDataForChild(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  public static org.apache.hadoop.fs.shell.PathData[] expandAsGlob(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String toString();\n  public java.io.File toFile();\n  public int compareTo(org.apache.hadoop.fs.shell.PathData);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RpcClientUtil.class": "Compiled from \"RpcClientUtil.java\"\npublic class org.apache.hadoop.ipc.RpcClientUtil {\n  public org.apache.hadoop.ipc.RpcClientUtil();\n  public static boolean isMethodSupported(java.lang.Object, java.lang.Class<?>, org.apache.hadoop.ipc.RPC$RpcKind, long, java.lang.String) throws java.io.IOException;\n  public static java.lang.String methodToTraceString(java.lang.reflect.Method);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/BlockCompressorStream.class": "Compiled from \"BlockCompressorStream.java\"\npublic class org.apache.hadoop.io.compress.BlockCompressorStream extends org.apache.hadoop.io.compress.CompressorStream {\n  public org.apache.hadoop.io.compress.BlockCompressorStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor, int, int);\n  public org.apache.hadoop.io.compress.BlockCompressorStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor);\n  public void write(byte[], int, int) throws java.io.IOException;\n  public void finish() throws java.io.IOException;\n  protected void compress() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$UserInformationProto$1.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceUtils.class": "Compiled from \"TraceUtils.java\"\npublic class org.apache.hadoop.tracing.TraceUtils {\n  public org.apache.hadoop.tracing.TraceUtils();\n  public static org.apache.htrace.HTraceConfiguration wrapHadoopConf(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public static org.apache.htrace.HTraceConfiguration wrapHadoopConf(java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.tracing.SpanReceiverInfo$ConfigurationPair>);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/generated/TokenMgrError.class": "Compiled from \"TokenMgrError.java\"\npublic class org.apache.hadoop.record.compiler.generated.TokenMgrError extends java.lang.Error {\n  static final int LEXICAL_ERROR;\n  static final int STATIC_LEXER_ERROR;\n  static final int INVALID_LEXICAL_STATE;\n  static final int LOOP_DETECTED;\n  int errorCode;\n  protected static final java.lang.String addEscapes(java.lang.String);\n  protected static java.lang.String LexicalError(boolean, int, int, int, java.lang.String, char);\n  public java.lang.String getMessage();\n  public org.apache.hadoop.record.compiler.generated.TokenMgrError();\n  public org.apache.hadoop.record.compiler.generated.TokenMgrError(java.lang.String, int);\n  public org.apache.hadoop.record.compiler.generated.TokenMgrError(boolean, int, int, int, java.lang.String, char, int);\n}\n", 
  "org/apache/hadoop/metrics2/impl/SinkQueue$Consumer.class": "Compiled from \"SinkQueue.java\"\nclass org.apache.hadoop.metrics2.impl.SinkQueue<T> {\n  org.apache.hadoop.metrics2.impl.SinkQueue(int);\n  synchronized boolean enqueue(T);\n  void consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer<T>) throws java.lang.InterruptedException;\n  void consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer<T>) throws java.lang.InterruptedException;\n  synchronized T dequeue() throws java.lang.InterruptedException;\n  synchronized T front();\n  synchronized T back();\n  synchronized void clear();\n  synchronized int size();\n  int capacity();\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns$Tags.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController$2.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/PositionedReadable.class": "Compiled from \"PositionedReadable.java\"\npublic interface org.apache.hadoop.fs.PositionedReadable {\n  public abstract int read(long, byte[], int, int) throws java.io.IOException;\n  public abstract void readFully(long, byte[], int, int) throws java.io.IOException;\n  public abstract void readFully(long, byte[]) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolServerSideTranslatorPB.class": "Compiled from \"RefreshCallQueueProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB implements org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB {\n  public org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB(org.apache.hadoop.ipc.RefreshCallQueueProtocol);\n  public org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto refreshCallQueue(com.google.protobuf.RpcController, org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto) throws com.google.protobuf.ServiceException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$CedeActiveResponseProto.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto$Builder.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SetFile.class": "Compiled from \"SetFile.java\"\npublic class org.apache.hadoop.io.SetFile extends org.apache.hadoop.io.MapFile {\n  protected org.apache.hadoop.io.SetFile();\n}\n", 
  "org/apache/hadoop/util/AsyncDiskService.class": "Compiled from \"AsyncDiskService.java\"\npublic class org.apache.hadoop.util.AsyncDiskService {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.util.AsyncDiskService(java.lang.String[]);\n  public synchronized void execute(java.lang.String, java.lang.Runnable);\n  public synchronized void shutdown();\n  public synchronized boolean awaitTermination(long) throws java.lang.InterruptedException;\n  public synchronized java.util.List<java.lang.Runnable> shutdownNow();\n  static java.lang.ThreadGroup access$000(org.apache.hadoop.util.AsyncDiskService);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshProtocolService$BlockingStub.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter$1.class": "Compiled from \"DelegationTokenAuthenticationFilter.java\"\npublic class org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter extends org.apache.hadoop.security.authentication.server.AuthenticationFilter {\n  public static final java.lang.String DELEGATION_TOKEN_SECRET_MANAGER_ATTR;\n  public static final java.lang.String PROXYUSER_PREFIX;\n  public org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter();\n  protected java.util.Properties getConfiguration(java.lang.String, javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void setAuthHandlerClass(java.util.Properties) throws javax.servlet.ServletException;\n  protected org.apache.hadoop.conf.Configuration getProxyuserConfiguration(javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  public void init(javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void initializeAuthHandler(java.lang.String, javax.servlet.FilterConfig) throws javax.servlet.ServletException;\n  protected void setHandlerAuthMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  static java.lang.String getDoAs(javax.servlet.http.HttpServletRequest);\n  static org.apache.hadoop.security.UserGroupInformation getHttpUserGroupInformationInContext();\n  protected void doFilter(javax.servlet.FilterChain, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n  static org.apache.hadoop.security.SaslRpcServer$AuthMethod access$000(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ProtoUtil$1.class": "Compiled from \"ProtoUtil.java\"\npublic abstract class org.apache.hadoop.util.ProtoUtil {\n  public org.apache.hadoop.util.ProtoUtil();\n  public static int readRawVarint32(java.io.DataInput) throws java.io.IOException;\n  public static org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto makeIpcConnectionContext(java.lang.String, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto);\n  public static org.apache.hadoop.security.UserGroupInformation getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto convert(org.apache.hadoop.ipc.RPC$RpcKind);\n  public static org.apache.hadoop.ipc.RPC$RpcKind convert(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto, int, int, byte[]);\n}\n", 
  "org/apache/hadoop/io/DataInputByteBuffer.class": "Compiled from \"DataInputByteBuffer.java\"\npublic class org.apache.hadoop.io.DataInputByteBuffer extends java.io.DataInputStream {\n  public org.apache.hadoop.io.DataInputByteBuffer();\n  public void reset(java.nio.ByteBuffer...);\n  public java.nio.ByteBuffer[] getData();\n  public int getPosition();\n  public int getLength();\n}\n", 
  "org/apache/hadoop/io/compress/zlib/ZlibDecompressor$CompressionHeader.class": "Compiled from \"ZlibDecompressor.java\"\npublic class org.apache.hadoop.io.compress.zlib.ZlibDecompressor implements org.apache.hadoop.io.compress.Decompressor {\n  static final boolean $assertionsDisabled;\n  static boolean isNativeZlibLoaded();\n  public org.apache.hadoop.io.compress.zlib.ZlibDecompressor(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader, int);\n  public org.apache.hadoop.io.compress.zlib.ZlibDecompressor();\n  public void setInput(byte[], int, int);\n  void setInputFromSavedData();\n  public void setDictionary(byte[], int, int);\n  public boolean needsInput();\n  public boolean needsDictionary();\n  public boolean finished();\n  public int decompress(byte[], int, int) throws java.io.IOException;\n  public long getBytesWritten();\n  public long getBytesRead();\n  public int getRemaining();\n  public void reset();\n  public void end();\n  protected void finalize();\n  int inflateDirect(java.nio.ByteBuffer, java.nio.ByteBuffer) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/util/MetricsCache$RecordCache.class": "Compiled from \"MetricsCache.java\"\npublic class org.apache.hadoop.metrics2.util.MetricsCache {\n  static final org.apache.commons.logging.Log LOG;\n  static final int MAX_RECS_PER_NAME_DEFAULT;\n  public org.apache.hadoop.metrics2.util.MetricsCache();\n  public org.apache.hadoop.metrics2.util.MetricsCache(int);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record update(org.apache.hadoop.metrics2.MetricsRecord, boolean);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record update(org.apache.hadoop.metrics2.MetricsRecord);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record get(java.lang.String, java.util.Collection<org.apache.hadoop.metrics2.MetricsTag>);\n  static int access$000(org.apache.hadoop.metrics2.util.MetricsCache);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Options$ClassOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider$1.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/tools/TableListing$Builder.class": "Compiled from \"TableListing.java\"\npublic class org.apache.hadoop.tools.TableListing {\n  org.apache.hadoop.tools.TableListing(org.apache.hadoop.tools.TableListing$Column[], boolean, int);\n  public void addRow(java.lang.String...);\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToStandbyResponseProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/token/SecretManager$InvalidToken.class": "Compiled from \"SecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.SecretManager<T extends org.apache.hadoop.security.token.TokenIdentifier> {\n  public org.apache.hadoop.security.token.SecretManager();\n  protected abstract byte[] createPassword(T);\n  public abstract byte[] retrievePassword(T) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public byte[] retriableRetrievePassword(T) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, org.apache.hadoop.ipc.StandbyException, org.apache.hadoop.ipc.RetriableException, java.io.IOException;\n  public abstract T createIdentifier();\n  public void checkAvailableForRead() throws org.apache.hadoop.ipc.StandbyException;\n  protected javax.crypto.SecretKey generateSecret();\n  protected static byte[] createPassword(byte[], javax.crypto.SecretKey);\n  protected static javax.crypto.SecretKey createSecretKey(byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.io.compress.bzip2.package-info {\n}\n", 
  "org/apache/hadoop/metrics2/util/MetricsCache$Record.class": "Compiled from \"MetricsCache.java\"\npublic class org.apache.hadoop.metrics2.util.MetricsCache {\n  static final org.apache.commons.logging.Log LOG;\n  static final int MAX_RECS_PER_NAME_DEFAULT;\n  public org.apache.hadoop.metrics2.util.MetricsCache();\n  public org.apache.hadoop.metrics2.util.MetricsCache(int);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record update(org.apache.hadoop.metrics2.MetricsRecord, boolean);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record update(org.apache.hadoop.metrics2.MetricsRecord);\n  public org.apache.hadoop.metrics2.util.MetricsCache$Record get(java.lang.String, java.util.Collection<org.apache.hadoop.metrics2.MetricsTag>);\n  static int access$000(org.apache.hadoop.metrics2.util.MetricsCache);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderExtension.class": "Compiled from \"KeyProviderExtension.java\"\npublic abstract class org.apache.hadoop.crypto.key.KeyProviderExtension<E extends org.apache.hadoop.crypto.key.KeyProviderExtension$Extension> extends org.apache.hadoop.crypto.key.KeyProvider {\n  public org.apache.hadoop.crypto.key.KeyProviderExtension(org.apache.hadoop.crypto.key.KeyProvider, E);\n  protected E getExtension();\n  protected org.apache.hadoop.crypto.key.KeyProvider getKeyProvider();\n  public boolean isTransient();\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.class": "Compiled from \"JniBasedUnixGroupsMappingWithFallback.java\"\npublic class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback implements org.apache.hadoop.security.GroupMappingServiceProvider {\n  public org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback();\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void cacheGroupsRefresh() throws java.io.IOException;\n  public void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$Server$ProtoBufRpcInvoker.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/io/UTF8.class": "Compiled from \"UTF8.java\"\npublic class org.apache.hadoop.io.UTF8 implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.UTF8> {\n  public org.apache.hadoop.io.UTF8();\n  public org.apache.hadoop.io.UTF8(java.lang.String);\n  public org.apache.hadoop.io.UTF8(org.apache.hadoop.io.UTF8);\n  public byte[] getBytes();\n  public int getLength();\n  public void set(java.lang.String);\n  public void set(org.apache.hadoop.io.UTF8);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static void skip(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public int compareTo(org.apache.hadoop.io.UTF8);\n  public java.lang.String toString();\n  public java.lang.String toStringChecked() throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static byte[] getBytes(java.lang.String);\n  public static java.lang.String fromBytes(byte[]) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static int writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolSignatureResponseProto$1.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/ValueQueue$3.class": "Compiled from \"ValueQueue.java\"\npublic class org.apache.hadoop.crypto.key.kms.ValueQueue<E> {\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public void initializeQueuesForKeys(java.lang.String...) throws java.util.concurrent.ExecutionException;\n  public E getNext(java.lang.String) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void drain(java.lang.String);\n  public int getSize(java.lang.String) throws java.util.concurrent.ExecutionException;\n  public java.util.List<E> getAtMost(java.lang.String, int) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void shutdown();\n  static int access$200(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static float access$300(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller access$400(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JCompType$CCompType.class": "Compiled from \"JCompType.java\"\nabstract class org.apache.hadoop.record.compiler.JCompType extends org.apache.hadoop.record.compiler.JType {\n  org.apache.hadoop.record.compiler.JCompType();\n}\n", 
  "org/apache/hadoop/io/LongWritable.class": "Compiled from \"LongWritable.java\"\npublic class org.apache.hadoop.io.LongWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.LongWritable> {\n  public org.apache.hadoop.io.LongWritable();\n  public org.apache.hadoop.io.LongWritable(long);\n  public void set(long);\n  public long get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.LongWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcKindProto.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricCounterLong.class": "Compiled from \"MetricCounterLong.java\"\nclass org.apache.hadoop.metrics2.impl.MetricCounterLong extends org.apache.hadoop.metrics2.AbstractMetric {\n  final long value;\n  org.apache.hadoop.metrics2.impl.MetricCounterLong(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public java.lang.Long value();\n  public org.apache.hadoop.metrics2.MetricType type();\n  public void visit(org.apache.hadoop.metrics2.MetricsVisitor);\n  public java.lang.Number value();\n}\n", 
  "org/apache/hadoop/fs/FileContext$7.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/util/StringUtils.class": "Compiled from \"StringUtils.java\"\npublic class org.apache.hadoop.util.StringUtils {\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  public static final java.util.regex.Pattern SHELL_ENV_VAR_PATTERN;\n  public static final java.util.regex.Pattern WIN_ENV_VAR_PATTERN;\n  public static final java.util.regex.Pattern ENV_VAR_PATTERN;\n  public static final java.lang.String[] emptyStringArray;\n  public static final char COMMA;\n  public static final java.lang.String COMMA_STR;\n  public static final char ESCAPE_CHAR;\n  public org.apache.hadoop.util.StringUtils();\n  public static java.lang.String stringifyException(java.lang.Throwable);\n  public static java.lang.String simpleHostname(java.lang.String);\n  public static java.lang.String humanReadableInt(long);\n  public static java.lang.String format(java.lang.String, java.lang.Object...);\n  public static java.lang.String formatPercent(double, int);\n  public static java.lang.String arrayToString(java.lang.String[]);\n  public static java.lang.String byteToHexString(byte[], int, int);\n  public static java.lang.String byteToHexString(byte[]);\n  public static byte[] hexStringToByte(java.lang.String);\n  public static java.lang.String uriToString(java.net.URI[]);\n  public static java.net.URI[] stringToURI(java.lang.String[]);\n  public static org.apache.hadoop.fs.Path[] stringToPath(java.lang.String[]);\n  public static java.lang.String formatTimeDiff(long, long);\n  public static java.lang.String formatTime(long);\n  public static java.lang.String getFormattedTimeWithDiff(java.text.DateFormat, long, long);\n  public static java.lang.String[] getStrings(java.lang.String);\n  public static java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public static java.util.Collection<java.lang.String> getStringCollection(java.lang.String, java.lang.String);\n  public static java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public static java.lang.String[] getTrimmedStrings(java.lang.String);\n  public static java.util.Set<java.lang.String> getTrimmedStrings(java.util.Collection<java.lang.String>);\n  public static java.lang.String[] split(java.lang.String);\n  public static java.lang.String[] split(java.lang.String, char, char);\n  public static java.lang.String[] split(java.lang.String, char);\n  public static int findNext(java.lang.String, char, char, int, java.lang.StringBuilder);\n  public static java.lang.String escapeString(java.lang.String);\n  public static java.lang.String escapeString(java.lang.String, char, char);\n  public static java.lang.String escapeString(java.lang.String, char, char[]);\n  public static java.lang.String unEscapeString(java.lang.String);\n  public static java.lang.String unEscapeString(java.lang.String, char, char);\n  public static java.lang.String unEscapeString(java.lang.String, char, char[]);\n  public static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.apache.commons.logging.Log);\n  public static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.slf4j.Logger);\n  static void startupShutdownMessage(java.lang.Class<?>, java.lang.String[], org.apache.hadoop.util.LogAdapter);\n  public static java.lang.String escapeHTML(java.lang.String);\n  public static java.lang.String byteDesc(long);\n  public static java.lang.String limitDecimalTo2(double);\n  public static java.lang.String join(java.lang.CharSequence, java.lang.Iterable<?>);\n  public static java.lang.String join(java.lang.CharSequence, java.lang.String[]);\n  public static java.lang.String camelize(java.lang.String);\n  public static java.lang.String replaceTokens(java.lang.String, java.util.regex.Pattern, java.util.Map<java.lang.String, java.lang.String>);\n  public static java.lang.String getStackTrace(java.lang.Thread);\n  public static java.lang.String popOptionWithArgument(java.lang.String, java.util.List<java.lang.String>) throws java.lang.IllegalArgumentException;\n  public static boolean popOption(java.lang.String, java.util.List<java.lang.String>);\n  public static java.lang.String popFirstNonOption(java.util.List<java.lang.String>);\n  public static java.lang.String toLowerCase(java.lang.String);\n  public static java.lang.String toUpperCase(java.lang.String);\n  public static boolean equalsIgnoreCase(java.lang.String, java.lang.String);\n  static java.lang.String access$000(java.lang.String, java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileUtil$HardLink.class": "Compiled from \"FileUtil.java\"\npublic class org.apache.hadoop.fs.FileUtil {\n  public static final int SYMLINK_NO_PRIVILEGE;\n  public org.apache.hadoop.fs.FileUtil();\n  public static org.apache.hadoop.fs.Path[] stat2Paths(org.apache.hadoop.fs.FileStatus[]);\n  public static org.apache.hadoop.fs.Path[] stat2Paths(org.apache.hadoop.fs.FileStatus[], org.apache.hadoop.fs.Path);\n  public static boolean fullyDelete(java.io.File);\n  public static boolean fullyDelete(java.io.File, boolean);\n  public static java.lang.String readLink(java.io.File);\n  public static boolean fullyDeleteContents(java.io.File);\n  public static boolean fullyDeleteContents(java.io.File, boolean);\n  public static void fullyDelete(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copyMerge(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public static boolean copy(java.io.File, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean copy(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.io.File, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.lang.String makeShellPath(java.lang.String) throws java.io.IOException;\n  public static java.lang.String makeShellPath(java.io.File) throws java.io.IOException;\n  public static java.lang.String makeShellPath(java.io.File, boolean) throws java.io.IOException;\n  public static long getDU(java.io.File);\n  public static void unZip(java.io.File, java.io.File) throws java.io.IOException;\n  public static void unTar(java.io.File, java.io.File) throws java.io.IOException;\n  public static int symLink(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static int chmod(java.lang.String, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static int chmod(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public static void setOwner(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static boolean setReadable(java.io.File, boolean);\n  public static boolean setWritable(java.io.File, boolean);\n  public static boolean setExecutable(java.io.File, boolean);\n  public static boolean canRead(java.io.File);\n  public static boolean canWrite(java.io.File);\n  public static boolean canExecute(java.io.File);\n  public static void setPermission(java.io.File, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  static java.lang.String execCommand(java.io.File, java.lang.String...) throws java.io.IOException;\n  public static final java.io.File createLocalTempFile(java.io.File, java.lang.String, boolean) throws java.io.IOException;\n  public static void replaceFile(java.io.File, java.io.File) throws java.io.IOException;\n  public static java.io.File[] listFiles(java.io.File) throws java.io.IOException;\n  public static java.lang.String[] list(java.io.File) throws java.io.IOException;\n  public static java.lang.String[] createJarWithClassPath(java.lang.String, org.apache.hadoop.fs.Path, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public static java.lang.String[] createJarWithClassPath(java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/PermissionStatus$1.class": "Compiled from \"PermissionStatus.java\"\npublic class org.apache.hadoop.fs.permission.PermissionStatus implements org.apache.hadoop.io.Writable {\n  static final org.apache.hadoop.io.WritableFactory FACTORY;\n  public static org.apache.hadoop.fs.permission.PermissionStatus createImmutable(java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.permission.PermissionStatus(java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission);\n  public java.lang.String getUserName();\n  public java.lang.String getGroupName();\n  public org.apache.hadoop.fs.permission.FsPermission getPermission();\n  public org.apache.hadoop.fs.permission.PermissionStatus applyUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public static org.apache.hadoop.fs.permission.PermissionStatus read(java.io.DataInput) throws java.io.IOException;\n  public static void write(java.io.DataOutput, java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public java.lang.String toString();\n  org.apache.hadoop.fs.permission.PermissionStatus(org.apache.hadoop.fs.permission.PermissionStatus$1);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsTag.class": "Compiled from \"MetricsTag.java\"\npublic class org.apache.hadoop.metrics2.MetricsTag implements org.apache.hadoop.metrics2.MetricsInfo {\n  public org.apache.hadoop.metrics2.MetricsTag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public java.lang.String name();\n  public java.lang.String description();\n  public org.apache.hadoop.metrics2.MetricsInfo info();\n  public java.lang.String value();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/fs/AbstractFileSystem.class": "Compiled from \"AbstractFileSystem.java\"\npublic abstract class org.apache.hadoop.fs.AbstractFileSystem {\n  static final org.apache.commons.logging.Log LOG;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  static final java.lang.String NO_ABSTRACT_FS_ERROR;\n  public org.apache.hadoop.fs.FileSystem$Statistics getStatistics();\n  public boolean isValidName(java.lang.String);\n  static <T extends java/lang/Object> T newInstance(java.lang.Class<T>, java.net.URI, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.fs.AbstractFileSystem createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  protected static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics();\n  protected static synchronized java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static org.apache.hadoop.fs.AbstractFileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem(java.net.URI, java.lang.String, boolean, int) throws java.net.URISyntaxException;\n  public void checkScheme(java.net.URI, java.lang.String);\n  public abstract int getUriDefaultPort();\n  public java.net.URI getUri();\n  public void checkPath(org.apache.hadoop.fs.Path);\n  public java.lang.String getUriPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public final org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public final void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FsStatus getFsStatus() throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract void setVerifyChecksum(boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/ConfigUtil.class": "Compiled from \"ConfigUtil.java\"\npublic class org.apache.hadoop.fs.viewfs.ConfigUtil {\n  public org.apache.hadoop.fs.viewfs.ConfigUtil();\n  public static java.lang.String getConfigViewFsPrefix(java.lang.String);\n  public static java.lang.String getConfigViewFsPrefix();\n  public static void addLink(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.net.URI);\n  public static void addLink(org.apache.hadoop.conf.Configuration, java.lang.String, java.net.URI);\n  public static void setHomeDirConf(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public static void setHomeDirConf(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String);\n  public static java.lang.String getHomeDirValue(org.apache.hadoop.conf.Configuration);\n  public static java.lang.String getHomeDirValue(org.apache.hadoop.conf.Configuration, java.lang.String);\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserMappingsProtocolService$2.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcResponseHeaderProto.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/MergeSort.class": "Compiled from \"MergeSort.java\"\npublic class org.apache.hadoop.util.MergeSort {\n  org.apache.hadoop.io.IntWritable I;\n  org.apache.hadoop.io.IntWritable J;\n  public org.apache.hadoop.util.MergeSort(java.util.Comparator<org.apache.hadoop.io.IntWritable>);\n  public void mergeSort(int[], int[], int, int);\n}\n", 
  "org/apache/hadoop/util/bloom/DynamicBloomFilter.class": "Compiled from \"DynamicBloomFilter.java\"\npublic class org.apache.hadoop.util.bloom.DynamicBloomFilter extends org.apache.hadoop.util.bloom.Filter {\n  public org.apache.hadoop.util.bloom.DynamicBloomFilter();\n  public org.apache.hadoop.util.bloom.DynamicBloomFilter(int, int, int, int);\n  public void add(org.apache.hadoop.util.bloom.Key);\n  public void and(org.apache.hadoop.util.bloom.Filter);\n  public boolean membershipTest(org.apache.hadoop.util.bloom.Key);\n  public void not();\n  public void or(org.apache.hadoop.util.bloom.Filter);\n  public void xor(org.apache.hadoop.util.bloom.Filter);\n  public java.lang.String toString();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/FloatWritable$Comparator.class": "Compiled from \"FloatWritable.java\"\npublic class org.apache.hadoop.io.FloatWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.FloatWritable> {\n  public org.apache.hadoop.io.FloatWritable();\n  public org.apache.hadoop.io.FloatWritable(float);\n  public void set(float);\n  public float get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.FloatWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$ProtocolSignatureProto.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$Handler$1.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyShell$DeleteCommand.class": "Compiled from \"KeyShell.java\"\npublic class org.apache.hadoop.crypto.key.KeyShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.crypto.key.KeyShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.crypto.key.KeyShell);\n  static boolean access$300(org.apache.hadoop.crypto.key.KeyShell);\n}\n", 
  "org/apache/hadoop/conf/ReconfigurationServlet.class": "Compiled from \"ReconfigurationServlet.java\"\npublic class org.apache.hadoop.conf.ReconfigurationServlet extends javax.servlet.http.HttpServlet {\n  public static final java.lang.String CONF_SERVLET_RECONFIGURABLE_PREFIX;\n  public org.apache.hadoop.conf.ReconfigurationServlet();\n  public void init() throws javax.servlet.ServletException;\n  protected void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException;\n  protected void doPost(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$ParsedTimeDuration$7.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/log/metrics/EventCounter.class": "Compiled from \"EventCounter.java\"\npublic class org.apache.hadoop.log.metrics.EventCounter extends org.apache.log4j.AppenderSkeleton {\n  public org.apache.hadoop.log.metrics.EventCounter();\n  public static long getFatal();\n  public static long getError();\n  public static long getWarn();\n  public static long getInfo();\n  public void append(org.apache.log4j.spi.LoggingEvent);\n  public void close();\n  public boolean requiresLayout();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ChecksumFs.class": "Compiled from \"ChecksumFs.java\"\npublic abstract class org.apache.hadoop.fs.ChecksumFs extends org.apache.hadoop.fs.FilterFs {\n  public static double getApproxChkSumLength(long);\n  public org.apache.hadoop.fs.ChecksumFs(org.apache.hadoop.fs.AbstractFileSystem) throws java.io.IOException, java.net.URISyntaxException;\n  public void setVerifyChecksum(boolean);\n  public org.apache.hadoop.fs.AbstractFileSystem getRawFs();\n  public org.apache.hadoop.fs.Path getChecksumFile(org.apache.hadoop.fs.Path);\n  public static boolean isChecksumFile(org.apache.hadoop.fs.Path);\n  public long getChecksumFileLength(org.apache.hadoop.fs.Path, long);\n  public int getBytesPerSum();\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public static long getChecksumLength(long, int);\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public boolean reportChecksumFailure(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataInputStream, long, org.apache.hadoop.fs.FSDataInputStream, long);\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  static int access$000(org.apache.hadoop.fs.ChecksumFs, int, int) throws java.io.IOException;\n  static byte[] access$100();\n  static boolean access$200(org.apache.hadoop.fs.ChecksumFs);\n  static {};\n}\n", 
  "org/apache/hadoop/record/XmlRecordInput$XmlIndex.class": "Compiled from \"XmlRecordInput.java\"\npublic class org.apache.hadoop.record.XmlRecordInput implements org.apache.hadoop.record.RecordInput {\n  public org.apache.hadoop.record.XmlRecordInput(java.io.InputStream);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  static int access$000(org.apache.hadoop.record.XmlRecordInput);\n  static java.util.ArrayList access$100(org.apache.hadoop.record.XmlRecordInput);\n  static int access$008(org.apache.hadoop.record.XmlRecordInput);\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsBuffer$Entry.class": "Compiled from \"MetricsBuffer.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsBuffer implements java.lang.Iterable<org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry> {\n  org.apache.hadoop.metrics2.impl.MetricsBuffer(java.lang.Iterable<org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry>);\n  public java.util.Iterator<org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry> iterator();\n}\n", 
  "org/apache/hadoop/io/SecureIOUtils$AlreadyExistsException.class": "Compiled from \"SecureIOUtils.java\"\npublic class org.apache.hadoop.io.SecureIOUtils {\n  public org.apache.hadoop.io.SecureIOUtils();\n  public static java.io.RandomAccessFile openForRandomRead(java.io.File, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  protected static java.io.RandomAccessFile forceSecureOpenForRandomRead(java.io.File, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FSDataInputStream openFSDataInputStream(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  protected static org.apache.hadoop.fs.FSDataInputStream forceSecureOpenFSDataInputStream(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.io.FileInputStream openForRead(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  protected static java.io.FileInputStream forceSecureOpenForRead(java.io.File, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.io.FileOutputStream createForWrite(java.io.File, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshRequestProto.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/TableMapping.class": "Compiled from \"TableMapping.java\"\npublic class org.apache.hadoop.net.TableMapping extends org.apache.hadoop.net.CachedDNSToSwitchMapping {\n  public org.apache.hadoop.net.TableMapping();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void reloadCachedMappings();\n  static org.apache.commons.logging.Log access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/TrashPolicyDefault$Emptier.class": "Compiled from \"TrashPolicyDefault.java\"\npublic class org.apache.hadoop.fs.TrashPolicyDefault extends org.apache.hadoop.fs.TrashPolicy {\n  public org.apache.hadoop.fs.TrashPolicyDefault();\n  public void initialize(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public boolean isEnabled();\n  public boolean moveToTrash(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void createCheckpoint() throws java.io.IOException;\n  public void deleteCheckpoint() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getCurrentTrashDir();\n  public java.lang.Runnable getEmptier() throws java.io.IOException;\n  static org.apache.commons.logging.Log access$000();\n  static org.apache.hadoop.fs.Path access$100(org.apache.hadoop.fs.TrashPolicyDefault);\n  org.apache.hadoop.fs.TrashPolicyDefault(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.TrashPolicyDefault$1) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ReadOption.class": "Compiled from \"ReadOption.java\"\npublic final class org.apache.hadoop.fs.ReadOption extends java.lang.Enum<org.apache.hadoop.fs.ReadOption> {\n  public static final org.apache.hadoop.fs.ReadOption SKIP_CHECKSUMS;\n  public static org.apache.hadoop.fs.ReadOption[] values();\n  public static org.apache.hadoop.fs.ReadOption valueOf(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RetryCache$CacheEntry.class": "Compiled from \"RetryCache.java\"\npublic class org.apache.hadoop.ipc.RetryCache {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.RetryCache(java.lang.String, double, long);\n  public void lock();\n  public void unlock();\n  public org.apache.hadoop.util.LightWeightGSet<org.apache.hadoop.ipc.RetryCache$CacheEntry, org.apache.hadoop.ipc.RetryCache$CacheEntry> getCacheSet();\n  public org.apache.hadoop.ipc.metrics.RetryCacheMetrics getMetricsForTests();\n  public java.lang.String getCacheName();\n  public void addCacheEntry(byte[], int);\n  public void addCacheEntryWithPayload(byte[], int, java.lang.Object);\n  public static org.apache.hadoop.ipc.RetryCache$CacheEntry waitForCompletion(org.apache.hadoop.ipc.RetryCache);\n  public static org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload waitForCompletion(org.apache.hadoop.ipc.RetryCache, java.lang.Object);\n  public static void setState(org.apache.hadoop.ipc.RetryCache$CacheEntry, boolean);\n  public static void setState(org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload, boolean, java.lang.Object);\n  public static void clear(org.apache.hadoop.ipc.RetryCache);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$MonitorHealthResponseProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/source/JvmMetricsInfo.class": "Compiled from \"JvmMetricsInfo.java\"\npublic final class org.apache.hadoop.metrics2.source.JvmMetricsInfo extends java.lang.Enum<org.apache.hadoop.metrics2.source.JvmMetricsInfo> implements org.apache.hadoop.metrics2.MetricsInfo {\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo JvmMetrics;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo MemNonHeapUsedM;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo MemNonHeapCommittedM;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo MemNonHeapMaxM;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo MemHeapUsedM;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo MemHeapCommittedM;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo MemHeapMaxM;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo MemMaxM;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo GcCount;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo GcTimeMillis;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo ThreadsNew;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo ThreadsRunnable;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo ThreadsBlocked;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo ThreadsWaiting;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo ThreadsTimedWaiting;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo ThreadsTerminated;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo LogFatal;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo LogError;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo LogWarn;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo LogInfo;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo GcNumWarnThresholdExceeded;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo GcNumInfoThresholdExceeded;\n  public static final org.apache.hadoop.metrics2.source.JvmMetricsInfo GcTotalExtraSleepTime;\n  public static org.apache.hadoop.metrics2.source.JvmMetricsInfo[] values();\n  public static org.apache.hadoop.metrics2.source.JvmMetricsInfo valueOf(java.lang.String);\n  public java.lang.String description();\n  public java.lang.String toString();\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JavaGenerator.class": "Compiled from \"JavaGenerator.java\"\nclass org.apache.hadoop.record.compiler.JavaGenerator extends org.apache.hadoop.record.compiler.CodeGenerator {\n  org.apache.hadoop.record.compiler.JavaGenerator();\n  void genCode(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JFile>, java.util.ArrayList<org.apache.hadoop.record.compiler.JRecord>, java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/metrics2/lib/MethodMetric.class": "Compiled from \"MethodMetric.java\"\nclass org.apache.hadoop.metrics2.lib.MethodMetric extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  org.apache.hadoop.metrics2.lib.MethodMetric(java.lang.Object, java.lang.reflect.Method, org.apache.hadoop.metrics2.MetricsInfo, org.apache.hadoop.metrics2.annotation.Metric$Type);\n  org.apache.hadoop.metrics2.lib.MutableMetric newCounter(java.lang.Class<?>);\n  static boolean isInt(java.lang.Class<?>);\n  static boolean isLong(java.lang.Class<?>);\n  static boolean isFloat(java.lang.Class<?>);\n  static boolean isDouble(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newGauge(java.lang.Class<?>);\n  org.apache.hadoop.metrics2.lib.MutableMetric newTag(java.lang.Class<?>);\n  public void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  static org.apache.hadoop.metrics2.MetricsInfo metricInfo(java.lang.reflect.Method);\n  static java.lang.String nameFrom(java.lang.reflect.Method);\n  static java.lang.Object access$000(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static java.lang.reflect.Method access$100(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.hadoop.metrics2.MetricsInfo access$200(org.apache.hadoop.metrics2.lib.MethodMetric);\n  static org.apache.commons.logging.Log access$300();\n  static {};\n}\n", 
  "org/apache/hadoop/util/PriorityQueue.class": "Compiled from \"PriorityQueue.java\"\npublic abstract class org.apache.hadoop.util.PriorityQueue<T> {\n  public org.apache.hadoop.util.PriorityQueue();\n  protected abstract boolean lessThan(java.lang.Object, java.lang.Object);\n  protected final void initialize(int);\n  public final void put(T);\n  public boolean insert(T);\n  public final T top();\n  public final T pop();\n  public final void adjustTop();\n  public final int size();\n  public final void clear();\n}\n", 
  "org/apache/hadoop/util/bloom/BloomFilter.class": "Compiled from \"BloomFilter.java\"\npublic class org.apache.hadoop.util.bloom.BloomFilter extends org.apache.hadoop.util.bloom.Filter {\n  java.util.BitSet bits;\n  public org.apache.hadoop.util.bloom.BloomFilter();\n  public org.apache.hadoop.util.bloom.BloomFilter(int, int, int);\n  public void add(org.apache.hadoop.util.bloom.Key);\n  public void and(org.apache.hadoop.util.bloom.Filter);\n  public boolean membershipTest(org.apache.hadoop.util.bloom.Key);\n  public void not();\n  public void or(org.apache.hadoop.util.bloom.Filter);\n  public void xor(org.apache.hadoop.util.bloom.Filter);\n  public java.lang.String toString();\n  public int getVectorSize();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/random/OpensslSecureRandom.class": "Compiled from \"OpensslSecureRandom.java\"\npublic class org.apache.hadoop.crypto.random.OpensslSecureRandom extends java.util.Random {\n  public static boolean isNativeCodeLoaded();\n  public org.apache.hadoop.crypto.random.OpensslSecureRandom();\n  public void nextBytes(byte[]);\n  public void setSeed(long);\n  protected final int next(int);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/Print$1.class": "Compiled from \"Print.java\"\nfinal class org.apache.hadoop.fs.shell.find.Print extends org.apache.hadoop.fs.shell.find.BaseExpression {\n  public static void registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Print();\n  public org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  public boolean isAction();\n  org.apache.hadoop.fs.shell.find.Print(java.lang.String, org.apache.hadoop.fs.shell.find.Print$1);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream$Data.class": "Compiled from \"CBZip2OutputStream.java\"\npublic class org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream extends java.io.OutputStream implements org.apache.hadoop.io.compress.bzip2.BZip2Constants {\n  public static final int MIN_BLOCKSIZE;\n  public static final int MAX_BLOCKSIZE;\n  protected static final int SETMASK;\n  protected static final int CLEARMASK;\n  protected static final int GREATER_ICOST;\n  protected static final int LESSER_ICOST;\n  protected static final int SMALL_THRESH;\n  protected static final int DEPTH_THRESH;\n  protected static final int WORK_FACTOR;\n  protected static final int QSORT_STACK_SIZE;\n  protected static void hbMakeCodeLengths(char[], int[], int, int);\n  public static int chooseBlockSize(long);\n  public org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream(java.io.OutputStream, int) throws java.io.IOException;\n  public void write(int) throws java.io.IOException;\n  protected void finalize() throws java.lang.Throwable;\n  public void finish() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public final int getBlockSize();\n  public void write(byte[], int, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$ZKFCProtocolService$1.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/Index.class": "Compiled from \"Index.java\"\npublic interface org.apache.hadoop.record.Index {\n  public abstract boolean done();\n  public abstract void incr();\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsRegistry.class": "Compiled from \"MetricsRegistry.java\"\npublic class org.apache.hadoop.metrics.util.MetricsRegistry {\n  public org.apache.hadoop.metrics.util.MetricsRegistry();\n  public int size();\n  public void add(java.lang.String, org.apache.hadoop.metrics.util.MetricsBase);\n  public org.apache.hadoop.metrics.util.MetricsBase get(java.lang.String);\n  public java.util.Collection<java.lang.String> getKeyList();\n  public java.util.Collection<org.apache.hadoop.metrics.util.MetricsBase> getMetricsList();\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Sorter$SegmentDescriptor.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/Encryptor.class": "Compiled from \"Encryptor.java\"\npublic interface org.apache.hadoop.crypto.Encryptor {\n  public abstract void init(byte[], byte[]) throws java.io.IOException;\n  public abstract boolean isContextReset();\n  public abstract void encrypt(java.nio.ByteBuffer, java.nio.ByteBuffer) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/LineReader.class": "Compiled from \"LineReader.java\"\npublic class org.apache.hadoop.util.LineReader implements java.io.Closeable {\n  public org.apache.hadoop.util.LineReader(java.io.InputStream);\n  public org.apache.hadoop.util.LineReader(java.io.InputStream, int);\n  public org.apache.hadoop.util.LineReader(java.io.InputStream, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.util.LineReader(java.io.InputStream, byte[]);\n  public org.apache.hadoop.util.LineReader(java.io.InputStream, int, byte[]);\n  public org.apache.hadoop.util.LineReader(java.io.InputStream, org.apache.hadoop.conf.Configuration, byte[]) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public int readLine(org.apache.hadoop.io.Text, int, int) throws java.io.IOException;\n  protected int fillBuffer(java.io.InputStream, byte[], boolean) throws java.io.IOException;\n  public int readLine(org.apache.hadoop.io.Text, int) throws java.io.IOException;\n  public int readLine(org.apache.hadoop.io.Text) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/service/Service$STATE.class": "Compiled from \"Service.java\"\npublic interface org.apache.hadoop.service.Service extends java.io.Closeable {\n  public abstract void init(org.apache.hadoop.conf.Configuration);\n  public abstract void start();\n  public abstract void stop();\n  public abstract void close() throws java.io.IOException;\n  public abstract void registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener);\n  public abstract void unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener);\n  public abstract java.lang.String getName();\n  public abstract org.apache.hadoop.conf.Configuration getConfig();\n  public abstract org.apache.hadoop.service.Service$STATE getServiceState();\n  public abstract long getStartTime();\n  public abstract boolean isInState(org.apache.hadoop.service.Service$STATE);\n  public abstract java.lang.Throwable getFailureCause();\n  public abstract org.apache.hadoop.service.Service$STATE getFailureState();\n  public abstract boolean waitForServiceToStop(long);\n  public abstract java.util.List<org.apache.hadoop.service.LifecycleEvent> getLifecycleHistory();\n  public abstract java.util.Map<java.lang.String, java.lang.String> getBlockers();\n}\n", 
  "org/apache/hadoop/tools/TableListing.class": "Compiled from \"TableListing.java\"\npublic class org.apache.hadoop.tools.TableListing {\n  org.apache.hadoop.tools.TableListing(org.apache.hadoop.tools.TableListing$Column[], boolean, int);\n  public void addRow(java.lang.String...);\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/fs/FileChecksum.class": "Compiled from \"FileChecksum.java\"\npublic abstract class org.apache.hadoop.fs.FileChecksum implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.fs.FileChecksum();\n  public abstract java.lang.String getAlgorithmName();\n  public abstract int getLength();\n  public abstract byte[] getBytes();\n  public org.apache.hadoop.fs.Options$ChecksumOpt getChecksumOpt();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$9.class": "Compiled from \"LoadBalancingKMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static org.slf4j.Logger LOG;\n  public org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider(org.apache.hadoop.crypto.key.kms.KMSClientProvider[], long, org.apache.hadoop.conf.Configuration);\n  org.apache.hadoop.crypto.key.kms.KMSClientProvider[] getProviders();\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/AclCommands$GetfaclCommand.class": "Compiled from \"AclCommands.java\"\nclass org.apache.hadoop.fs.shell.AclCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.AclCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  static java.lang.String access$000();\n  static java.lang.String access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$GracefulFailoverResponseProto$1.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts$CreateParent.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/record/BinaryRecordOutput$1.class": "Compiled from \"BinaryRecordOutput.java\"\npublic class org.apache.hadoop.record.BinaryRecordOutput implements org.apache.hadoop.record.RecordOutput {\n  public static org.apache.hadoop.record.BinaryRecordOutput get(java.io.DataOutput);\n  public org.apache.hadoop.record.BinaryRecordOutput(java.io.OutputStream);\n  public org.apache.hadoop.record.BinaryRecordOutput(java.io.DataOutput);\n  public void writeByte(byte, java.lang.String) throws java.io.IOException;\n  public void writeBool(boolean, java.lang.String) throws java.io.IOException;\n  public void writeInt(int, java.lang.String) throws java.io.IOException;\n  public void writeLong(long, java.lang.String) throws java.io.IOException;\n  public void writeFloat(float, java.lang.String) throws java.io.IOException;\n  public void writeDouble(double, java.lang.String) throws java.io.IOException;\n  public void writeString(java.lang.String, java.lang.String) throws java.io.IOException;\n  public void writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String) throws java.io.IOException;\n  public void startRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public void endRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public void startVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public void endVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public void startMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n  public void endMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.record.BinaryRecordOutput(org.apache.hadoop.record.BinaryRecordOutput$1);\n  static {};\n}\n", 
  "org/apache/hadoop/io/MapFile$Writer$Option.class": "Compiled from \"MapFile.java\"\npublic class org.apache.hadoop.io.MapFile {\n  public static final java.lang.String INDEX_FILE_NAME;\n  public static final java.lang.String DATA_FILE_NAME;\n  protected org.apache.hadoop.io.MapFile();\n  public static void rename(org.apache.hadoop.fs.FileSystem, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void delete(org.apache.hadoop.fs.FileSystem, java.lang.String) throws java.io.IOException;\n  public static long fix(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.Class<? extends org.apache.hadoop.io.Writable>, java.lang.Class<? extends org.apache.hadoop.io.Writable>, boolean, org.apache.hadoop.conf.Configuration) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.class": "Compiled from \"GangliaSink30.java\"\npublic class org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30 extends org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink {\n  public final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30();\n  public void init(org.apache.commons.configuration.SubsetConfiguration);\n  public void appendPrefix(org.apache.hadoop.metrics2.MetricsRecord, java.lang.StringBuilder);\n  public void putMetrics(org.apache.hadoop.metrics2.MetricsRecord);\n  protected void emitMetric(java.lang.String, java.lang.String, java.lang.String, java.lang.String, org.apache.hadoop.metrics2.sink.ganglia.GangliaConf, org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/metrics2/MetricsSink.class": "Compiled from \"MetricsSink.java\"\npublic interface org.apache.hadoop.metrics2.MetricsSink extends org.apache.hadoop.metrics2.MetricsPlugin {\n  public abstract void putMetrics(org.apache.hadoop.metrics2.MetricsRecord);\n  public abstract void flush();\n}\n", 
  "org/apache/hadoop/io/ElasticByteBufferPool.class": "Compiled from \"ElasticByteBufferPool.java\"\npublic final class org.apache.hadoop.io.ElasticByteBufferPool implements org.apache.hadoop.io.ByteBufferPool {\n  public org.apache.hadoop.io.ElasticByteBufferPool();\n  public synchronized java.nio.ByteBuffer getBuffer(boolean, int);\n  public synchronized void putBuffer(java.nio.ByteBuffer);\n}\n", 
  "org/apache/hadoop/util/Classpath.class": "Compiled from \"Classpath.java\"\npublic final class org.apache.hadoop.util.Classpath {\n  public org.apache.hadoop.util.Classpath();\n  public static void main(java.lang.String[]);\n}\n", 
  "org/apache/hadoop/metrics2/filter/GlobFilter.class": "Compiled from \"GlobFilter.java\"\npublic class org.apache.hadoop.metrics2.filter.GlobFilter extends org.apache.hadoop.metrics2.filter.AbstractPatternFilter {\n  public org.apache.hadoop.metrics2.filter.GlobFilter();\n  protected java.util.regex.Pattern compile(java.lang.String);\n}\n", 
  "org/apache/hadoop/io/OutputBuffer$Buffer.class": "Compiled from \"OutputBuffer.java\"\npublic class org.apache.hadoop.io.OutputBuffer extends java.io.FilterOutputStream {\n  public org.apache.hadoop.io.OutputBuffer();\n  public byte[] getData();\n  public int getLength();\n  public org.apache.hadoop.io.OutputBuffer reset();\n  public void write(java.io.InputStream, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/conf/Configuration$IntegerRanges$Range.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/JceAesCtrCryptoCodec.class": "Compiled from \"JceAesCtrCryptoCodec.java\"\npublic class org.apache.hadoop.crypto.JceAesCtrCryptoCodec extends org.apache.hadoop.crypto.AesCtrCryptoCodec {\n  public org.apache.hadoop.crypto.JceAesCtrCryptoCodec();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.crypto.Encryptor createEncryptor() throws java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.Decryptor createDecryptor() throws java.security.GeneralSecurityException;\n  public void generateSecureRandom(byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/WritableSerialization$WritableDeserializer.class": "Compiled from \"WritableSerialization.java\"\npublic class org.apache.hadoop.io.serializer.WritableSerialization extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.io.serializer.Serialization<org.apache.hadoop.io.Writable> {\n  public org.apache.hadoop.io.serializer.WritableSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.io.Writable> getSerializer(java.lang.Class<org.apache.hadoop.io.Writable>);\n  public org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.io.Writable> getDeserializer(java.lang.Class<org.apache.hadoop.io.Writable>);\n}\n", 
  "org/apache/hadoop/util/ClassUtil.class": "Compiled from \"ClassUtil.java\"\npublic class org.apache.hadoop.util.ClassUtil {\n  public org.apache.hadoop.util.ClassUtil();\n  public static java.lang.String findContainingJar(java.lang.Class<?>);\n}\n", 
  "org/apache/hadoop/io/EnumSetWritable$1.class": "Compiled from \"EnumSetWritable.java\"\npublic class org.apache.hadoop.io.EnumSetWritable<E extends java.lang.Enum<E>> extends java.util.AbstractCollection<E> implements org.apache.hadoop.io.Writable, org.apache.hadoop.conf.Configurable {\n  org.apache.hadoop.io.EnumSetWritable();\n  public java.util.Iterator<E> iterator();\n  public int size();\n  public boolean add(E);\n  public org.apache.hadoop.io.EnumSetWritable(java.util.EnumSet<E>, java.lang.Class<E>);\n  public org.apache.hadoop.io.EnumSetWritable(java.util.EnumSet<E>);\n  public void set(java.util.EnumSet<E>, java.lang.Class<E>);\n  public java.util.EnumSet<E> get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public java.lang.Class<E> getElementType();\n  public int hashCode();\n  public java.lang.String toString();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public boolean add(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/HAServiceTarget.class": "Compiled from \"HAServiceTarget.java\"\npublic abstract class org.apache.hadoop.ha.HAServiceTarget {\n  public org.apache.hadoop.ha.HAServiceTarget();\n  public abstract java.net.InetSocketAddress getAddress();\n  public abstract java.net.InetSocketAddress getZKFCAddress();\n  public abstract org.apache.hadoop.ha.NodeFencer getFencer();\n  public abstract void checkFencingConfigured() throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public org.apache.hadoop.ha.HAServiceProtocol getProxy(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public org.apache.hadoop.ha.ZKFCProtocol getZKFCProxy(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public final java.util.Map<java.lang.String, java.lang.String> getFencingParameters();\n  protected void addFencingParameters(java.util.Map<java.lang.String, java.lang.String>);\n  public boolean isAutoFailoverEnabled();\n}\n", 
  "org/apache/hadoop/util/LightWeightCache$2.class": "Compiled from \"LightWeightCache.java\"\npublic class org.apache.hadoop.util.LightWeightCache<K, E extends K> extends org.apache.hadoop.util.LightWeightGSet<K, E> {\n  public org.apache.hadoop.util.LightWeightCache(int, int, long, long);\n  org.apache.hadoop.util.LightWeightCache(int, int, long, long, org.apache.hadoop.util.LightWeightCache$Clock);\n  void setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  boolean isExpired(org.apache.hadoop.util.LightWeightCache$Entry, long);\n  public E get(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/FsPermission.class": "Compiled from \"FsPermission.java\"\npublic class org.apache.hadoop.fs.permission.FsPermission implements org.apache.hadoop.io.Writable {\n  static final org.apache.hadoop.io.WritableFactory FACTORY;\n  public static final int MAX_PERMISSION_LENGTH;\n  public static final java.lang.String DEPRECATED_UMASK_LABEL;\n  public static final java.lang.String UMASK_LABEL;\n  public static final int DEFAULT_UMASK;\n  public static org.apache.hadoop.fs.permission.FsPermission createImmutable(short);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, org.apache.hadoop.fs.permission.FsAction, boolean);\n  public org.apache.hadoop.fs.permission.FsPermission(short);\n  public org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.permission.FsPermission(java.lang.String);\n  public org.apache.hadoop.fs.permission.FsAction getUserAction();\n  public org.apache.hadoop.fs.permission.FsAction getGroupAction();\n  public org.apache.hadoop.fs.permission.FsAction getOtherAction();\n  public void fromShort(short);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static org.apache.hadoop.fs.permission.FsPermission read(java.io.DataInput) throws java.io.IOException;\n  public short toShort();\n  public short toExtendedShort();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.permission.FsPermission applyUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public static org.apache.hadoop.fs.permission.FsPermission getUMask(org.apache.hadoop.conf.Configuration);\n  public boolean getStickyBit();\n  public boolean getAclBit();\n  public boolean getEncryptedBit();\n  public static void setUMask(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.permission.FsPermission);\n  public static org.apache.hadoop.fs.permission.FsPermission getDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getDirDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getFileDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission getCachePoolDefault();\n  public static org.apache.hadoop.fs.permission.FsPermission valueOf(java.lang.String);\n  org.apache.hadoop.fs.permission.FsPermission(org.apache.hadoop.fs.permission.FsPermission$1);\n  static {};\n}\n", 
  "org/apache/hadoop/io/MultipleIOException.class": "Compiled from \"MultipleIOException.java\"\npublic class org.apache.hadoop.io.MultipleIOException extends java.io.IOException {\n  public java.util.List<java.io.IOException> getExceptions();\n  public static java.io.IOException createIOException(java.util.List<java.io.IOException>);\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$TokenProtoOrBuilder.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolPB.class": "Compiled from \"RefreshCallQueueProtocolPB.java\"\npublic interface org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB extends org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueProtocolService$BlockingInterface {\n}\n", 
  "org/apache/hadoop/log/Log4Json.class": "Compiled from \"Log4Json.java\"\npublic class org.apache.hadoop.log.Log4Json extends org.apache.log4j.Layout {\n  public static final java.lang.String DATE;\n  public static final java.lang.String EXCEPTION_CLASS;\n  public static final java.lang.String LEVEL;\n  public static final java.lang.String MESSAGE;\n  public static final java.lang.String NAME;\n  public static final java.lang.String STACK;\n  public static final java.lang.String THREAD;\n  public static final java.lang.String TIME;\n  public static final java.lang.String JSON_TYPE;\n  public org.apache.hadoop.log.Log4Json();\n  public java.lang.String getContentType();\n  public java.lang.String format(org.apache.log4j.spi.LoggingEvent);\n  public java.lang.String toJson(org.apache.log4j.spi.LoggingEvent) throws java.io.IOException;\n  public java.io.Writer toJson(java.io.Writer, org.apache.log4j.spi.LoggingEvent) throws java.io.IOException;\n  public java.io.Writer toJson(java.io.Writer, java.lang.String, long, java.lang.String, java.lang.String, java.lang.String, org.apache.log4j.spi.ThrowableInformation) throws java.io.IOException;\n  public boolean ignoresThrowable();\n  public void activateOptions();\n  public static org.codehaus.jackson.node.ContainerNode parse(java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/ReflectionUtils$CopyInCopyOutBuffer.class": "Compiled from \"ReflectionUtils.java\"\npublic class org.apache.hadoop.util.ReflectionUtils {\n  public org.apache.hadoop.util.ReflectionUtils();\n  public static void setConf(java.lang.Object, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T newInstance(java.lang.Class<T>, org.apache.hadoop.conf.Configuration);\n  public static void setContentionTracing(boolean);\n  public static synchronized void printThreadInfo(java.io.PrintStream, java.lang.String);\n  public static void logThreadInfo(org.apache.commons.logging.Log, java.lang.String, long);\n  public static <T extends java/lang/Object> java.lang.Class<T> getClass(T);\n  static void clearCache();\n  static int getCacheSize();\n  public static <T extends java/lang/Object> T copy(org.apache.hadoop.conf.Configuration, T, T) throws java.io.IOException;\n  public static void cloneWritableInto(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable) throws java.io.IOException;\n  public static java.util.List<java.lang.reflect.Field> getDeclaredFieldsIncludingInherited(java.lang.Class<?>);\n  public static java.util.List<java.lang.reflect.Method> getDeclaredMethodsIncludingInherited(java.lang.Class<?>);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ftp/FtpFs.class": "Compiled from \"FtpFs.java\"\npublic class org.apache.hadoop.fs.ftp.FtpFs extends org.apache.hadoop.fs.DelegateToFileSystem {\n  org.apache.hadoop.fs.ftp.FtpFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/Lz4Codec.class": "Compiled from \"Lz4Codec.java\"\npublic class org.apache.hadoop.io.compress.Lz4Codec implements org.apache.hadoop.conf.Configurable,org.apache.hadoop.io.compress.CompressionCodec {\n  org.apache.hadoop.conf.Configuration conf;\n  public org.apache.hadoop.io.compress.Lz4Codec();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public static boolean isNativeCodeLoaded();\n  public static java.lang.String getLibraryName();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.String getDefaultExtension();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$34.class": "", 
  "org/apache/hadoop/fs/shell/find/Print$Print0.class": "Compiled from \"Print.java\"\nfinal class org.apache.hadoop.fs.shell.find.Print extends org.apache.hadoop.fs.shell.find.BaseExpression {\n  public static void registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Print();\n  public org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  public boolean isAction();\n  org.apache.hadoop.fs.shell.find.Print(java.lang.String, org.apache.hadoop.fs.shell.find.Print$1);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/DecompressorStream.class": "Compiled from \"DecompressorStream.java\"\npublic class org.apache.hadoop.io.compress.DecompressorStream extends org.apache.hadoop.io.compress.CompressionInputStream {\n  protected org.apache.hadoop.io.compress.Decompressor decompressor;\n  protected byte[] buffer;\n  protected boolean eof;\n  protected boolean closed;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.io.compress.DecompressorStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, int) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.DecompressorStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  protected org.apache.hadoop.io.compress.DecompressorStream(java.io.InputStream) throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  protected int decompress(byte[], int, int) throws java.io.IOException;\n  protected int getCompressedData() throws java.io.IOException;\n  protected void checkStream() throws java.io.IOException;\n  public void resetState() throws java.io.IOException;\n  public long skip(long) throws java.io.IOException;\n  public int available() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean markSupported();\n  public synchronized void mark(int);\n  public synchronized void reset() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos$GenericRefreshResponseProto.class": "Compiled from \"GenericRefreshProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/Display.class": "Compiled from \"Display.java\"\nclass org.apache.hadoop.fs.shell.Display extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.Display();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/net/SocketIOWithTimeout$1.class": "Compiled from \"SocketIOWithTimeout.java\"\nabstract class org.apache.hadoop.net.SocketIOWithTimeout {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.net.SocketIOWithTimeout(java.nio.channels.SelectableChannel, long) throws java.io.IOException;\n  void close();\n  boolean isOpen();\n  java.nio.channels.SelectableChannel getChannel();\n  static void checkChannelValidity(java.lang.Object) throws java.io.IOException;\n  abstract int performIO(java.nio.ByteBuffer) throws java.io.IOException;\n  int doIO(java.nio.ByteBuffer, int) throws java.io.IOException;\n  static void connect(java.nio.channels.SocketChannel, java.net.SocketAddress, int) throws java.io.IOException;\n  void waitForIO(int) throws java.io.IOException;\n  public void setTimeout(long);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider$14.class": "", 
  "org/apache/hadoop/fs/PathIsDirectoryException.class": "Compiled from \"PathIsDirectoryException.java\"\npublic class org.apache.hadoop.fs.PathIsDirectoryException extends org.apache.hadoop.fs.PathExistsException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.PathIsDirectoryException(java.lang.String);\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts$ChecksumParam.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/fs/shell/find/Name$1.class": "Compiled from \"Name.java\"\nfinal class org.apache.hadoop.fs.shell.find.Name extends org.apache.hadoop.fs.shell.find.BaseExpression {\n  public static void registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Name();\n  public void addArguments(java.util.Deque<java.lang.String>);\n  public void prepare() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  org.apache.hadoop.fs.shell.find.Name(boolean, org.apache.hadoop.fs.shell.find.Name$1);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/KMSClientProvider$KMSEncryptedKeyVersion.class": "Compiled from \"KMSClientProvider.java\"\npublic class org.apache.hadoop.crypto.key.kms.KMSClientProvider extends org.apache.hadoop.crypto.key.KeyProvider implements org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension {\n  public static final java.lang.String TOKEN_KIND;\n  public static final java.lang.String SCHEME_NAME;\n  public static final java.lang.String TIMEOUT_ATTR;\n  public static final int DEFAULT_TIMEOUT;\n  public static final java.lang.String AUTH_RETRY;\n  public static final int DEFAULT_AUTH_RETRY;\n  public static <T extends java/lang/Object> T checkNotNull(T, java.lang.String) throws java.lang.IllegalArgumentException;\n  public static java.lang.String checkNotEmpty(java.lang.String, java.lang.String) throws java.lang.IllegalArgumentException;\n  public java.lang.String toString();\n  public org.apache.hadoop.crypto.key.kms.KMSClientProvider(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public void deleteKey(java.lang.String) throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public void drain(java.lang.String);\n  public int getEncKeyQueueSize(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  java.lang.String getKMSUrl();\n  static java.net.URL access$000(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.lang.String, java.lang.String, java.lang.String, java.util.Map) throws java.io.IOException;\n  static java.net.HttpURLConnection access$100(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.URL, java.lang.String) throws java.io.IOException;\n  static java.lang.Object access$200(org.apache.hadoop.crypto.key.kms.KMSClientProvider, java.net.HttpURLConnection, java.util.Map, int, java.lang.Class) throws java.io.IOException;\n  static java.util.List access$300(java.lang.String, java.util.List);\n  static org.apache.hadoop.fs.Path access$400(java.net.URI) throws java.net.MalformedURLException, java.io.IOException;\n  static org.apache.hadoop.security.authentication.client.ConnectionConfigurator access$600(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n  static org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token access$700(org.apache.hadoop.crypto.key.kms.KMSClientProvider);\n}\n", 
  "org/apache/hadoop/io/compress/SnappyCodec.class": "Compiled from \"SnappyCodec.java\"\npublic class org.apache.hadoop.io.compress.SnappyCodec implements org.apache.hadoop.conf.Configurable,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.compress.DirectDecompressionCodec {\n  org.apache.hadoop.conf.Configuration conf;\n  public org.apache.hadoop.io.compress.SnappyCodec();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public static void checkNativeCodeLoaded();\n  public static boolean isNativeCodeLoaded();\n  public static java.lang.String getLibraryName();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public org.apache.hadoop.io.compress.DirectDecompressor createDirectDecompressor();\n  public java.lang.String getDefaultExtension();\n}\n", 
  "org/apache/hadoop/util/ServletUtil.class": "Compiled from \"ServletUtil.java\"\npublic class org.apache.hadoop.util.ServletUtil {\n  public static final java.lang.String HTML_TAIL;\n  public org.apache.hadoop.util.ServletUtil();\n  public static java.io.PrintWriter initHTML(javax.servlet.ServletResponse, java.lang.String) throws java.io.IOException;\n  public static java.lang.String getParameter(javax.servlet.ServletRequest, java.lang.String);\n  public static long parseLongParam(javax.servlet.ServletRequest, java.lang.String) throws java.io.IOException;\n  public static java.lang.String htmlFooter();\n  public static java.lang.String encodeQueryValue(java.lang.String);\n  public static java.lang.String encodePath(java.lang.String);\n  public static java.lang.String getDecodedPath(javax.servlet.http.HttpServletRequest, java.lang.String);\n  public static java.lang.String getRawPath(javax.servlet.http.HttpServletRequest, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$TraceAdminService$Stub.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToActiveRequestProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/unix/DomainSocketWatcher$1.class": "Compiled from \"DomainSocketWatcher.java\"\npublic final class org.apache.hadoop.net.unix.DomainSocketWatcher implements java.io.Closeable {\n  static org.apache.commons.logging.Log LOG;\n  final java.lang.Thread watcherThread;\n  static final boolean $assertionsDisabled;\n  public static java.lang.String getLoadingFailureReason();\n  public org.apache.hadoop.net.unix.DomainSocketWatcher(int, java.lang.String) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void add(org.apache.hadoop.net.unix.DomainSocket, org.apache.hadoop.net.unix.DomainSocketWatcher$Handler);\n  public void remove(org.apache.hadoop.net.unix.DomainSocket);\n  public java.lang.String toString();\n  static java.util.concurrent.locks.ReentrantLock access$000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$102(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static boolean access$202(org.apache.hadoop.net.unix.DomainSocketWatcher, boolean);\n  static int access$300(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static void access$400(org.apache.hadoop.net.unix.DomainSocketWatcher, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet);\n  static void access$500(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static java.util.LinkedList access$600(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.TreeMap access$700(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static java.util.concurrent.locks.Condition access$800(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$200(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static int access$900(int, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet) throws java.io.IOException;\n  static void access$1000(org.apache.hadoop.net.unix.DomainSocketWatcher);\n  static boolean access$1100(org.apache.hadoop.net.unix.DomainSocketWatcher, java.lang.String, java.util.TreeMap, org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet, int);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$ValueClassOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/EnumSetWritable.class": "Compiled from \"EnumSetWritable.java\"\npublic class org.apache.hadoop.io.EnumSetWritable<E extends java.lang.Enum<E>> extends java.util.AbstractCollection<E> implements org.apache.hadoop.io.Writable, org.apache.hadoop.conf.Configurable {\n  org.apache.hadoop.io.EnumSetWritable();\n  public java.util.Iterator<E> iterator();\n  public int size();\n  public boolean add(E);\n  public org.apache.hadoop.io.EnumSetWritable(java.util.EnumSet<E>, java.lang.Class<E>);\n  public org.apache.hadoop.io.EnumSetWritable(java.util.EnumSet<E>);\n  public void set(java.util.EnumSet<E>, java.lang.Class<E>);\n  public java.util.EnumSet<E> get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public java.lang.Class<E> getElementType();\n  public int hashCode();\n  public java.lang.String toString();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public boolean add(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/util/DirectBufferPool.class": "Compiled from \"DirectBufferPool.java\"\npublic class org.apache.hadoop.util.DirectBufferPool {\n  final java.util.concurrent.ConcurrentMap<java.lang.Integer, java.util.Queue<java.lang.ref.WeakReference<java.nio.ByteBuffer>>> buffersBySize;\n  public org.apache.hadoop.util.DirectBufferPool();\n  public java.nio.ByteBuffer getBuffer(int);\n  public void returnBuffer(java.nio.ByteBuffer);\n  int countBuffersOfSize(int);\n}\n", 
  "org/apache/hadoop/fs/FSError.class": "Compiled from \"FSError.java\"\npublic class org.apache.hadoop.fs.FSError extends java.lang.Error {\n  org.apache.hadoop.fs.FSError(java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/util/CombinedIPWhiteList.class": "Compiled from \"CombinedIPWhiteList.java\"\npublic class org.apache.hadoop.util.CombinedIPWhiteList implements org.apache.hadoop.util.IPList {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.util.CombinedIPWhiteList(java.lang.String, java.lang.String, long);\n  public boolean isIn(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JRecord$CRecord.class": "Compiled from \"JRecord.java\"\npublic class org.apache.hadoop.record.compiler.JRecord extends org.apache.hadoop.record.compiler.JCompType {\n  public org.apache.hadoop.record.compiler.JRecord(java.lang.String, java.util.ArrayList<org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType>>);\n  java.lang.String getSignature();\n  void genCppCode(java.io.FileWriter, java.io.FileWriter, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n  void genJavaCode(java.lang.String, java.util.ArrayList<java.lang.String>) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/ExitUtil.class": "Compiled from \"ExitUtil.java\"\npublic final class org.apache.hadoop.util.ExitUtil {\n  public org.apache.hadoop.util.ExitUtil();\n  public static void disableSystemExit();\n  public static void disableSystemHalt();\n  public static boolean terminateCalled();\n  public static boolean haltCalled();\n  public static org.apache.hadoop.util.ExitUtil$ExitException getFirstExitException();\n  public static org.apache.hadoop.util.ExitUtil$HaltException getFirstHaltException();\n  public static void resetFirstExitException();\n  public static void resetFirstHaltException();\n  public static void terminate(int, java.lang.String) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int, java.lang.String) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  public static void terminate(int, java.lang.Throwable) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int, java.lang.Throwable) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  public static void terminate(int) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/ssl/SSLHostnameVerifier$5.class": "Compiled from \"SSLHostnameVerifier.java\"\npublic interface org.apache.hadoop.security.ssl.SSLHostnameVerifier extends javax.net.ssl.HostnameVerifier {\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier DEFAULT_AND_LOCALHOST;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier STRICT_IE6;\n  public static final org.apache.hadoop.security.ssl.SSLHostnameVerifier ALLOW_ALL;\n  public abstract boolean verify(java.lang.String, javax.net.ssl.SSLSession);\n  public abstract void check(java.lang.String, javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String, java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String, java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], javax.net.ssl.SSLSocket) throws java.io.IOException;\n  public abstract void check(java.lang.String[], java.security.cert.X509Certificate) throws javax.net.ssl.SSLException;\n  public abstract void check(java.lang.String[], java.lang.String[], java.lang.String[]) throws javax.net.ssl.SSLException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Trash.class": "Compiled from \"Trash.java\"\npublic class org.apache.hadoop.fs.Trash extends org.apache.hadoop.conf.Configured {\n  public org.apache.hadoop.fs.Trash(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Trash(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public boolean isEnabled();\n  public boolean moveToTrash(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void checkpoint() throws java.io.IOException;\n  public void expunge() throws java.io.IOException;\n  org.apache.hadoop.fs.Path getCurrentTrashDir();\n  org.apache.hadoop.fs.TrashPolicy getTrashPolicy();\n  public java.lang.Runnable getEmptier() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/SequentialNumber.class": "Compiled from \"SequentialNumber.java\"\npublic abstract class org.apache.hadoop.util.SequentialNumber {\n  protected org.apache.hadoop.util.SequentialNumber(long);\n  public long getCurrentValue();\n  public void setCurrentValue(long);\n  public long nextValue();\n  public void skipTo(long) throws java.lang.IllegalStateException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableQuantiles.class": "Compiled from \"MutableQuantiles.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableQuantiles extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  public static final org.apache.hadoop.metrics2.util.Quantile[] quantiles;\n  protected java.util.Map<org.apache.hadoop.metrics2.util.Quantile, java.lang.Long> previousSnapshot;\n  public org.apache.hadoop.metrics2.lib.MutableQuantiles(java.lang.String, java.lang.String, java.lang.String, java.lang.String, int);\n  public synchronized void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  public synchronized void add(long);\n  public int getInterval();\n  static long access$002(org.apache.hadoop.metrics2.lib.MutableQuantiles, long);\n  static org.apache.hadoop.metrics2.util.SampleQuantiles access$100(org.apache.hadoop.metrics2.lib.MutableQuantiles);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/UnresolvedLinkException.class": "Compiled from \"UnresolvedLinkException.java\"\npublic class org.apache.hadoop.fs.UnresolvedLinkException extends java.io.IOException {\n  public org.apache.hadoop.fs.UnresolvedLinkException();\n  public org.apache.hadoop.fs.UnresolvedLinkException(java.lang.String);\n}\n", 
  "org/apache/hadoop/io/file/tfile/TFile$TFileIndex.class": "Compiled from \"TFile.java\"\npublic class org.apache.hadoop.io.file.tfile.TFile {\n  static final org.apache.commons.logging.Log LOG;\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  public static final java.lang.String COMPRESSION_GZ;\n  public static final java.lang.String COMPRESSION_LZO;\n  public static final java.lang.String COMPRESSION_NONE;\n  public static final java.lang.String COMPARATOR_MEMCMP;\n  public static final java.lang.String COMPARATOR_JCLASS;\n  static int getChunkBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSInputBufferSize(org.apache.hadoop.conf.Configuration);\n  static int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration);\n  public static java.util.Comparator<org.apache.hadoop.io.file.tfile.RawComparable> makeComparator(java.lang.String);\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static void main(java.lang.String[]);\n  static {};\n}\n", 
  "org/apache/hadoop/io/FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer$1.class": "Compiled from \"FastByteComparisons.java\"\nabstract class org.apache.hadoop.io.FastByteComparisons {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.io.FastByteComparisons();\n  public static int compareTo(byte[], int, int, byte[], int, int);\n  static org.apache.hadoop.io.FastByteComparisons$Comparer access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/util/MetricsLongValue.class": "Compiled from \"MetricsLongValue.java\"\npublic class org.apache.hadoop.metrics.util.MetricsLongValue extends org.apache.hadoop.metrics.util.MetricsBase {\n  public org.apache.hadoop.metrics.util.MetricsLongValue(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry, java.lang.String);\n  public org.apache.hadoop.metrics.util.MetricsLongValue(java.lang.String, org.apache.hadoop.metrics.util.MetricsRegistry);\n  public synchronized void set(long);\n  public synchronized long get();\n  public synchronized void pushMetric(org.apache.hadoop.metrics.MetricsRecord);\n}\n", 
  "org/apache/hadoop/fs/FileContext$15.class": "", 
  "org/apache/hadoop/ipc/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.ipc.package-info {\n}\n", 
  "org/apache/hadoop/net/SocketInputWrapper.class": "Compiled from \"SocketInputWrapper.java\"\npublic class org.apache.hadoop.net.SocketInputWrapper extends java.io.FilterInputStream {\n  org.apache.hadoop.net.SocketInputWrapper(java.net.Socket, java.io.InputStream);\n  public void setTimeout(long) throws java.net.SocketException;\n  public java.nio.channels.ReadableByteChannel getReadableByteChannel();\n}\n", 
  "org/apache/hadoop/io/serializer/WritableSerialization$WritableSerializer.class": "Compiled from \"WritableSerialization.java\"\npublic class org.apache.hadoop.io.serializer.WritableSerialization extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.io.serializer.Serialization<org.apache.hadoop.io.Writable> {\n  public org.apache.hadoop.io.serializer.WritableSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.hadoop.io.serializer.Serializer<org.apache.hadoop.io.Writable> getSerializer(java.lang.Class<org.apache.hadoop.io.Writable>);\n  public org.apache.hadoop.io.serializer.Deserializer<org.apache.hadoop.io.Writable> getDeserializer(java.lang.Class<org.apache.hadoop.io.Writable>);\n}\n", 
  "org/apache/hadoop/tools/TableListing$Justification.class": "Compiled from \"TableListing.java\"\npublic class org.apache.hadoop.tools.TableListing {\n  org.apache.hadoop.tools.TableListing(org.apache.hadoop.tools.TableListing$Column[], boolean, int);\n  public void addRow(java.lang.String...);\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/security/ShellBasedUnixGroupsMapping.class": "Compiled from \"ShellBasedUnixGroupsMapping.java\"\npublic class org.apache.hadoop.security.ShellBasedUnixGroupsMapping implements org.apache.hadoop.security.GroupMappingServiceProvider {\n  public org.apache.hadoop.security.ShellBasedUnixGroupsMapping();\n  public java.util.List<java.lang.String> getGroups(java.lang.String) throws java.io.IOException;\n  public void cacheGroupsRefresh() throws java.io.IOException;\n  public void cacheGroupsAdd(java.util.List<java.lang.String>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolPB.class": "Compiled from \"GetUserMappingsProtocolPB.java\"\npublic interface org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB extends org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetUserMappingsProtocolService$BlockingInterface {\n}\n", 
  "org/apache/hadoop/ha/ZKFailoverController$HealthCallbacks.class": "Compiled from \"ZKFailoverController.java\"\npublic abstract class org.apache.hadoop.ha.ZKFailoverController {\n  static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String ZK_QUORUM_KEY;\n  public static final java.lang.String ZK_ACL_KEY;\n  public static final java.lang.String ZK_AUTH_KEY;\n  static final java.lang.String ZK_PARENT_ZNODE_DEFAULT;\n  protected static final java.lang.String[] ZKFC_CONF_KEYS;\n  protected static final java.lang.String USAGE;\n  static final int ERR_CODE_FORMAT_DENIED;\n  static final int ERR_CODE_NO_PARENT_ZNODE;\n  static final int ERR_CODE_NO_FENCER;\n  static final int ERR_CODE_AUTO_FAILOVER_NOT_ENABLED;\n  static final int ERR_CODE_NO_ZK;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected final org.apache.hadoop.ha.HAServiceTarget localTarget;\n  protected org.apache.hadoop.ha.ZKFCRpcServer rpcServer;\n  int serviceStateMismatchCount;\n  boolean quitElectionOnBadState;\n  static final boolean $assertionsDisabled;\n  protected org.apache.hadoop.ha.ZKFailoverController(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract byte[] targetToData(org.apache.hadoop.ha.HAServiceTarget);\n  protected abstract org.apache.hadoop.ha.HAServiceTarget dataToTarget(byte[]);\n  protected abstract void loginAsFCUser() throws java.io.IOException;\n  protected abstract void checkRpcAdminAccess() throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected abstract java.net.InetSocketAddress getRpcAddressToBindTo();\n  protected abstract org.apache.hadoop.security.authorize.PolicyProvider getPolicyProvider();\n  protected abstract java.lang.String getScopeInsideParentNode();\n  public org.apache.hadoop.ha.HAServiceTarget getLocalTarget();\n  org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getServiceState();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected void initRPC() throws java.io.IOException;\n  protected void startRPC() throws java.io.IOException;\n  void cedeActive(int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void gracefulFailoverToYou() throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState);\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getLastHealthState();\n  org.apache.hadoop.ha.ActiveStandbyElector getElectorForTests();\n  org.apache.hadoop.ha.ZKFCRpcServer getRpcServerForTests();\n  static int access$000(org.apache.hadoop.ha.ZKFailoverController, java.lang.String[]) throws org.apache.hadoop.HadoopIllegalArgumentException, java.io.IOException, java.lang.InterruptedException;\n  static org.apache.hadoop.ha.ActiveStandbyElector access$100(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$300(org.apache.hadoop.ha.ZKFailoverController, int) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.ha.ServiceFailedException, java.io.IOException;\n  static void access$400(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException, java.io.IOException, java.lang.InterruptedException;\n  static void access$700(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$800(org.apache.hadoop.ha.ZKFailoverController, java.lang.String);\n  static void access$900(org.apache.hadoop.ha.ZKFailoverController) throws org.apache.hadoop.ha.ServiceFailedException;\n  static void access$1000(org.apache.hadoop.ha.ZKFailoverController);\n  static void access$1100(org.apache.hadoop.ha.ZKFailoverController, byte[]);\n  static void access$1200(org.apache.hadoop.ha.ZKFailoverController, org.apache.hadoop.ha.HealthMonitor$State);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$18.class": "", 
  "org/apache/hadoop/crypto/AesCtrCryptoCodec.class": "Compiled from \"AesCtrCryptoCodec.java\"\npublic abstract class org.apache.hadoop.crypto.AesCtrCryptoCodec extends org.apache.hadoop.crypto.CryptoCodec {\n  protected static final org.apache.hadoop.crypto.CipherSuite SUITE;\n  public org.apache.hadoop.crypto.AesCtrCryptoCodec();\n  public org.apache.hadoop.crypto.CipherSuite getCipherSuite();\n  public void calculateIV(byte[], long, byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/util/DataChecksum$Type.class": "Compiled from \"DataChecksum.java\"\npublic class org.apache.hadoop.util.DataChecksum implements java.util.zip.Checksum {\n  public static final int CHECKSUM_NULL;\n  public static final int CHECKSUM_CRC32;\n  public static final int CHECKSUM_CRC32C;\n  public static final int CHECKSUM_DEFAULT;\n  public static final int CHECKSUM_MIXED;\n  public static final int SIZE_OF_INTEGER;\n  public static java.util.zip.Checksum newCrc32();\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(org.apache.hadoop.util.DataChecksum$Type, int);\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(byte[], int);\n  public static org.apache.hadoop.util.DataChecksum newDataChecksum(java.io.DataInputStream) throws java.io.IOException;\n  public void writeHeader(java.io.DataOutputStream) throws java.io.IOException;\n  public byte[] getHeader();\n  public int writeValue(java.io.DataOutputStream, boolean) throws java.io.IOException;\n  public int writeValue(byte[], int, boolean) throws java.io.IOException;\n  public boolean compare(byte[], int);\n  public org.apache.hadoop.util.DataChecksum$Type getChecksumType();\n  public int getChecksumSize();\n  public int getChecksumSize(int);\n  public int getBytesPerChecksum();\n  public int getNumBytesInSum();\n  public static int getChecksumHeaderSize();\n  public long getValue();\n  public void reset();\n  public void update(byte[], int, int);\n  public void update(int);\n  public void verifyChunkedSums(java.nio.ByteBuffer, java.nio.ByteBuffer, java.lang.String, long) throws org.apache.hadoop.fs.ChecksumException;\n  public void calculateChunkedSums(java.nio.ByteBuffer, java.nio.ByteBuffer);\n  public void calculateChunkedSums(byte[], int, int, byte[], int);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/record/compiler/generated/RccConstants.class": "Compiled from \"RccConstants.java\"\npublic interface org.apache.hadoop.record.compiler.generated.RccConstants {\n  public static final int EOF;\n  public static final int MODULE_TKN;\n  public static final int RECORD_TKN;\n  public static final int INCLUDE_TKN;\n  public static final int BYTE_TKN;\n  public static final int BOOLEAN_TKN;\n  public static final int INT_TKN;\n  public static final int LONG_TKN;\n  public static final int FLOAT_TKN;\n  public static final int DOUBLE_TKN;\n  public static final int USTRING_TKN;\n  public static final int BUFFER_TKN;\n  public static final int VECTOR_TKN;\n  public static final int MAP_TKN;\n  public static final int LBRACE_TKN;\n  public static final int RBRACE_TKN;\n  public static final int LT_TKN;\n  public static final int GT_TKN;\n  public static final int SEMICOLON_TKN;\n  public static final int COMMA_TKN;\n  public static final int DOT_TKN;\n  public static final int CSTRING_TKN;\n  public static final int IDENT_TKN;\n  public static final int DEFAULT;\n  public static final int WithinOneLineComment;\n  public static final int WithinMultiLineComment;\n  public static final java.lang.String[] tokenImage;\n  static {};\n}\n", 
  "org/apache/hadoop/util/ExitUtil$HaltException.class": "Compiled from \"ExitUtil.java\"\npublic final class org.apache.hadoop.util.ExitUtil {\n  public org.apache.hadoop.util.ExitUtil();\n  public static void disableSystemExit();\n  public static void disableSystemHalt();\n  public static boolean terminateCalled();\n  public static boolean haltCalled();\n  public static org.apache.hadoop.util.ExitUtil$ExitException getFirstExitException();\n  public static org.apache.hadoop.util.ExitUtil$HaltException getFirstHaltException();\n  public static void resetFirstExitException();\n  public static void resetFirstHaltException();\n  public static void terminate(int, java.lang.String) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int, java.lang.String) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  public static void terminate(int, java.lang.Throwable) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int, java.lang.Throwable) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  public static void terminate(int) throws org.apache.hadoop.util.ExitUtil$ExitException;\n  public static void halt(int) throws org.apache.hadoop.util.ExitUtil$HaltException;\n  static {};\n}\n", 
  "org/apache/hadoop/util/ThreadUtil.class": "Compiled from \"ThreadUtil.java\"\npublic class org.apache.hadoop.util.ThreadUtil {\n  public org.apache.hadoop.util.ThreadUtil();\n  public static void sleepAtLeastIgnoreInterrupts(long);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceProtocolService$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProviderCryptoExtension$EncryptedKeyVersion.class": "Compiled from \"KeyProviderCryptoExtension.java\"\npublic class org.apache.hadoop.crypto.key.KeyProviderCryptoExtension extends org.apache.hadoop.crypto.key.KeyProviderExtension<org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension> {\n  public static final java.lang.String EEK;\n  public static final java.lang.String EK;\n  protected org.apache.hadoop.crypto.key.KeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider, org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension);\n  public void warmUpEncryptedKeys(java.lang.String...) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion generateEncryptedKey(java.lang.String) throws java.io.IOException, java.security.GeneralSecurityException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion) throws java.io.IOException, java.security.GeneralSecurityException;\n  public static org.apache.hadoop.crypto.key.KeyProviderCryptoExtension createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider);\n  public void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/FloatWritable.class": "Compiled from \"FloatWritable.java\"\npublic class org.apache.hadoop.io.FloatWritable implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.FloatWritable> {\n  public org.apache.hadoop.io.FloatWritable();\n  public org.apache.hadoop.io.FloatWritable(float);\n  public void set(float);\n  public float get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.FloatWritable);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/sink/ganglia/GangliaMetricVisitor.class": "Compiled from \"GangliaMetricVisitor.java\"\nclass org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor implements org.apache.hadoop.metrics2.MetricsVisitor {\n  org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor();\n  java.lang.String getType();\n  org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope getSlope();\n  public void gauge(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public void gauge(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public void gauge(org.apache.hadoop.metrics2.MetricsInfo, float);\n  public void gauge(org.apache.hadoop.metrics2.MetricsInfo, double);\n  public void counter(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public void counter(org.apache.hadoop.metrics2.MetricsInfo, long);\n}\n", 
  "org/apache/hadoop/security/authorize/ServiceAuthorizationManager.class": "Compiled from \"ServiceAuthorizationManager.java\"\npublic class org.apache.hadoop.security.authorize.ServiceAuthorizationManager {\n  static final java.lang.String BLOCKED;\n  static final java.lang.String HOSTS;\n  public static final java.lang.String SERVICE_AUTHORIZATION_CONFIG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager();\n  public void authorize(org.apache.hadoop.security.UserGroupInformation, java.lang.Class<?>, org.apache.hadoop.conf.Configuration, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  public void refresh(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public java.util.Set<java.lang.Class<?>> getProtocolsWithAcls();\n  public org.apache.hadoop.security.authorize.AccessControlList getProtocolsAcls(java.lang.Class<?>);\n  public org.apache.hadoop.security.authorize.AccessControlList getProtocolsBlockedAcls(java.lang.Class<?>);\n  public java.util.Set<java.lang.Class<?>> getProtocolsWithMachineLists();\n  public org.apache.hadoop.util.MachineList getProtocolsMachineList(java.lang.Class<?>);\n  public org.apache.hadoop.util.MachineList getProtocolsBlockedMachineList(java.lang.Class<?>);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/CryptoInputStream.class": "Compiled from \"CryptoInputStream.java\"\npublic class org.apache.hadoop.crypto.CryptoInputStream extends java.io.FilterInputStream implements org.apache.hadoop.fs.Seekable,org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.ByteBufferReadable,org.apache.hadoop.fs.HasFileDescriptor,org.apache.hadoop.fs.CanSetDropBehind,org.apache.hadoop.fs.CanSetReadahead,org.apache.hadoop.fs.HasEnhancedByteBufferAccess,java.nio.channels.ReadableByteChannel {\n  public org.apache.hadoop.crypto.CryptoInputStream(java.io.InputStream, org.apache.hadoop.crypto.CryptoCodec, int, byte[], byte[]) throws java.io.IOException;\n  public org.apache.hadoop.crypto.CryptoInputStream(java.io.InputStream, org.apache.hadoop.crypto.CryptoCodec, int, byte[], byte[], long) throws java.io.IOException;\n  public org.apache.hadoop.crypto.CryptoInputStream(java.io.InputStream, org.apache.hadoop.crypto.CryptoCodec, byte[], byte[]) throws java.io.IOException;\n  public java.io.InputStream getWrappedStream();\n  public int read(byte[], int, int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public int read(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[]) throws java.io.IOException;\n  public void seek(long) throws java.io.IOException;\n  public long skip(long) throws java.io.IOException;\n  public long getPos() throws java.io.IOException;\n  public int read(java.nio.ByteBuffer) throws java.io.IOException;\n  public int available() throws java.io.IOException;\n  public boolean markSupported();\n  public void mark(int);\n  public void reset() throws java.io.IOException;\n  public boolean seekToNewSource(long) throws java.io.IOException;\n  public java.nio.ByteBuffer read(org.apache.hadoop.io.ByteBufferPool, int, java.util.EnumSet<org.apache.hadoop.fs.ReadOption>) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public void releaseBuffer(java.nio.ByteBuffer);\n  public void setReadahead(java.lang.Long) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public void setDropBehind(java.lang.Boolean) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public java.io.FileDescriptor getFileDescriptor() throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public boolean isOpen();\n}\n", 
  "org/apache/hadoop/fs/ByteBufferUtil.class": "Compiled from \"ByteBufferUtil.java\"\npublic final class org.apache.hadoop.fs.ByteBufferUtil {\n  public org.apache.hadoop.fs.ByteBufferUtil();\n  public static java.nio.ByteBuffer fallbackRead(java.io.InputStream, org.apache.hadoop.io.ByteBufferPool, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/serializer/JavaSerialization$JavaSerializationDeserializer.class": "Compiled from \"JavaSerialization.java\"\npublic class org.apache.hadoop.io.serializer.JavaSerialization implements org.apache.hadoop.io.serializer.Serialization<java.io.Serializable> {\n  public org.apache.hadoop.io.serializer.JavaSerialization();\n  public boolean accept(java.lang.Class<?>);\n  public org.apache.hadoop.io.serializer.Deserializer<java.io.Serializable> getDeserializer(java.lang.Class<java.io.Serializable>);\n  public org.apache.hadoop.io.serializer.Serializer<java.io.Serializable> getSerializer(java.lang.Class<java.io.Serializable>);\n}\n", 
  "org/apache/hadoop/security/token/delegation/web/HttpUserGroupInformation.class": "Compiled from \"HttpUserGroupInformation.java\"\npublic class org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation {\n  public org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation();\n  public static org.apache.hadoop.security.UserGroupInformation get();\n}\n", 
  "org/apache/hadoop/ha/HealthMonitor.class": "Compiled from \"HealthMonitor.java\"\npublic class org.apache.hadoop.ha.HealthMonitor {\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.HealthMonitor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.ha.HAServiceTarget);\n  public void addCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public void removeCallback(org.apache.hadoop.ha.HealthMonitor$Callback);\n  public synchronized void addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public synchronized void removeServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback);\n  public void shutdown();\n  public synchronized org.apache.hadoop.ha.HAServiceProtocol getProxy();\n  protected org.apache.hadoop.ha.HAServiceProtocol createProxy() throws java.io.IOException;\n  synchronized org.apache.hadoop.ha.HealthMonitor$State getHealthState();\n  synchronized org.apache.hadoop.ha.HAServiceStatus getLastServiceStatus();\n  boolean isAlive();\n  void join() throws java.lang.InterruptedException;\n  void start();\n  static org.apache.hadoop.ha.HAServiceTarget access$100(org.apache.hadoop.ha.HealthMonitor);\n  static org.apache.commons.logging.Log access$200();\n  static void access$300(org.apache.hadoop.ha.HealthMonitor, org.apache.hadoop.ha.HealthMonitor$State);\n  static boolean access$400(org.apache.hadoop.ha.HealthMonitor);\n  static void access$500(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static void access$600(org.apache.hadoop.ha.HealthMonitor) throws java.lang.InterruptedException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$1.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/InvalidRequestException.class": "Compiled from \"InvalidRequestException.java\"\npublic class org.apache.hadoop.fs.InvalidRequestException extends java.io.IOException {\n  static final long serialVersionUID;\n  public org.apache.hadoop.fs.InvalidRequestException(java.lang.String);\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$ConfigPairOrBuilder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$6.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/filter/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.metrics2.filter.package-info {\n}\n", 
  "org/apache/hadoop/util/GSet.class": "Compiled from \"GSet.java\"\npublic interface org.apache.hadoop.util.GSet<K, E extends K> extends java.lang.Iterable<E> {\n  public static final org.apache.commons.logging.Log LOG;\n  public abstract int size();\n  public abstract boolean contains(K);\n  public abstract E get(K);\n  public abstract E put(E);\n  public abstract E remove(K);\n  public abstract void clear();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/RawLocalFileSystem$LocalFSFileInputStream.class": "Compiled from \"RawLocalFileSystem.java\"\npublic class org.apache.hadoop.fs.RawLocalFileSystem extends org.apache.hadoop.fs.FileSystem {\n  static final java.net.URI NAME;\n  public static void useStatIfAvailable();\n  public org.apache.hadoop.fs.RawLocalFileSystem();\n  public java.io.File pathToFile(org.apache.hadoop.fs.Path);\n  public java.net.URI getUri();\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStream(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  protected java.io.OutputStream createOutputStreamWithMode(org.apache.hadoop.fs.Path, boolean, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected boolean mkOneDir(java.io.File) throws java.io.IOException;\n  protected boolean mkOneDirWithMode(org.apache.hadoop.fs.Path, java.io.File, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public java.lang.String toString();\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/authorize/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.security.authorize.package-info {\n}\n", 
  "org/apache/hadoop/util/CloseableReferenceCount.class": "Compiled from \"CloseableReferenceCount.java\"\npublic class org.apache.hadoop.util.CloseableReferenceCount {\n  public org.apache.hadoop.util.CloseableReferenceCount();\n  public void reference() throws java.nio.channels.ClosedChannelException;\n  public boolean unreference();\n  public void unreferenceCheckClosed() throws java.nio.channels.ClosedChannelException;\n  public boolean isOpen();\n  public int setClosed() throws java.nio.channels.ClosedChannelException;\n  public int getReferenceCount();\n}\n", 
  "org/apache/hadoop/security/SaslRpcClient$WrappedInputStream.class": "Compiled from \"SaslRpcClient.java\"\npublic class org.apache.hadoop.security.SaslRpcClient {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.SaslRpcClient(org.apache.hadoop.security.UserGroupInformation, java.lang.Class<?>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration);\n  public java.lang.Object getNegotiatedProperty(java.lang.String);\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod getAuthMethod();\n  java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth) throws java.io.IOException;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod saslConnect(java.io.InputStream, java.io.OutputStream) throws java.io.IOException;\n  public java.io.InputStream getInputStream(java.io.InputStream) throws java.io.IOException;\n  public java.io.OutputStream getOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public void dispose() throws javax.security.sasl.SaslException;\n  static javax.security.sasl.SaslClient access$000(org.apache.hadoop.security.SaslRpcClient);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/http/lib/StaticUserWebFilter$User.class": "Compiled from \"StaticUserWebFilter.java\"\npublic class org.apache.hadoop.http.lib.StaticUserWebFilter extends org.apache.hadoop.http.FilterInitializer {\n  static final java.lang.String DEPRECATED_UGI_KEY;\n  public org.apache.hadoop.http.lib.StaticUserWebFilter();\n  public void initFilter(org.apache.hadoop.http.FilterContainer, org.apache.hadoop.conf.Configuration);\n  static java.lang.String getUsernameFromConf(org.apache.hadoop.conf.Configuration);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/CommandWithDestination.class": "Compiled from \"CommandWithDestination.java\"\nabstract class org.apache.hadoop.fs.shell.CommandWithDestination extends org.apache.hadoop.fs.shell.FsCommand {\n  protected org.apache.hadoop.fs.shell.PathData dst;\n  org.apache.hadoop.fs.shell.CommandWithDestination();\n  protected void setOverwrite(boolean);\n  protected void setLazyPersist(boolean);\n  protected void setVerifyChecksum(boolean);\n  protected void setWriteChecksum(boolean);\n  protected void setPreserve(boolean);\n  protected void preserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute);\n  protected void getLocalDestination(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void getRemoteDestination(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  protected void processPathArgument(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void recursePath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected org.apache.hadoop.fs.shell.PathData getTargetPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void copyFileToTarget(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void copyStreamToTarget(java.io.InputStream, org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void preserveAttributes(org.apache.hadoop.fs.shell.PathData, org.apache.hadoop.fs.shell.PathData, boolean) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/compress/CompressionCodec.class": "Compiled from \"CompressionCodec.java\"\npublic interface org.apache.hadoop.io.compress.CompressionCodec {\n  public abstract org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public abstract org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public abstract java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public abstract org.apache.hadoop.io.compress.Compressor createCompressor();\n  public abstract org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public abstract org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public abstract java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public abstract org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public abstract java.lang.String getDefaultExtension();\n}\n", 
  "org/apache/hadoop/ipc/Client$Connection$1.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/FilterExpression.class": "Compiled from \"FilterExpression.java\"\npublic abstract class org.apache.hadoop.fs.shell.find.FilterExpression implements org.apache.hadoop.fs.shell.find.Expression,org.apache.hadoop.conf.Configurable {\n  protected org.apache.hadoop.fs.shell.find.Expression expression;\n  protected org.apache.hadoop.fs.shell.find.FilterExpression(org.apache.hadoop.fs.shell.find.Expression);\n  public void setOptions(org.apache.hadoop.fs.shell.find.FindOptions) throws java.io.IOException;\n  public void prepare() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  public void finish() throws java.io.IOException;\n  public java.lang.String[] getUsage();\n  public java.lang.String[] getHelp();\n  public boolean isAction();\n  public boolean isOperator();\n  public int getPrecedence();\n  public void addChildren(java.util.Deque<org.apache.hadoop.fs.shell.find.Expression>);\n  public void addArguments(java.util.Deque<java.lang.String>);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos$RequestHeaderProtoOrBuilder.class": "Compiled from \"ProtobufRpcEngineProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1102(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsRecordFiltered$1.class": "Compiled from \"MetricsRecordFiltered.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsRecordFiltered extends org.apache.hadoop.metrics2.impl.AbstractMetricsRecord {\n  org.apache.hadoop.metrics2.impl.MetricsRecordFiltered(org.apache.hadoop.metrics2.MetricsRecord, org.apache.hadoop.metrics2.MetricsFilter);\n  public long timestamp();\n  public java.lang.String name();\n  public java.lang.String description();\n  public java.lang.String context();\n  public java.util.Collection<org.apache.hadoop.metrics2.MetricsTag> tags();\n  public java.lang.Iterable<org.apache.hadoop.metrics2.AbstractMetric> metrics();\n  static org.apache.hadoop.metrics2.MetricsRecord access$000(org.apache.hadoop.metrics2.impl.MetricsRecordFiltered);\n  static org.apache.hadoop.metrics2.MetricsFilter access$100(org.apache.hadoop.metrics2.impl.MetricsRecordFiltered);\n}\n", 
  "org/apache/hadoop/fs/shell/CommandFormat$IllegalNumberOfArgumentsException.class": "Compiled from \"CommandFormat.java\"\npublic class org.apache.hadoop.fs.shell.CommandFormat {\n  final int minPar;\n  final int maxPar;\n  final java.util.Map<java.lang.String, java.lang.Boolean> options;\n  boolean ignoreUnknownOpts;\n  public org.apache.hadoop.fs.shell.CommandFormat(java.lang.String, int, int, java.lang.String...);\n  public org.apache.hadoop.fs.shell.CommandFormat(int, int, java.lang.String...);\n  public java.util.List<java.lang.String> parse(java.lang.String[], int);\n  public void parse(java.util.List<java.lang.String>);\n  public boolean getOpt(java.lang.String);\n  public java.util.Set<java.lang.String> getOpts();\n}\n", 
  "org/apache/hadoop/HadoopIllegalArgumentException.class": "Compiled from \"HadoopIllegalArgumentException.java\"\npublic class org.apache.hadoop.HadoopIllegalArgumentException extends java.lang.IllegalArgumentException {\n  public org.apache.hadoop.HadoopIllegalArgumentException(java.lang.String);\n}\n", 
  "org/apache/hadoop/security/UserGroupInformation$HadoopLoginModule.class": "Compiled from \"UserGroupInformation.java\"\npublic class org.apache.hadoop.security.UserGroupInformation {\n  static final java.lang.String HADOOP_USER_NAME;\n  static final java.lang.String HADOOP_PROXY_USER;\n  static org.apache.hadoop.security.UserGroupInformation$UgiMetrics metrics;\n  public static final java.lang.String HADOOP_TOKEN_FILE_LOCATION;\n  static void setShouldRenewImmediatelyForTests(boolean);\n  public static void setConfiguration(org.apache.hadoop.conf.Configuration);\n  static void reset();\n  public static boolean isSecurityEnabled();\n  org.apache.hadoop.security.UserGroupInformation(javax.security.auth.Subject);\n  public boolean hasKerberosCredentials();\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getCurrentUser() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getBestUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromTicketCache(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation getUGIFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation getLoginUser() throws java.io.IOException;\n  public static java.lang.String trimLoginMethod(java.lang.String);\n  public static synchronized void loginUserFromSubject(javax.security.auth.Subject) throws java.io.IOException;\n  public static synchronized void setLoginUser(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isFromKeytab();\n  public static synchronized void loginUserFromKeytab(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void checkTGTAndReloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromKeytab() throws java.io.IOException;\n  public synchronized void reloginFromTicketCache() throws java.io.IOException;\n  public static synchronized org.apache.hadoop.security.UserGroupInformation loginUserFromKeytabAndReturnUGI(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static synchronized boolean isLoginKeytabBased() throws java.io.IOException;\n  public static boolean isLoginTicketBased() throws java.io.IOException;\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String);\n  public static org.apache.hadoop.security.UserGroupInformation createRemoteUser(java.lang.String, org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUser(java.lang.String, org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.security.UserGroupInformation getRealUser();\n  public static org.apache.hadoop.security.UserGroupInformation createUserForTesting(java.lang.String, java.lang.String[]);\n  public static org.apache.hadoop.security.UserGroupInformation createProxyUserForTesting(java.lang.String, org.apache.hadoop.security.UserGroupInformation, java.lang.String[]);\n  public java.lang.String getShortUserName();\n  public java.lang.String getPrimaryGroupName() throws java.io.IOException;\n  public java.lang.String getUserName();\n  public synchronized boolean addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier);\n  public synchronized java.util.Set<org.apache.hadoop.security.token.TokenIdentifier> getTokenIdentifiers();\n  public boolean addToken(org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public boolean addToken(org.apache.hadoop.io.Text, org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public java.util.Collection<org.apache.hadoop.security.token.Token<? extends org.apache.hadoop.security.token.TokenIdentifier>> getTokens();\n  public org.apache.hadoop.security.Credentials getCredentials();\n  public void addCredentials(org.apache.hadoop.security.Credentials);\n  public synchronized java.lang.String[] getGroupNames();\n  public java.lang.String toString();\n  public synchronized void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  public void setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod);\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod();\n  public synchronized org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod();\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  protected javax.security.auth.Subject getSubject();\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedAction<T>);\n  public <T extends java/lang/Object> T doAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static org.apache.commons.logging.Log access$000();\n  static boolean access$100(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod);\n  static java.lang.Class access$200();\n  static java.lang.String access$300();\n  static java.lang.String access$400();\n  static java.lang.String access$500(java.lang.String);\n  static java.lang.String access$600();\n  static org.apache.hadoop.conf.Configuration access$900();\n  static javax.security.auth.kerberos.KerberosTicket access$1000(org.apache.hadoop.security.UserGroupInformation);\n  static long access$1100(org.apache.hadoop.security.UserGroupInformation, javax.security.auth.kerberos.KerberosTicket);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/XAttrCommands$SetfattrCommand.class": "Compiled from \"XAttrCommands.java\"\nclass org.apache.hadoop.fs.shell.XAttrCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.XAttrCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ipc/Server$Connection.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$TraceAdminService$Interface.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JFloat$CppFloat.class": "Compiled from \"JFloat.java\"\npublic class org.apache.hadoop.record.compiler.JFloat extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JFloat();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/net/ScriptBasedMapping.class": "Compiled from \"ScriptBasedMapping.java\"\npublic class org.apache.hadoop.net.ScriptBasedMapping extends org.apache.hadoop.net.CachedDNSToSwitchMapping {\n  static final int MIN_ALLOWABLE_ARGS;\n  static final int DEFAULT_ARG_COUNT;\n  static final java.lang.String SCRIPT_FILENAME_KEY;\n  static final java.lang.String SCRIPT_ARG_COUNT_KEY;\n  public static final java.lang.String NO_SCRIPT;\n  public org.apache.hadoop.net.ScriptBasedMapping();\n  public org.apache.hadoop.net.ScriptBasedMapping(org.apache.hadoop.net.DNSToSwitchMapping);\n  public org.apache.hadoop.net.ScriptBasedMapping(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.lang.String toString();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n}\n", 
  "org/apache/hadoop/record/XmlRecordOutput.class": "Compiled from \"XmlRecordOutput.java\"\npublic class org.apache.hadoop.record.XmlRecordOutput implements org.apache.hadoop.record.RecordOutput {\n  public org.apache.hadoop.record.XmlRecordOutput(java.io.OutputStream);\n  public void writeByte(byte, java.lang.String) throws java.io.IOException;\n  public void writeBool(boolean, java.lang.String) throws java.io.IOException;\n  public void writeInt(int, java.lang.String) throws java.io.IOException;\n  public void writeLong(long, java.lang.String) throws java.io.IOException;\n  public void writeFloat(float, java.lang.String) throws java.io.IOException;\n  public void writeDouble(double, java.lang.String) throws java.io.IOException;\n  public void writeString(java.lang.String, java.lang.String) throws java.io.IOException;\n  public void writeBuffer(org.apache.hadoop.record.Buffer, java.lang.String) throws java.io.IOException;\n  public void startRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public void endRecord(org.apache.hadoop.record.Record, java.lang.String) throws java.io.IOException;\n  public void startVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public void endVector(java.util.ArrayList, java.lang.String) throws java.io.IOException;\n  public void startMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n  public void endMap(java.util.TreeMap, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/ProviderUtils.class": "Compiled from \"ProviderUtils.java\"\npublic class org.apache.hadoop.security.ProviderUtils {\n  public org.apache.hadoop.security.ProviderUtils();\n  public static org.apache.hadoop.fs.Path unnestUri(java.net.URI);\n}\n", 
  "org/apache/hadoop/fs/permission/AclEntryScope.class": "Compiled from \"AclEntryScope.java\"\npublic final class org.apache.hadoop.fs.permission.AclEntryScope extends java.lang.Enum<org.apache.hadoop.fs.permission.AclEntryScope> {\n  public static final org.apache.hadoop.fs.permission.AclEntryScope ACCESS;\n  public static final org.apache.hadoop.fs.permission.AclEntryScope DEFAULT;\n  public static org.apache.hadoop.fs.permission.AclEntryScope[] values();\n  public static org.apache.hadoop.fs.permission.AclEntryScope valueOf(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.class": "Compiled from \"HAServiceProtocolClientSideTranslatorPB.java\"\npublic class org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB implements org.apache.hadoop.ha.HAServiceProtocol,java.io.Closeable,org.apache.hadoop.ipc.ProtocolTranslator {\n  public org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public void monitorHealth() throws java.io.IOException;\n  public void transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws java.io.IOException;\n  public void transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo) throws java.io.IOException;\n  public org.apache.hadoop.ha.HAServiceStatus getServiceStatus() throws java.io.IOException;\n  public void close();\n  public java.lang.Object getUnderlyingProxyObject();\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer$QuotingInputFilter.class": "Compiled from \"HttpServer.java\"\npublic class org.apache.hadoop.http.HttpServer implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.Connector listener;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector) throws java.io.IOException;\n  public org.apache.hadoop.http.HttpServer(java.lang.String, java.lang.String, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.AccessControlList, org.mortbay.jetty.Connector, java.lang.String[]) throws java.io.IOException;\n  public org.mortbay.jetty.Connector createBaseListener(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean) throws java.io.IOException;\n  protected void addContext(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public void setThreads(int, int);\n  public void addSslListener(java.net.InetSocketAddress, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void addSslListener(java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  protected void initSpnego(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void start() throws java.io.IOException;\n  void openListener() throws java.lang.Exception;\n  public java.net.InetSocketAddress getListenerAddress();\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  static org.apache.hadoop.security.ssl.SSLFactory access$000(org.apache.hadoop.http.HttpServer);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JInt.class": "Compiled from \"JInt.java\"\npublic class org.apache.hadoop.record.compiler.JInt extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JInt();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/fs/shell/MoveCommands$MoveToLocal.class": "Compiled from \"MoveCommands.java\"\nclass org.apache.hadoop.fs.shell.MoveCommands {\n  org.apache.hadoop.fs.shell.MoveCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/io/compress/BZip2Codec.class": "Compiled from \"BZip2Codec.java\"\npublic class org.apache.hadoop.io.compress.BZip2Codec implements org.apache.hadoop.conf.Configurable,org.apache.hadoop.io.compress.SplittableCompressionCodec {\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public org.apache.hadoop.io.compress.BZip2Codec();\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream, org.apache.hadoop.io.compress.Compressor) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Compressor> getCompressorType();\n  public org.apache.hadoop.io.compress.Compressor createCompressor();\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.SplitCompressionInputStream createInputStream(java.io.InputStream, org.apache.hadoop.io.compress.Decompressor, long, long, org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE) throws java.io.IOException;\n  public java.lang.Class<? extends org.apache.hadoop.io.compress.Decompressor> getDecompressorType();\n  public org.apache.hadoop.io.compress.Decompressor createDecompressor();\n  public java.lang.String getDefaultExtension();\n  static int access$000();\n  static int access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.class": "Compiled from \"CBZip2OutputStream.java\"\npublic class org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream extends java.io.OutputStream implements org.apache.hadoop.io.compress.bzip2.BZip2Constants {\n  public static final int MIN_BLOCKSIZE;\n  public static final int MAX_BLOCKSIZE;\n  protected static final int SETMASK;\n  protected static final int CLEARMASK;\n  protected static final int GREATER_ICOST;\n  protected static final int LESSER_ICOST;\n  protected static final int SMALL_THRESH;\n  protected static final int DEPTH_THRESH;\n  protected static final int WORK_FACTOR;\n  protected static final int QSORT_STACK_SIZE;\n  protected static void hbMakeCodeLengths(char[], int[], int, int);\n  public static int chooseBlockSize(long);\n  public org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream(java.io.OutputStream) throws java.io.IOException;\n  public org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream(java.io.OutputStream, int) throws java.io.IOException;\n  public void write(int) throws java.io.IOException;\n  protected void finalize() throws java.lang.Throwable;\n  public void finish() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void flush() throws java.io.IOException;\n  public final int getBlockSize();\n  public void write(byte[], int, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/CodecPool.class": "Compiled from \"CodecPool.java\"\npublic class org.apache.hadoop.io.compress.CodecPool {\n  public org.apache.hadoop.io.compress.CodecPool();\n  public static org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec);\n  public static org.apache.hadoop.io.compress.Decompressor getDecompressor(org.apache.hadoop.io.compress.CompressionCodec);\n  public static void returnCompressor(org.apache.hadoop.io.compress.Compressor);\n  public static void returnDecompressor(org.apache.hadoop.io.compress.Decompressor);\n  public static int getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec);\n  public static int getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec);\n  static {};\n}\n", 
  "org/apache/hadoop/util/Options$FSDataInputStreamOption.class": "Compiled from \"Options.java\"\npublic class org.apache.hadoop.util.Options {\n  public org.apache.hadoop.util.Options();\n  public static <base extends java/lang/Object, T extends base> T getOption(java.lang.Class<T>, base[]) throws java.io.IOException;\n  public static <T extends java/lang/Object> T[] prependOptions(T[], T...);\n}\n", 
  "org/apache/hadoop/security/SecurityUtil$StandardHostResolver.class": "Compiled from \"SecurityUtil.java\"\npublic class org.apache.hadoop.security.SecurityUtil {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final java.lang.String HOSTNAME_PATTERN;\n  public static final java.lang.String FAILED_TO_GET_UGI_MSG_HEADER;\n  static boolean useIpForTokenService;\n  static org.apache.hadoop.security.SecurityUtil$HostResolver hostResolver;\n  public org.apache.hadoop.security.SecurityUtil();\n  public static void setTokenServiceUseIp(boolean);\n  static boolean isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal);\n  protected static boolean isOriginalTGT(javax.security.auth.kerberos.KerberosTicket);\n  public static java.lang.String getServerPrincipal(java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String getServerPrincipal(java.lang.String, java.net.InetAddress) throws java.io.IOException;\n  static java.lang.String getLocalHostName() throws java.net.UnknownHostException;\n  public static void login(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void login(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String buildDTServiceName(java.net.URI, int);\n  public static java.lang.String getHostFromPrincipal(java.lang.String);\n  public static void setSecurityInfoProviders(org.apache.hadoop.security.SecurityInfo...);\n  public static org.apache.hadoop.security.KerberosInfo getKerberosInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.security.token.TokenInfo getTokenInfo(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static java.net.InetSocketAddress getTokenServiceAddr(org.apache.hadoop.security.token.Token<?>);\n  public static void setTokenService(org.apache.hadoop.security.token.Token<?>, java.net.InetSocketAddress);\n  public static org.apache.hadoop.io.Text buildTokenService(java.net.InetSocketAddress);\n  public static org.apache.hadoop.io.Text buildTokenService(java.net.URI);\n  public static <T extends java/lang/Object> T doAsLoginUserOrFatal(java.security.PrivilegedAction<T>);\n  public static <T extends java/lang/Object> T doAsLoginUser(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException;\n  public static <T extends java/lang/Object> T doAsCurrentUser(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException;\n  public static java.net.InetAddress getByName(java.lang.String) throws java.net.UnknownHostException;\n  public static org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod(org.apache.hadoop.conf.Configuration);\n  public static void setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod, org.apache.hadoop.conf.Configuration);\n  public static boolean isPrivilegedPort(int);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$7.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Sorter$SegmentContainer.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/Interns$1.class": "Compiled from \"Interns.java\"\npublic class org.apache.hadoop.metrics2.lib.Interns {\n  static final int MAX_INFO_NAMES;\n  static final int MAX_INFO_DESCS;\n  static final int MAX_TAG_NAMES;\n  static final int MAX_TAG_VALUES;\n  public org.apache.hadoop.metrics2.lib.Interns();\n  public static org.apache.hadoop.metrics2.MetricsInfo info(java.lang.String, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  public static org.apache.hadoop.metrics2.MetricsTag tag(java.lang.String, java.lang.String, java.lang.String);\n  static org.apache.commons.logging.Log access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/util/LightWeightGSet$SetIterator.class": "Compiled from \"LightWeightGSet.java\"\npublic class org.apache.hadoop.util.LightWeightGSet<K, E extends K> implements org.apache.hadoop.util.GSet<K, E> {\n  static final int MAX_ARRAY_LENGTH;\n  static final int MIN_ARRAY_LENGTH;\n  public org.apache.hadoop.util.LightWeightGSet(int);\n  public int size();\n  public E get(K);\n  public boolean contains(K);\n  public E put(E);\n  public E remove(K);\n  public java.util.Iterator<E> iterator();\n  public java.lang.String toString();\n  public void printDetails(java.io.PrintStream);\n  public static int computeCapacity(double, java.lang.String);\n  static int computeCapacity(long, double, java.lang.String);\n  public void clear();\n  static int access$000(org.apache.hadoop.util.LightWeightGSet);\n  static org.apache.hadoop.util.LightWeightGSet$LinkedElement[] access$100(org.apache.hadoop.util.LightWeightGSet);\n  static java.lang.Object access$200(org.apache.hadoop.util.LightWeightGSet, org.apache.hadoop.util.LightWeightGSet$LinkedElement);\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Reader.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/permission/ScopedAclEntries.class": "Compiled from \"ScopedAclEntries.java\"\npublic final class org.apache.hadoop.fs.permission.ScopedAclEntries {\n  public org.apache.hadoop.fs.permission.ScopedAclEntries(java.util.List<org.apache.hadoop.fs.permission.AclEntry>);\n  public java.util.List<org.apache.hadoop.fs.permission.AclEntry> getAccessEntries();\n  public java.util.List<org.apache.hadoop.fs.permission.AclEntry> getDefaultEntries();\n}\n", 
  "org/apache/hadoop/jmx/JMXJsonServlet.class": "Compiled from \"JMXJsonServlet.java\"\npublic class org.apache.hadoop.jmx.JMXJsonServlet extends javax.servlet.http.HttpServlet {\n  static final java.lang.String ACCESS_CONTROL_ALLOW_METHODS;\n  static final java.lang.String ACCESS_CONTROL_ALLOW_ORIGIN;\n  protected transient javax.management.MBeanServer mBeanServer;\n  public org.apache.hadoop.jmx.JMXJsonServlet();\n  public void init() throws javax.servlet.ServletException;\n  protected boolean isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/AclCommands.class": "Compiled from \"AclCommands.java\"\nclass org.apache.hadoop.fs.shell.AclCommands extends org.apache.hadoop.fs.shell.FsCommand {\n  org.apache.hadoop.fs.shell.AclCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  static java.lang.String access$000();\n  static java.lang.String access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/util/QuickSort.class": "Compiled from \"QuickSort.java\"\npublic final class org.apache.hadoop.util.QuickSort implements org.apache.hadoop.util.IndexedSorter {\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.util.QuickSort();\n  protected static int getMaxDepth(int);\n  public void sort(org.apache.hadoop.util.IndexedSortable, int, int);\n  public void sort(org.apache.hadoop.util.IndexedSortable, int, int, org.apache.hadoop.util.Progressable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/StreamPumper$1.class": "Compiled from \"StreamPumper.java\"\nclass org.apache.hadoop.ha.StreamPumper {\n  final java.lang.Thread thread;\n  final java.lang.String logPrefix;\n  final org.apache.hadoop.ha.StreamPumper$StreamType type;\n  static final boolean $assertionsDisabled;\n  org.apache.hadoop.ha.StreamPumper(org.apache.commons.logging.Log, java.lang.String, java.io.InputStream, org.apache.hadoop.ha.StreamPumper$StreamType);\n  void join() throws java.lang.InterruptedException;\n  void start();\n  protected void pump() throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricCounterInt.class": "Compiled from \"MetricCounterInt.java\"\nclass org.apache.hadoop.metrics2.impl.MetricCounterInt extends org.apache.hadoop.metrics2.AbstractMetric {\n  final int value;\n  org.apache.hadoop.metrics2.impl.MetricCounterInt(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public java.lang.Integer value();\n  public org.apache.hadoop.metrics2.MetricType type();\n  public void visit(org.apache.hadoop.metrics2.MetricsVisitor);\n  public java.lang.Number value();\n}\n", 
  "org/apache/hadoop/net/SocketIOWithTimeout.class": "Compiled from \"SocketIOWithTimeout.java\"\nabstract class org.apache.hadoop.net.SocketIOWithTimeout {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.net.SocketIOWithTimeout(java.nio.channels.SelectableChannel, long) throws java.io.IOException;\n  void close();\n  boolean isOpen();\n  java.nio.channels.SelectableChannel getChannel();\n  static void checkChannelValidity(java.lang.Object) throws java.io.IOException;\n  abstract int performIO(java.nio.ByteBuffer) throws java.io.IOException;\n  int doIO(java.nio.ByteBuffer, int) throws java.io.IOException;\n  static void connect(java.nio.channels.SocketChannel, java.net.SocketAddress, int) throws java.io.IOException;\n  void waitForIO(int) throws java.io.IOException;\n  public void setTimeout(long);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/MetricsConfig$2.class": "Compiled from \"MetricsConfig.java\"\nclass org.apache.hadoop.metrics2.impl.MetricsConfig extends org.apache.commons.configuration.SubsetConfiguration {\n  static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String DEFAULT_FILE_NAME;\n  static final java.lang.String PREFIX_DEFAULT;\n  static final java.lang.String PERIOD_KEY;\n  static final int PERIOD_DEFAULT;\n  static final java.lang.String QUEUE_CAPACITY_KEY;\n  static final int QUEUE_CAPACITY_DEFAULT;\n  static final java.lang.String RETRY_DELAY_KEY;\n  static final int RETRY_DELAY_DEFAULT;\n  static final java.lang.String RETRY_BACKOFF_KEY;\n  static final int RETRY_BACKOFF_DEFAULT;\n  static final java.lang.String RETRY_COUNT_KEY;\n  static final int RETRY_COUNT_DEFAULT;\n  static final java.lang.String JMX_CACHE_TTL_KEY;\n  static final java.lang.String START_MBEANS_KEY;\n  static final java.lang.String PLUGIN_URLS_KEY;\n  static final java.lang.String CONTEXT_KEY;\n  static final java.lang.String NAME_KEY;\n  static final java.lang.String DESC_KEY;\n  static final java.lang.String SOURCE_KEY;\n  static final java.lang.String SINK_KEY;\n  static final java.lang.String METRIC_FILTER_KEY;\n  static final java.lang.String RECORD_FILTER_KEY;\n  static final java.lang.String SOURCE_FILTER_KEY;\n  static final java.util.regex.Pattern INSTANCE_REGEX;\n  static final com.google.common.base.Splitter SPLITTER;\n  org.apache.hadoop.metrics2.impl.MetricsConfig(org.apache.commons.configuration.Configuration, java.lang.String);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig create(java.lang.String);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig create(java.lang.String, java.lang.String...);\n  static org.apache.hadoop.metrics2.impl.MetricsConfig loadFirst(java.lang.String, java.lang.String...);\n  public org.apache.hadoop.metrics2.impl.MetricsConfig subset(java.lang.String);\n  java.util.Map<java.lang.String, org.apache.hadoop.metrics2.impl.MetricsConfig> getInstanceConfigs(java.lang.String);\n  java.lang.Iterable<java.lang.String> keys();\n  public java.lang.Object getProperty(java.lang.String);\n  <T extends org/apache/hadoop/metrics2/MetricsPlugin> T getPlugin(java.lang.String);\n  java.lang.String getClassName(java.lang.String);\n  java.lang.ClassLoader getPluginLoader();\n  public void clear();\n  org.apache.hadoop.metrics2.MetricsFilter getFilter(java.lang.String);\n  public java.lang.String toString();\n  static java.lang.String toString(org.apache.commons.configuration.Configuration);\n  public org.apache.commons.configuration.Configuration subset(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/CredentialShell$1.class": "Compiled from \"CredentialShell.java\"\npublic class org.apache.hadoop.security.alias.CredentialShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.security.alias.CredentialShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int init(java.lang.String[]) throws java.io.IOException;\n  protected char[] promptForCredential() throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialShell$PasswordReader getPasswordReader();\n  public void setPasswordReader(org.apache.hadoop.security.alias.CredentialShell$PasswordReader);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.security.alias.CredentialShell);\n  static boolean access$300(org.apache.hadoop.security.alias.CredentialShell);\n  static java.lang.String access$400(org.apache.hadoop.security.alias.CredentialShell);\n}\n", 
  "org/apache/hadoop/service/ServiceOperations$ServiceListeners.class": "Compiled from \"ServiceOperations.java\"\npublic final class org.apache.hadoop.service.ServiceOperations {\n  public static void stop(org.apache.hadoop.service.Service);\n  public static java.lang.Exception stopQuietly(org.apache.hadoop.service.Service);\n  public static java.lang.Exception stopQuietly(org.apache.commons.logging.Log, org.apache.hadoop.service.Service);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RPC$Server$ProtoNameVer.class": "Compiled from \"RPC.java\"\npublic class org.apache.hadoop.ipc.RPC {\n  static final int RPC_SERVICE_CLASS_DEFAULT;\n  static final org.apache.commons.logging.Log LOG;\n  static java.lang.Class<?>[] getSuperInterfaces(java.lang.Class<?>[]);\n  static java.lang.Class<?>[] getProtocolInterfaces(java.lang.Class<?>);\n  public static java.lang.String getProtocolName(java.lang.Class<?>);\n  public static long getProtocolVersion(java.lang.Class<?>);\n  public static void setProtocolEngine(org.apache.hadoop.conf.Configuration, java.lang.Class<?>, java.lang.Class<?>);\n  static synchronized org.apache.hadoop.ipc.RpcEngine getProtocolEngine(java.lang.Class<?>, org.apache.hadoop.conf.Configuration);\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T waitForProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> waitForProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.io.retry.RetryPolicy, long) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public static <T extends java/lang/Object> T getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.InetSocketAddress getServerAddress(java.lang.Object);\n  public static org.apache.hadoop.ipc.Client$ConnectionId getConnectionIdForProxy(java.lang.Object);\n  public static <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProtocolProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void stopProxy(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/ha/ServiceFailedException.class": "Compiled from \"ServiceFailedException.java\"\npublic class org.apache.hadoop.ha.ServiceFailedException extends java.io.IOException {\n  public org.apache.hadoop.ha.ServiceFailedException(java.lang.String);\n  public org.apache.hadoop.ha.ServiceFailedException(java.lang.String, java.lang.Throwable);\n}\n", 
  "org/apache/hadoop/io/file/tfile/CompareUtils$MemcmpRawComparator.class": "Compiled from \"CompareUtils.java\"\nclass org.apache.hadoop.io.file.tfile.CompareUtils {\n}\n", 
  "org/apache/hadoop/fs/shell/CopyCommands$CopyToLocal.class": "Compiled from \"CopyCommands.java\"\nclass org.apache.hadoop.fs.shell.CopyCommands {\n  org.apache.hadoop.fs.shell.CopyCommands();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$1.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminProtocolPB.class": "Compiled from \"TraceAdminProtocolPB.java\"\npublic interface org.apache.hadoop.tracing.TraceAdminProtocolPB extends org.apache.hadoop.tracing.TraceAdminPB$TraceAdminService$BlockingInterface,org.apache.hadoop.ipc.VersionedProtocol {\n}\n", 
  "org/apache/hadoop/io/WritableUtils.class": "Compiled from \"WritableUtils.java\"\npublic final class org.apache.hadoop.io.WritableUtils {\n  public org.apache.hadoop.io.WritableUtils();\n  public static byte[] readCompressedByteArray(java.io.DataInput) throws java.io.IOException;\n  public static void skipCompressedByteArray(java.io.DataInput) throws java.io.IOException;\n  public static int writeCompressedByteArray(java.io.DataOutput, byte[]) throws java.io.IOException;\n  public static java.lang.String readCompressedString(java.io.DataInput) throws java.io.IOException;\n  public static int writeCompressedString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public static void writeString(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public static java.lang.String readString(java.io.DataInput) throws java.io.IOException;\n  public static void writeStringArray(java.io.DataOutput, java.lang.String[]) throws java.io.IOException;\n  public static void writeCompressedStringArray(java.io.DataOutput, java.lang.String[]) throws java.io.IOException;\n  public static java.lang.String[] readStringArray(java.io.DataInput) throws java.io.IOException;\n  public static java.lang.String[] readCompressedStringArray(java.io.DataInput) throws java.io.IOException;\n  public static void displayByteArray(byte[]);\n  public static <T extends org/apache/hadoop/io/Writable> T clone(T, org.apache.hadoop.conf.Configuration);\n  public static void cloneInto(org.apache.hadoop.io.Writable, org.apache.hadoop.io.Writable) throws java.io.IOException;\n  public static void writeVInt(java.io.DataOutput, int) throws java.io.IOException;\n  public static void writeVLong(java.io.DataOutput, long) throws java.io.IOException;\n  public static long readVLong(java.io.DataInput) throws java.io.IOException;\n  public static int readVInt(java.io.DataInput) throws java.io.IOException;\n  public static int readVIntInRange(java.io.DataInput, int, int) throws java.io.IOException;\n  public static boolean isNegativeVInt(byte);\n  public static int decodeVIntSize(byte);\n  public static int getVIntSize(long);\n  public static <T extends java/lang/Enum<T>> T readEnum(java.io.DataInput, java.lang.Class<T>) throws java.io.IOException;\n  public static void writeEnum(java.io.DataOutput, java.lang.Enum<?>) throws java.io.IOException;\n  public static void skipFully(java.io.DataInput, int) throws java.io.IOException;\n  public static byte[] toByteArray(org.apache.hadoop.io.Writable...);\n  public static java.lang.String readStringSafely(java.io.DataInput, int) throws java.io.IOException, java.lang.IllegalArgumentException;\n}\n", 
  "org/apache/hadoop/fs/shell/find/Name.class": "Compiled from \"Name.java\"\nfinal class org.apache.hadoop.fs.shell.find.Name extends org.apache.hadoop.fs.shell.find.BaseExpression {\n  public static void registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Name();\n  public void addArguments(java.util.Deque<java.lang.String>);\n  public void prepare() throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  org.apache.hadoop.fs.shell.find.Name(boolean, org.apache.hadoop.fs.shell.find.Name$1);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/RpcMultiplexer.class": "Compiled from \"RpcMultiplexer.java\"\npublic interface org.apache.hadoop.ipc.RpcMultiplexer {\n  public abstract int getAndAdvanceCurrentIndex();\n}\n", 
  "org/apache/hadoop/net/NetworkTopology$InvalidTopologyException.class": "Compiled from \"NetworkTopology.java\"\npublic class org.apache.hadoop.net.NetworkTopology {\n  public static final java.lang.String DEFAULT_RACK;\n  public static final int DEFAULT_HOST_LEVEL;\n  public static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.net.NetworkTopology$InnerNode clusterMap;\n  protected int numOfRacks;\n  protected java.util.concurrent.locks.ReadWriteLock netlock;\n  public static org.apache.hadoop.net.NetworkTopology getInstance(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.net.NetworkTopology();\n  public void add(org.apache.hadoop.net.Node);\n  protected org.apache.hadoop.net.Node getNodeForNetworkLocation(org.apache.hadoop.net.Node);\n  public java.util.List<org.apache.hadoop.net.Node> getDatanodesInRack(java.lang.String);\n  public void remove(org.apache.hadoop.net.Node);\n  public boolean contains(org.apache.hadoop.net.Node);\n  public org.apache.hadoop.net.Node getNode(java.lang.String);\n  public java.lang.String getRack(java.lang.String);\n  public int getNumOfRacks();\n  public int getNumOfLeaves();\n  public int getDistance(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isOnSameRack(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public boolean isNodeGroupAware();\n  public boolean isOnSameNodeGroup(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  protected boolean isSameParents(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  void setRandomSeed(long);\n  public org.apache.hadoop.net.Node chooseRandom(java.lang.String);\n  public java.util.List<org.apache.hadoop.net.Node> getLeaves(java.lang.String);\n  public int countNumOfAvailableNodes(java.lang.String, java.util.Collection<org.apache.hadoop.net.Node>);\n  public java.lang.String toString();\n  public static java.lang.String getFirstHalf(java.lang.String);\n  public static java.lang.String getLastHalf(java.lang.String);\n  protected int getWeight(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node);\n  public void sortByDistance(org.apache.hadoop.net.Node, org.apache.hadoop.net.Node[], int);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ComparableVersion$IntegerItem.class": "Compiled from \"ComparableVersion.java\"\npublic class org.apache.hadoop.util.ComparableVersion implements java.lang.Comparable<org.apache.hadoop.util.ComparableVersion> {\n  public org.apache.hadoop.util.ComparableVersion(java.lang.String);\n  public final void parseVersion(java.lang.String);\n  public int compareTo(org.apache.hadoop.util.ComparableVersion);\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/fs/FileContext$1.class": "Compiled from \"FileContext.java\"\npublic class org.apache.hadoop.fs.FileContext {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.fs.permission.FsPermission DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission DIR_DEFAULT_PERM;\n  public static final org.apache.hadoop.fs.permission.FsPermission FILE_DEFAULT_PERM;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final java.util.Map<org.apache.hadoop.fs.FileContext, java.util.Set<org.apache.hadoop.fs.Path>> DELETE_ON_EXIT;\n  static final org.apache.hadoop.fs.FileContext$FileContextFinalizer FINALIZER;\n  final boolean resolveSymlinks;\n  org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  static void processDeleteOnExit();\n  protected org.apache.hadoop.fs.AbstractFileSystem getFSofPath(org.apache.hadoop.fs.Path) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem, org.apache.hadoop.conf.Configuration);\n  protected static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.fs.AbstractFileSystem);\n  public static org.apache.hadoop.fs.FileContext getFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext() throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(java.net.URI, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public static org.apache.hadoop.fs.FileContext getLocalFSFileContext(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException;\n  public org.apache.hadoop.fs.AbstractFileSystem getDefaultFileSystem();\n  public void setWorkingDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getWorkingDirectory();\n  public org.apache.hadoop.security.UserGroupInformation getUgi();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.permission.FsPermission getUMask();\n  public void setUMask(org.apache.hadoop.fs.permission.FsPermission);\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.FileNotFoundException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public void setVerifyChecksum(boolean, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.FileContext$Util util();\n  protected org.apache.hadoop.fs.Path resolve(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveIntermediate(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.fs.AbstractFileSystem> resolveAbstractFileSystems(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.net.URI);\n  public static void clearStatistics();\n  public static void printStatistics();\n  public static java.util.Map<java.net.URI, org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.fs.PathFilter access$000();\n  static void access$100(org.apache.hadoop.fs.FileContext, java.lang.String, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  static void access$200(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  static org.apache.hadoop.conf.Configuration access$300(org.apache.hadoop.fs.FileContext);\n  static {};\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicies$RemoteExceptionDependentRetry.class": "Compiled from \"RetryPolicies.java\"\npublic class org.apache.hadoop.io.retry.RetryPolicies {\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.hadoop.io.retry.RetryPolicy TRY_ONCE_THEN_FAIL;\n  public static final org.apache.hadoop.io.retry.RetryPolicy RETRY_FOREVER;\n  public org.apache.hadoop.io.retry.RetryPolicies();\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithFixedSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumTimeWithFixedSleep(long, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryUpToMaximumCountWithProportionalSleep(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy exponentialBackoffRetry(int, long, java.util.concurrent.TimeUnit);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy, java.util.Map<java.lang.Class<? extends java.lang.Exception>, org.apache.hadoop.io.retry.RetryPolicy>);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, long, long);\n  public static final org.apache.hadoop.io.retry.RetryPolicy failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy, int, int, long, long);\n  static org.apache.hadoop.ipc.RetriableException getWrappedRetriableException(java.lang.Exception);\n  static java.lang.ThreadLocal access$000();\n  static long access$100(long, int);\n  static long access$200(long, int, long);\n  static boolean access$300(java.lang.Exception);\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/BCFile$Reader$RBlockState.class": "Compiled from \"BCFile.java\"\nfinal class org.apache.hadoop.io.file.tfile.BCFile {\n  static final org.apache.hadoop.io.file.tfile.Utils$Version API_VERSION;\n  static final org.apache.commons.logging.Log LOG;\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$DeprecatedKeyInfo.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/compress/CompressionInputStream.class": "Compiled from \"CompressionInputStream.java\"\npublic abstract class org.apache.hadoop.io.compress.CompressionInputStream extends java.io.InputStream implements org.apache.hadoop.fs.Seekable {\n  protected final java.io.InputStream in;\n  protected long maxAvailableData;\n  protected org.apache.hadoop.io.compress.CompressionInputStream(java.io.InputStream) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public abstract int read(byte[], int, int) throws java.io.IOException;\n  public abstract void resetState() throws java.io.IOException;\n  public long getPos() throws java.io.IOException;\n  public void seek(long) throws java.lang.UnsupportedOperationException;\n  public boolean seekToNewSource(long) throws java.lang.UnsupportedOperationException;\n  void setTrackedDecompressor(org.apache.hadoop.io.compress.Decompressor);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RPCTraceInfoProto$Builder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos$1.class": "Compiled from \"IpcConnectionContextProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$2002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tracing/SpanReceiverInfo.class": "Compiled from \"SpanReceiverInfo.java\"\npublic class org.apache.hadoop.tracing.SpanReceiverInfo {\n  final java.util.List<org.apache.hadoop.tracing.SpanReceiverInfo$ConfigurationPair> configPairs;\n  org.apache.hadoop.tracing.SpanReceiverInfo(long, java.lang.String);\n  public long getId();\n  public java.lang.String getClassName();\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto$1.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/ArrayWritable.class": "Compiled from \"ArrayWritable.java\"\npublic class org.apache.hadoop.io.ArrayWritable implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.io.ArrayWritable(java.lang.Class<? extends org.apache.hadoop.io.Writable>);\n  public org.apache.hadoop.io.ArrayWritable(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.io.Writable[]);\n  public org.apache.hadoop.io.ArrayWritable(java.lang.String[]);\n  public java.lang.Class getValueClass();\n  public java.lang.String[] toStrings();\n  public java.lang.Object toArray();\n  public void set(org.apache.hadoop.io.Writable[]);\n  public org.apache.hadoop.io.Writable[] get();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$HAServiceProtocolService.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/authorize/AccessControlList$1.class": "Compiled from \"AccessControlList.java\"\npublic class org.apache.hadoop.security.authorize.AccessControlList implements org.apache.hadoop.io.Writable {\n  public static final java.lang.String WILDCARD_ACL_VALUE;\n  public org.apache.hadoop.security.authorize.AccessControlList();\n  public org.apache.hadoop.security.authorize.AccessControlList(java.lang.String);\n  public org.apache.hadoop.security.authorize.AccessControlList(java.lang.String, java.lang.String);\n  public boolean isAllAllowed();\n  public void addUser(java.lang.String);\n  public void addGroup(java.lang.String);\n  public void removeUser(java.lang.String);\n  public void removeGroup(java.lang.String);\n  public java.util.Collection<java.lang.String> getUsers();\n  public java.util.Collection<java.lang.String> getGroups();\n  public final boolean isUserInList(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isUserAllowed(org.apache.hadoop.security.UserGroupInformation);\n  public java.lang.String toString();\n  public java.lang.String getAclString();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/http/HttpServer2$SelectChannelConnectorWithSafeStartup.class": "Compiled from \"HttpServer2.java\"\npublic final class org.apache.hadoop.http.HttpServer2 implements org.apache.hadoop.http.FilterContainer {\n  public static final org.apache.commons.logging.Log LOG;\n  static final java.lang.String FILTER_INITIALIZER_PROPERTY;\n  public static final java.lang.String HTTP_MAX_THREADS;\n  public static final java.lang.String CONF_CONTEXT_ATTRIBUTE;\n  public static final java.lang.String ADMINS_ACL;\n  public static final java.lang.String SPNEGO_FILTER;\n  public static final java.lang.String NO_CACHE_FILTER;\n  public static final java.lang.String BIND_ADDRESS;\n  protected final org.mortbay.jetty.Server webServer;\n  protected final org.mortbay.jetty.webapp.WebAppContext webAppContext;\n  protected final boolean findPort;\n  protected final java.util.Map<org.mortbay.jetty.servlet.Context, java.lang.Boolean> defaultContexts;\n  protected final java.util.List<java.lang.String> filterNames;\n  static final java.lang.String STATE_DESCRIPTION_ALIVE;\n  static final java.lang.String STATE_DESCRIPTION_NOT_LIVE;\n  public static org.mortbay.jetty.Connector createDefaultChannelConnector();\n  protected void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected void addDefaultServlets();\n  public void addContext(org.mortbay.jetty.servlet.Context, boolean);\n  public void setAttribute(java.lang.String, java.lang.Object);\n  public void addJerseyResourcePackage(java.lang.String, java.lang.String);\n  public void addServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>);\n  public void addInternalServlet(java.lang.String, java.lang.String, java.lang.Class<? extends javax.servlet.http.HttpServlet>, boolean);\n  public void addFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public void addGlobalFilter(java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>);\n  public static void defineFilter(org.mortbay.jetty.servlet.Context, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.lang.String>, java.lang.String[]);\n  protected void addFilterPathMapping(java.lang.String, org.mortbay.jetty.servlet.Context);\n  public java.lang.Object getAttribute(java.lang.String);\n  public org.mortbay.jetty.webapp.WebAppContext getWebAppContext();\n  protected java.lang.String getWebAppsPath(java.lang.String) throws java.io.FileNotFoundException;\n  public int getPort();\n  public java.net.InetSocketAddress getConnectorAddress(int);\n  public void setThreads(int, int);\n  public void start() throws java.io.IOException;\n  void openListeners() throws java.lang.Exception;\n  public void stop() throws java.lang.Exception;\n  public void join() throws java.lang.InterruptedException;\n  public boolean isAlive();\n  public java.lang.String toString();\n  public static boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean hasAdministratorAccess(javax.servlet.ServletContext, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static boolean userHasAdministratorAccess(javax.servlet.ServletContext, java.lang.String);\n  org.apache.hadoop.http.HttpServer2(org.apache.hadoop.http.HttpServer2$Builder, org.apache.hadoop.http.HttpServer2$1) throws java.io.IOException;\n  static void access$100(org.apache.hadoop.http.HttpServer2, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  static void access$200(org.apache.hadoop.http.HttpServer2, org.mortbay.jetty.Connector);\n  static void access$300(org.apache.hadoop.http.HttpServer2);\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/UserProvider$1.class": "Compiled from \"UserProvider.java\"\npublic class org.apache.hadoop.security.alias.UserProvider extends org.apache.hadoop.security.alias.CredentialProvider {\n  public static final java.lang.String SCHEME_NAME;\n  public boolean isTransient();\n  public org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry getCredentialEntry(java.lang.String);\n  public org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry createCredentialEntry(java.lang.String, char[]) throws java.io.IOException;\n  public void deleteCredentialEntry(java.lang.String) throws java.io.IOException;\n  public java.lang.String toString();\n  public void flush();\n  public java.util.List<java.lang.String> getAliases() throws java.io.IOException;\n  org.apache.hadoop.security.alias.UserProvider(org.apache.hadoop.security.alias.UserProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$TraceAdminService.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/MetricsRecord.class": "Compiled from \"MetricsRecord.java\"\npublic interface org.apache.hadoop.metrics2.MetricsRecord {\n  public abstract long timestamp();\n  public abstract java.lang.String name();\n  public abstract java.lang.String description();\n  public abstract java.lang.String context();\n  public abstract java.util.Collection<org.apache.hadoop.metrics2.MetricsTag> tags();\n  public abstract java.lang.Iterable<org.apache.hadoop.metrics2.AbstractMetric> metrics();\n}\n", 
  "org/apache/hadoop/io/FastByteComparisons$Comparer.class": "Compiled from \"FastByteComparisons.java\"\nabstract class org.apache.hadoop.io.FastByteComparisons {\n  static final org.apache.commons.logging.Log LOG;\n  org.apache.hadoop.io.FastByteComparisons();\n  public static int compareTo(byte[], int, int, byte[], int, int);\n  static org.apache.hadoop.io.FastByteComparisons$Comparer access$000();\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$MonitorHealthRequestProto.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/net/CachedDNSToSwitchMapping.class": "Compiled from \"CachedDNSToSwitchMapping.java\"\npublic class org.apache.hadoop.net.CachedDNSToSwitchMapping extends org.apache.hadoop.net.AbstractDNSToSwitchMapping {\n  protected final org.apache.hadoop.net.DNSToSwitchMapping rawMapping;\n  public org.apache.hadoop.net.CachedDNSToSwitchMapping(org.apache.hadoop.net.DNSToSwitchMapping);\n  public java.util.List<java.lang.String> resolve(java.util.List<java.lang.String>);\n  public java.util.Map<java.lang.String, java.lang.String> getSwitchMap();\n  public java.lang.String toString();\n  public boolean isSingleSwitch();\n  public void reloadCachedMappings();\n  public void reloadCachedMappings(java.util.List<java.lang.String>);\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos$GetProtocolSignatureResponseProto$Builder.class": "Compiled from \"ProtocolInfoProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2000();\n  static com.google.protobuf.Descriptors$Descriptor access$2800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2900();\n  static com.google.protobuf.Descriptors$Descriptor access$3800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3900();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$5902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/Find$1.class": "Compiled from \"Find.java\"\npublic class org.apache.hadoop.fs.shell.find.Find extends org.apache.hadoop.fs.shell.FsCommand {\n  public static final java.lang.String NAME;\n  public static final java.lang.String USAGE;\n  public static final java.lang.String DESCRIPTION;\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  public org.apache.hadoop.fs.shell.find.Find();\n  protected void processOptions(java.util.LinkedList<java.lang.String>) throws java.io.IOException;\n  void setRootExpression(org.apache.hadoop.fs.shell.find.Expression);\n  org.apache.hadoop.fs.shell.find.Expression getRootExpression();\n  org.apache.hadoop.fs.shell.find.FindOptions getOptions();\n  protected void recursePath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected boolean isPathRecursable(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void postProcessPath(org.apache.hadoop.fs.shell.PathData) throws java.io.IOException;\n  protected void processArguments(java.util.LinkedList<org.apache.hadoop.fs.shell.PathData>) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.class": "Compiled from \"ZKFCProtocolServerSideTranslatorPB.java\"\npublic class org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB implements org.apache.hadoop.ha.protocolPB.ZKFCProtocolPB {\n  public org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB(org.apache.hadoop.ha.ZKFCProtocol);\n  public org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveResponseProto cedeActive(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto) throws com.google.protobuf.ServiceException;\n  public org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverResponseProto gracefulFailover(com.google.protobuf.RpcController, org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto) throws com.google.protobuf.ServiceException;\n  public long getProtocolVersion(java.lang.String, long) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(java.lang.String, long, int) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/security/token/SecretManager.class": "Compiled from \"SecretManager.java\"\npublic abstract class org.apache.hadoop.security.token.SecretManager<T extends org.apache.hadoop.security.token.TokenIdentifier> {\n  public org.apache.hadoop.security.token.SecretManager();\n  protected abstract byte[] createPassword(T);\n  public abstract byte[] retrievePassword(T) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public byte[] retriableRetrievePassword(T) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, org.apache.hadoop.ipc.StandbyException, org.apache.hadoop.ipc.RetriableException, java.io.IOException;\n  public abstract T createIdentifier();\n  public void checkAvailableForRead() throws org.apache.hadoop.ipc.StandbyException;\n  protected javax.crypto.SecretKey generateSecret();\n  protected static byte[] createPassword(byte[], javax.crypto.SecretKey);\n  protected static javax.crypto.SecretKey createSecretKey(byte[]);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFs$MountPoint.class": "Compiled from \"ViewFs.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFs extends org.apache.hadoop.fs.AbstractFileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  final org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.AbstractFileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  org.apache.hadoop.fs.viewfs.ViewFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus() throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setVerifyChecksum(boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFs$MountPoint[] getMountPoints();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public boolean isValidName(java.lang.String);\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystemLinkResolver.class": "Compiled from \"FileSystemLinkResolver.java\"\npublic abstract class org.apache.hadoop.fs.FileSystemLinkResolver<T> {\n  public org.apache.hadoop.fs.FileSystemLinkResolver();\n  public abstract T doCall(org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public abstract T next(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public T resolve(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/IntrusiveCollection$1.class": "Compiled from \"IntrusiveCollection.java\"\npublic class org.apache.hadoop.util.IntrusiveCollection<E extends org.apache.hadoop.util.IntrusiveCollection$Element> implements java.util.Collection<E> {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.util.IntrusiveCollection();\n  public java.util.Iterator<E> iterator();\n  public int size();\n  public boolean isEmpty();\n  public boolean contains(java.lang.Object);\n  public java.lang.Object[] toArray();\n  public <T extends java/lang/Object> T[] toArray(T[]);\n  public boolean add(E);\n  public boolean addFirst(org.apache.hadoop.util.IntrusiveCollection$Element);\n  public boolean remove(java.lang.Object);\n  public boolean containsAll(java.util.Collection<?>);\n  public boolean addAll(java.util.Collection<? extends E>);\n  public boolean removeAll(java.util.Collection<?>);\n  public boolean retainAll(java.util.Collection<?>);\n  public void clear();\n  public boolean add(java.lang.Object);\n  static org.apache.hadoop.util.IntrusiveCollection$Element access$000(org.apache.hadoop.util.IntrusiveCollection);\n  static org.apache.hadoop.util.IntrusiveCollection$Element access$100(org.apache.hadoop.util.IntrusiveCollection, org.apache.hadoop.util.IntrusiveCollection$Element);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/kms/ValueQueue$SyncGenerationPolicy.class": "Compiled from \"ValueQueue.java\"\npublic class org.apache.hadoop.crypto.key.kms.ValueQueue<E> {\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public org.apache.hadoop.crypto.key.kms.ValueQueue(int, float, long, int, org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller<E>);\n  public void initializeQueuesForKeys(java.lang.String...) throws java.util.concurrent.ExecutionException;\n  public E getNext(java.lang.String) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void drain(java.lang.String);\n  public int getSize(java.lang.String) throws java.util.concurrent.ExecutionException;\n  public java.util.List<E> getAtMost(java.lang.String, int) throws java.io.IOException, java.util.concurrent.ExecutionException;\n  public void shutdown();\n  static int access$200(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static float access$300(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller access$400(org.apache.hadoop.crypto.key.kms.ValueQueue);\n  static {};\n}\n", 
  "org/apache/hadoop/record/BinaryRecordInput$1.class": "Compiled from \"BinaryRecordInput.java\"\npublic class org.apache.hadoop.record.BinaryRecordInput implements org.apache.hadoop.record.RecordInput {\n  public static org.apache.hadoop.record.BinaryRecordInput get(java.io.DataInput);\n  public org.apache.hadoop.record.BinaryRecordInput(java.io.InputStream);\n  public org.apache.hadoop.record.BinaryRecordInput(java.io.DataInput);\n  public byte readByte(java.lang.String) throws java.io.IOException;\n  public boolean readBool(java.lang.String) throws java.io.IOException;\n  public int readInt(java.lang.String) throws java.io.IOException;\n  public long readLong(java.lang.String) throws java.io.IOException;\n  public float readFloat(java.lang.String) throws java.io.IOException;\n  public double readDouble(java.lang.String) throws java.io.IOException;\n  public java.lang.String readString(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Buffer readBuffer(java.lang.String) throws java.io.IOException;\n  public void startRecord(java.lang.String) throws java.io.IOException;\n  public void endRecord(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startVector(java.lang.String) throws java.io.IOException;\n  public void endVector(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.record.Index startMap(java.lang.String) throws java.io.IOException;\n  public void endMap(java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.record.BinaryRecordInput(org.apache.hadoop.record.BinaryRecordInput$1);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RPCTraceInfoProtoOrBuilder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/FsUsage$Dus.class": "Compiled from \"FsUsage.java\"\nclass org.apache.hadoop.fs.shell.FsUsage extends org.apache.hadoop.fs.shell.FsCommand {\n  protected boolean humanReadable;\n  protected org.apache.hadoop.fs.shell.FsUsage$TableBuilder usagesTable;\n  org.apache.hadoop.fs.shell.FsUsage();\n  public static void registerCommands(org.apache.hadoop.fs.shell.CommandFactory);\n  protected java.lang.String formatSize(long);\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts$ReplicationFactor.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/fs/FSExceptionMessages.class": "Compiled from \"FSExceptionMessages.java\"\npublic class org.apache.hadoop.fs.FSExceptionMessages {\n  public static final java.lang.String STREAM_IS_CLOSED;\n  public static final java.lang.String NEGATIVE_SEEK;\n  public static final java.lang.String CANNOT_SEEK_PAST_EOF;\n  public org.apache.hadoop.fs.FSExceptionMessages();\n}\n", 
  "org/apache/hadoop/crypto/key/KeyShell$CreateCommand.class": "Compiled from \"KeyShell.java\"\npublic class org.apache.hadoop.crypto.key.KeyShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.crypto.key.KeyShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.crypto.key.KeyShell);\n  static boolean access$300(org.apache.hadoop.crypto.key.KeyShell);\n}\n", 
  "org/apache/hadoop/fs/Options$CreateOpts$BlockSize.class": "Compiled from \"Options.java\"\npublic final class org.apache.hadoop.fs.Options {\n  public org.apache.hadoop.fs.Options();\n}\n", 
  "org/apache/hadoop/fs/XAttrCodec.class": "Compiled from \"XAttrCodec.java\"\npublic final class org.apache.hadoop.fs.XAttrCodec extends java.lang.Enum<org.apache.hadoop.fs.XAttrCodec> {\n  public static final org.apache.hadoop.fs.XAttrCodec TEXT;\n  public static final org.apache.hadoop.fs.XAttrCodec HEX;\n  public static final org.apache.hadoop.fs.XAttrCodec BASE64;\n  public static org.apache.hadoop.fs.XAttrCodec[] values();\n  public static org.apache.hadoop.fs.XAttrCodec valueOf(java.lang.String);\n  public static byte[] decodeValue(java.lang.String) throws java.io.IOException;\n  public static java.lang.String encodeValue(byte[], org.apache.hadoop.fs.XAttrCodec) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/Consts.class": "Compiled from \"Consts.java\"\npublic class org.apache.hadoop.record.compiler.Consts {\n  public static final java.lang.String RIO_PREFIX;\n  public static final java.lang.String RTI_VAR;\n  public static final java.lang.String RTI_FILTER;\n  public static final java.lang.String RTI_FILTER_FIELDS;\n  public static final java.lang.String RECORD_OUTPUT;\n  public static final java.lang.String RECORD_INPUT;\n  public static final java.lang.String TAG;\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToStandbyRequestProto$1.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/shell/find/Print.class": "Compiled from \"Print.java\"\nfinal class org.apache.hadoop.fs.shell.find.Print extends org.apache.hadoop.fs.shell.find.BaseExpression {\n  public static void registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory) throws java.io.IOException;\n  public org.apache.hadoop.fs.shell.find.Print();\n  public org.apache.hadoop.fs.shell.find.Result apply(org.apache.hadoop.fs.shell.PathData, int) throws java.io.IOException;\n  public boolean isAction();\n  org.apache.hadoop.fs.shell.find.Print(java.lang.String, org.apache.hadoop.fs.shell.find.Print$1);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProvider$KeyVersion.class": "Compiled from \"KeyProvider.java\"\npublic abstract class org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String DEFAULT_CIPHER_NAME;\n  public static final java.lang.String DEFAULT_CIPHER;\n  public static final java.lang.String DEFAULT_BITLENGTH_NAME;\n  public static final int DEFAULT_BITLENGTH;\n  public org.apache.hadoop.crypto.key.KeyProvider(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public static org.apache.hadoop.crypto.key.KeyProvider$Options options(org.apache.hadoop.conf.Configuration);\n  public boolean isTransient();\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  protected byte[] generateKey(int, java.lang.String) throws java.security.NoSuchAlgorithmException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public abstract void deleteKey(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public abstract void flush() throws java.io.IOException;\n  public static java.lang.String getBaseName(java.lang.String) throws java.io.IOException;\n  protected static java.lang.String buildVersionName(java.lang.String, int);\n  public static org.apache.hadoop.crypto.key.KeyProvider findProvider(java.util.List<org.apache.hadoop.crypto.key.KeyProvider>, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/util/ComparableVersion$Item.class": "Compiled from \"ComparableVersion.java\"\npublic class org.apache.hadoop.util.ComparableVersion implements java.lang.Comparable<org.apache.hadoop.util.ComparableVersion> {\n  public org.apache.hadoop.util.ComparableVersion(java.lang.String);\n  public final void parseVersion(java.lang.String);\n  public int compareTo(org.apache.hadoop.util.ComparableVersion);\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$RemoveSpanReceiverRequestProto$Builder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/RemoteIterator.class": "Compiled from \"RemoteIterator.java\"\npublic interface org.apache.hadoop.fs.RemoteIterator<E> {\n  public abstract boolean hasNext() throws java.io.IOException;\n  public abstract E next() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$3.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/ZKFCProtocolProtos$ZKFCProtocolService$Stub.class": "Compiled from \"ZKFCProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.ZKFCProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3202(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/crypto/key/KeyProvider$Metadata.class": "Compiled from \"KeyProvider.java\"\npublic abstract class org.apache.hadoop.crypto.key.KeyProvider {\n  public static final java.lang.String DEFAULT_CIPHER_NAME;\n  public static final java.lang.String DEFAULT_CIPHER;\n  public static final java.lang.String DEFAULT_BITLENGTH_NAME;\n  public static final int DEFAULT_BITLENGTH;\n  public org.apache.hadoop.crypto.key.KeyProvider(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public static org.apache.hadoop.crypto.key.KeyProvider$Options options(org.apache.hadoop.conf.Configuration);\n  public boolean isTransient();\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getKeyVersion(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> getKeys() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$Metadata[] getKeysMetadata(java.lang.String...) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.crypto.key.KeyProvider$KeyVersion> getKeyVersions(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion getCurrentKey(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$Metadata getMetadata(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, byte[], org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.io.IOException;\n  protected byte[] generateKey(int, java.lang.String) throws java.security.NoSuchAlgorithmException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion createKey(java.lang.String, org.apache.hadoop.crypto.key.KeyProvider$Options) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public abstract void deleteKey(java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String, byte[]) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.crypto.key.KeyProvider$KeyVersion rollNewVersion(java.lang.String) throws java.security.NoSuchAlgorithmException, java.io.IOException;\n  public abstract void flush() throws java.io.IOException;\n  public static java.lang.String getBaseName(java.lang.String) throws java.io.IOException;\n  protected static java.lang.String buildVersionName(java.lang.String, int);\n  public static org.apache.hadoop.crypto.key.KeyProvider findProvider(java.util.List<org.apache.hadoop.crypto.key.KeyProvider>, java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$SpanReceiverListInfo.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/ViewFs$1.class": "Compiled from \"ViewFs.java\"\npublic class org.apache.hadoop.fs.viewfs.ViewFs extends org.apache.hadoop.fs.AbstractFileSystem {\n  final long creationTime;\n  final org.apache.hadoop.security.UserGroupInformation ugi;\n  final org.apache.hadoop.conf.Configuration config;\n  org.apache.hadoop.fs.viewfs.InodeTree<org.apache.hadoop.fs.AbstractFileSystem> fsState;\n  org.apache.hadoop.fs.Path homeDir;\n  static final boolean $assertionsDisabled;\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, java.lang.String);\n  static org.apache.hadoop.security.AccessControlException readOnlyMountTable(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.viewfs.ViewFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  org.apache.hadoop.fs.viewfs.ViewFs(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.net.URISyntaxException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public int getUriDefaultPort();\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createInternal(org.apache.hadoop.fs.Path, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.permission.FsPermission, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getFsStatus() throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void mkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public void renameInternal(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.fs.UnresolvedLinkException;\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnresolvedLinkException, java.io.IOException;\n  public void setVerifyChecksum(boolean) throws org.apache.hadoop.security.AccessControlException, java.io.IOException;\n  public org.apache.hadoop.fs.viewfs.ViewFs$MountPoint[] getMountPoints();\n  public java.util.List<org.apache.hadoop.security.token.Token<?>> getDelegationTokens(java.lang.String) throws java.io.IOException;\n  public boolean isValidName(java.lang.String);\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/fs/viewfs/InodeTree$ResolveResult.class": "Compiled from \"InodeTree.java\"\nabstract class org.apache.hadoop.fs.viewfs.InodeTree<T> {\n  static final org.apache.hadoop.fs.Path SlashPath;\n  final org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T> root;\n  final java.lang.String homedirPrefix;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> mountPoints;\n  static final boolean $assertionsDisabled;\n  static java.lang.String[] breakIntoPathComponents(java.lang.String);\n  protected abstract T getTargetFileSystem(java.net.URI) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, java.io.IOException;\n  protected abstract T getTargetFileSystem(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir<T>) throws java.net.URISyntaxException;\n  protected abstract T getTargetFileSystem(java.net.URI[]) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException;\n  protected org.apache.hadoop.fs.viewfs.InodeTree(org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.net.URISyntaxException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.IOException;\n  org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult<T> resolve(java.lang.String, boolean) throws java.io.FileNotFoundException;\n  java.util.List<org.apache.hadoop.fs.viewfs.InodeTree$MountPoint<T>> getMountPoints();\n  java.lang.String getHomeDirPrefixValue();\n  static {};\n}\n", 
  "org/apache/hadoop/metrics/spi/CompositeContext.class": "Compiled from \"CompositeContext.java\"\npublic class org.apache.hadoop.metrics.spi.CompositeContext extends org.apache.hadoop.metrics.spi.AbstractMetricsContext {\n  public org.apache.hadoop.metrics.spi.CompositeContext();\n  public void init(java.lang.String, org.apache.hadoop.metrics.ContextFactory);\n  public org.apache.hadoop.metrics.MetricsRecord newRecord(java.lang.String);\n  protected void emitRecord(java.lang.String, java.lang.String, org.apache.hadoop.metrics.spi.OutputRecord) throws java.io.IOException;\n  protected void flush() throws java.io.IOException;\n  public void startMonitoring() throws java.io.IOException;\n  public void stopMonitoring();\n  public boolean isMonitoring();\n  public void close();\n  public void registerUpdater(org.apache.hadoop.metrics.Updater);\n  public void unregisterUpdater(org.apache.hadoop.metrics.Updater);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToActiveRequestProto$Builder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/SaslRpcClient$SaslClientCallbackHandler.class": "Compiled from \"SaslRpcClient.java\"\npublic class org.apache.hadoop.security.SaslRpcClient {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.security.SaslRpcClient(org.apache.hadoop.security.UserGroupInformation, java.lang.Class<?>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration);\n  public java.lang.Object getNegotiatedProperty(java.lang.String);\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod getAuthMethod();\n  java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth) throws java.io.IOException;\n  public org.apache.hadoop.security.SaslRpcServer$AuthMethod saslConnect(java.io.InputStream, java.io.OutputStream) throws java.io.IOException;\n  public java.io.InputStream getInputStream(java.io.InputStream) throws java.io.IOException;\n  public java.io.OutputStream getOutputStream(java.io.OutputStream) throws java.io.IOException;\n  public void dispose() throws javax.security.sasl.SaslException;\n  static javax.security.sasl.SaslClient access$000(org.apache.hadoop.security.SaslRpcClient);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto access$100();\n  static {};\n}\n", 
  "org/apache/hadoop/security/NetgroupCache.class": "Compiled from \"NetgroupCache.java\"\npublic class org.apache.hadoop.security.NetgroupCache {\n  public org.apache.hadoop.security.NetgroupCache();\n  public static void getNetgroups(java.lang.String, java.util.List<java.lang.String>);\n  public static java.util.List<java.lang.String> getNetgroupNames();\n  public static boolean isCached(java.lang.String);\n  public static void clear();\n  public static void add(java.lang.String, java.util.List<java.lang.String>);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$ConnectionState.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/UniqueNames.class": "Compiled from \"UniqueNames.java\"\npublic class org.apache.hadoop.metrics2.lib.UniqueNames {\n  static final com.google.common.base.Joiner joiner;\n  final java.util.Map<java.lang.String, org.apache.hadoop.metrics2.lib.UniqueNames$Count> map;\n  public org.apache.hadoop.metrics2.lib.UniqueNames();\n  public synchronized java.lang.String uniqueName(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetUserMappingsProtocolService$BlockingInterface.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/record/compiler/JInt$JavaInt.class": "Compiled from \"JInt.java\"\npublic class org.apache.hadoop.record.compiler.JInt extends org.apache.hadoop.record.compiler.JType {\n  public org.apache.hadoop.record.compiler.JInt();\n  java.lang.String getSignature();\n}\n", 
  "org/apache/hadoop/metrics2/lib/MutableStat.class": "Compiled from \"MutableStat.java\"\npublic class org.apache.hadoop.metrics2.lib.MutableStat extends org.apache.hadoop.metrics2.lib.MutableMetric {\n  public org.apache.hadoop.metrics2.lib.MutableStat(java.lang.String, java.lang.String, java.lang.String, java.lang.String, boolean);\n  public org.apache.hadoop.metrics2.lib.MutableStat(java.lang.String, java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void setExtended(boolean);\n  public synchronized void add(long, long);\n  public synchronized void add(long);\n  public synchronized void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  public void resetMinMax();\n}\n", 
  "org/apache/hadoop/security/authorize/AccessControlList.class": "Compiled from \"AccessControlList.java\"\npublic class org.apache.hadoop.security.authorize.AccessControlList implements org.apache.hadoop.io.Writable {\n  public static final java.lang.String WILDCARD_ACL_VALUE;\n  public org.apache.hadoop.security.authorize.AccessControlList();\n  public org.apache.hadoop.security.authorize.AccessControlList(java.lang.String);\n  public org.apache.hadoop.security.authorize.AccessControlList(java.lang.String, java.lang.String);\n  public boolean isAllAllowed();\n  public void addUser(java.lang.String);\n  public void addGroup(java.lang.String);\n  public void removeUser(java.lang.String);\n  public void removeGroup(java.lang.String);\n  public java.util.Collection<java.lang.String> getUsers();\n  public java.util.Collection<java.lang.String> getGroups();\n  public final boolean isUserInList(org.apache.hadoop.security.UserGroupInformation);\n  public boolean isUserAllowed(org.apache.hadoop.security.UserGroupInformation);\n  public java.lang.String toString();\n  public java.lang.String getAclString();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/CredentialShell$ListCommand.class": "Compiled from \"CredentialShell.java\"\npublic class org.apache.hadoop.security.alias.CredentialShell extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public java.io.PrintStream out;\n  public java.io.PrintStream err;\n  public org.apache.hadoop.security.alias.CredentialShell();\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  protected int init(java.lang.String[]) throws java.io.IOException;\n  protected char[] promptForCredential() throws java.io.IOException;\n  public org.apache.hadoop.security.alias.CredentialShell$PasswordReader getPasswordReader();\n  public void setPasswordReader(org.apache.hadoop.security.alias.CredentialShell$PasswordReader);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  static boolean access$100(org.apache.hadoop.security.alias.CredentialShell);\n  static boolean access$300(org.apache.hadoop.security.alias.CredentialShell);\n  static java.lang.String access$400(org.apache.hadoop.security.alias.CredentialShell);\n}\n", 
  "org/apache/hadoop/ha/ActiveStandbyElector$3.class": "Compiled from \"ActiveStandbyElector.java\"\npublic class org.apache.hadoop.ha.ActiveStandbyElector implements org.apache.zookeeper.AsyncCallback$StatCallback,org.apache.zookeeper.AsyncCallback$StringCallback {\n  protected static final java.lang.String LOCK_FILENAME;\n  protected static final java.lang.String BREADCRUMB_FILENAME;\n  public static final org.apache.commons.logging.Log LOG;\n  static final boolean $assertionsDisabled;\n  public org.apache.hadoop.ha.ActiveStandbyElector(java.lang.String, int, java.lang.String, java.util.List<org.apache.zookeeper.data.ACL>, java.util.List<org.apache.hadoop.util.ZKUtil$ZKAuthInfo>, org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback, int) throws java.io.IOException, org.apache.hadoop.HadoopIllegalArgumentException, org.apache.zookeeper.KeeperException;\n  public synchronized void joinElection(byte[]) throws org.apache.hadoop.HadoopIllegalArgumentException;\n  public synchronized boolean parentZNodeExists() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void ensureParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void clearParentZNode() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void quitElection(boolean);\n  public synchronized byte[] getActiveData() throws org.apache.hadoop.ha.ActiveStandbyElector$ActiveNotFoundException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, java.io.IOException;\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n  public synchronized void processResult(int, java.lang.String, java.lang.Object, org.apache.zookeeper.data.Stat);\n  synchronized void processWatchEvent(org.apache.zookeeper.ZooKeeper, org.apache.zookeeper.WatchedEvent);\n  protected synchronized org.apache.zookeeper.ZooKeeper getNewZooKeeper() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  protected void sleepFor(int);\n  void preventSessionReestablishmentForTests();\n  void allowSessionReestablishmentForTests();\n  synchronized long getZKSessionIdForTests();\n  synchronized org.apache.hadoop.ha.ActiveStandbyElector$State getStateForTests();\n  public synchronized void terminateConnection();\n  public java.lang.String toString();\n  static org.apache.zookeeper.ZooKeeper access$000(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$100(org.apache.hadoop.ha.ActiveStandbyElector);\n  static java.lang.String access$500(org.apache.hadoop.ha.ActiveStandbyElector);\n  static int access$600(org.apache.hadoop.ha.ActiveStandbyElector);\n  static void access$700(org.apache.hadoop.ha.ActiveStandbyElector, java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcSaslProto$Builder.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$TokenProto.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ha/ShellCommandFencer.class": "Compiled from \"ShellCommandFencer.java\"\npublic class org.apache.hadoop.ha.ShellCommandFencer extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.ha.FenceMethod {\n  static org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ha.ShellCommandFencer();\n  public void checkArgs(java.lang.String) throws org.apache.hadoop.ha.BadFencingConfigurationException;\n  public boolean tryFence(org.apache.hadoop.ha.HAServiceTarget, java.lang.String);\n  static java.lang.String abbreviate(java.lang.String, int);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos$RequestHeaderProto$Builder.class": "Compiled from \"ProtobufRpcEngineProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1102(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.class": "Compiled from \"MetricsSourceBuilder.java\"\npublic class org.apache.hadoop.metrics2.lib.MetricsSourceBuilder {\n  org.apache.hadoop.metrics2.lib.MetricsSourceBuilder(java.lang.Object, org.apache.hadoop.metrics2.lib.MutableMetricsFactory);\n  public org.apache.hadoop.metrics2.MetricsSource build();\n  public org.apache.hadoop.metrics2.MetricsInfo info();\n  static org.apache.hadoop.metrics2.lib.MetricsRegistry access$000(org.apache.hadoop.metrics2.lib.MetricsSourceBuilder);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.class": "Compiled from \"ProtobufRpcEngineProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1102(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/util/ComparableVersion$ListItem.class": "Compiled from \"ComparableVersion.java\"\npublic class org.apache.hadoop.util.ComparableVersion implements java.lang.Comparable<org.apache.hadoop.util.ComparableVersion> {\n  public org.apache.hadoop.util.ComparableVersion(java.lang.String);\n  public final void parseVersion(java.lang.String);\n  public int compareTo(org.apache.hadoop.util.ComparableVersion);\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n}\n", 
  "org/apache/hadoop/io/retry/RetryPolicy.class": "Compiled from \"RetryPolicy.java\"\npublic interface org.apache.hadoop.io.retry.RetryPolicy {\n  public abstract org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception, int, int, boolean) throws java.lang.Exception;\n}\n", 
  "org/apache/hadoop/ha/proto/HAServiceProtocolProtos$TransitionToStandbyResponseProtoOrBuilder.class": "Compiled from \"HAServiceProtocolProtos.java\"\npublic final class org.apache.hadoop.ha.proto.HAServiceProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$Descriptor access$1600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1700();\n  static com.google.protobuf.Descriptors$Descriptor access$2300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2400();\n  static com.google.protobuf.Descriptors$Descriptor access$3200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3300();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5600();\n  static com.google.protobuf.Descriptors$Descriptor access$6200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6300();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7502(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/SecurityProtos$RenewDelegationTokenRequestProto$1.class": "Compiled from \"SecurityProtos.java\"\npublic final class org.apache.hadoop.security.proto.SecurityProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1200();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1300();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$Descriptor access$3000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3100();\n  static com.google.protobuf.Descriptors$Descriptor access$3900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4000();\n  static com.google.protobuf.Descriptors$Descriptor access$4800();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4900();\n  static com.google.protobuf.Descriptors$Descriptor access$5700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1202(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1302(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4802(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4902(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$StreamOption.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/SequenceFile$Writer$Option.class": "Compiled from \"SequenceFile.java\"\npublic class org.apache.hadoop.io.SequenceFile {\n  public static final int SYNC_INTERVAL;\n  public static org.apache.hadoop.io.SequenceFile$CompressionType getDefaultCompressionType(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultCompressionType(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$CompressionType);\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.io.SequenceFile$Writer$Option...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, int, short, long, boolean, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileContext, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, org.apache.hadoop.fs.Options$CreateOpts...) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec, org.apache.hadoop.io.SequenceFile$Metadata) throws java.io.IOException;\n  public static org.apache.hadoop.io.SequenceFile$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataOutputStream, java.lang.Class, java.lang.Class, org.apache.hadoop.io.SequenceFile$CompressionType, org.apache.hadoop.io.compress.CompressionCodec) throws java.io.IOException;\n  static int access$100(org.apache.hadoop.conf.Configuration);\n  static byte[] access$200();\n  static org.apache.commons.logging.Log access$800();\n  static {};\n}\n", 
  "org/apache/hadoop/io/file/tfile/CompareUtils.class": "Compiled from \"CompareUtils.java\"\nclass org.apache.hadoop.io.file.tfile.CompareUtils {\n}\n", 
  "org/apache/hadoop/ipc/ProtocolSignature.class": "Compiled from \"ProtocolSignature.java\"\npublic class org.apache.hadoop.ipc.ProtocolSignature implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.ipc.ProtocolSignature();\n  public org.apache.hadoop.ipc.ProtocolSignature(long, int[]);\n  public long getVersion();\n  public int[] getMethods();\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  static int getFingerprint(java.lang.reflect.Method);\n  static int getFingerprint(java.lang.reflect.Method[]);\n  static int getFingerprint(int[]);\n  public static void resetCache();\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(int, long, java.lang.Class<? extends org.apache.hadoop.ipc.VersionedProtocol>);\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(java.lang.String, long) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.ProtocolSignature getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol, java.lang.String, long, int) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/security/alias/JavaKeyStoreProvider$1.class": "Compiled from \"JavaKeyStoreProvider.java\"\npublic class org.apache.hadoop.security.alias.JavaKeyStoreProvider extends org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider {\n  public static final java.lang.String SCHEME_NAME;\n  protected java.lang.String getSchemeName();\n  protected java.io.OutputStream getOutputStreamForKeystore() throws java.io.IOException;\n  protected boolean keystoreExists() throws java.io.IOException;\n  protected java.io.InputStream getInputStreamForFile() throws java.io.IOException;\n  protected void createPermissions(java.lang.String);\n  protected void stashOriginalFilePermissions() throws java.io.IOException;\n  protected void initFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  org.apache.hadoop.security.alias.JavaKeyStoreProvider(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.alias.JavaKeyStoreProvider$1) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.class": "Compiled from \"Bzip2Decompressor.java\"\npublic class org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor implements org.apache.hadoop.io.compress.Decompressor {\n  public org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor(boolean, int);\n  public org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor();\n  public synchronized void setInput(byte[], int, int);\n  synchronized void setInputFromSavedData();\n  public synchronized void setDictionary(byte[], int, int);\n  public synchronized boolean needsInput();\n  public synchronized boolean needsDictionary();\n  public synchronized boolean finished();\n  public synchronized int decompress(byte[], int, int) throws java.io.IOException;\n  public synchronized long getBytesWritten();\n  public synchronized long getBytesRead();\n  public synchronized int getRemaining();\n  public synchronized void reset();\n  public synchronized void end();\n  static void initSymbols(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.class": "Compiled from \"AbstractMetricsRecord.java\"\nabstract class org.apache.hadoop.metrics2.impl.AbstractMetricsRecord implements org.apache.hadoop.metrics2.MetricsRecord {\n  org.apache.hadoop.metrics2.impl.AbstractMetricsRecord();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n", 
  "org/apache/hadoop/security/KerberosInfo.class": "Compiled from \"KerberosInfo.java\"\npublic interface org.apache.hadoop.security.KerberosInfo extends java.lang.annotation.Annotation {\n  public abstract java.lang.String serverPrincipal();\n  public abstract java.lang.String clientPrincipal();\n}\n", 
  "org/apache/hadoop/ipc/Client$Call.class": "Compiled from \"Client.java\"\npublic class org.apache.hadoop.ipc.Client {\n  public static final org.apache.commons.logging.Log LOG;\n  static final int CONNECTION_CONTEXT_CALL_ID;\n  public static void setCallIdAndRetryCount(int, int);\n  public static final void setPingInterval(org.apache.hadoop.conf.Configuration, int);\n  public static final int getPingInterval(org.apache.hadoop.conf.Configuration);\n  public static final int getTimeout(org.apache.hadoop.conf.Configuration);\n  public static final void setConnectTimeout(org.apache.hadoop.conf.Configuration, int);\n  synchronized void incCount();\n  synchronized void decCount();\n  synchronized boolean isZeroReference();\n  void checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto) throws java.io.IOException;\n  org.apache.hadoop.ipc.Client$Call createCall(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory);\n  public org.apache.hadoop.ipc.Client(java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.conf.Configuration);\n  javax.net.SocketFactory getSocketFactory();\n  public void stop();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, java.net.InetSocketAddress, java.lang.Class<?>, org.apache.hadoop.security.UserGroupInformation, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int) throws java.io.IOException;\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.io.Writable, org.apache.hadoop.ipc.Client$ConnectionId, int, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  java.util.Set<org.apache.hadoop.ipc.Client$ConnectionId> getConnectionIds();\n  public static int nextCallId();\n  static java.lang.ThreadLocal access$200();\n  static java.lang.ThreadLocal access$300();\n  static byte[] access$600(org.apache.hadoop.ipc.Client);\n  static javax.net.SocketFactory access$700(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.atomic.AtomicBoolean access$900(org.apache.hadoop.ipc.Client);\n  static int access$1300(org.apache.hadoop.ipc.Client);\n  static boolean access$2000(org.apache.hadoop.ipc.Client);\n  static java.util.Hashtable access$2100(org.apache.hadoop.ipc.Client);\n  static java.util.concurrent.ExecutorService access$2400(org.apache.hadoop.ipc.Client);\n  static java.lang.Class access$2500(org.apache.hadoop.ipc.Client);\n  static org.apache.hadoop.conf.Configuration access$2600(org.apache.hadoop.ipc.Client);\n  static {};\n}\n", 
  "org/apache/hadoop/io/serializer/Deserializer.class": "Compiled from \"Deserializer.java\"\npublic interface org.apache.hadoop.io.serializer.Deserializer<T> {\n  public abstract void open(java.io.InputStream) throws java.io.IOException;\n  public abstract T deserialize(T) throws java.io.IOException;\n  public abstract void close() throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/service/LifecycleEvent.class": "Compiled from \"LifecycleEvent.java\"\npublic class org.apache.hadoop.service.LifecycleEvent implements java.io.Serializable {\n  public long time;\n  public org.apache.hadoop.service.Service$STATE state;\n  public org.apache.hadoop.service.LifecycleEvent();\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminProtocolTranslatorPB.class": "Compiled from \"TraceAdminProtocolTranslatorPB.java\"\npublic class org.apache.hadoop.tracing.TraceAdminProtocolTranslatorPB implements org.apache.hadoop.tracing.TraceAdminProtocol,org.apache.hadoop.ipc.ProtocolTranslator,java.io.Closeable {\n  public org.apache.hadoop.tracing.TraceAdminProtocolTranslatorPB(org.apache.hadoop.tracing.TraceAdminProtocolPB);\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.tracing.SpanReceiverInfo[] listSpanReceivers() throws java.io.IOException;\n  public long addSpanReceiver(org.apache.hadoop.tracing.SpanReceiverInfo) throws java.io.IOException;\n  public void removeSpanReceiver(long) throws java.io.IOException;\n  public java.lang.Object getUnderlyingProxyObject();\n}\n", 
  "org/apache/hadoop/tracing/TraceAdminPB$SpanReceiverListInfoOrBuilder.class": "Compiled from \"TraceAdminPB.java\"\npublic final class org.apache.hadoop.tracing.TraceAdminPB {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1800();\n  static com.google.protobuf.Descriptors$Descriptor access$2600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2700();\n  static com.google.protobuf.Descriptors$Descriptor access$3600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3700();\n  static com.google.protobuf.Descriptors$Descriptor access$4700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4800();\n  static com.google.protobuf.Descriptors$Descriptor access$5600();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5700();\n  static com.google.protobuf.Descriptors$Descriptor access$6500();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6600();\n  static com.google.protobuf.Descriptors$FileDescriptor access$7402(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$3602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$3702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$5602(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$5702(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$6502(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$6602(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/Server$Handler.class": "Compiled from \"Server.java\"\npublic abstract class org.apache.hadoop.ipc.Server {\n  static final java.lang.String RECEIVED_HTTP_REQ_RESPONSE;\n  static int INITIAL_RESP_BUF_SIZE;\n  static java.util.Map<org.apache.hadoop.ipc.RPC$RpcKind, org.apache.hadoop.ipc.Server$RpcKindMapValue> rpcKindMap;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final org.apache.commons.logging.Log AUDITLOG;\n  protected final org.apache.hadoop.ipc.metrics.RpcMetrics rpcMetrics;\n  protected final org.apache.hadoop.ipc.metrics.RpcDetailedMetrics rpcDetailedMetrics;\n  public void addTerseExceptions(java.lang.Class<?>...);\n  public static void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.Class<? extends org.apache.hadoop.io.Writable>, org.apache.hadoop.ipc.RPC$RpcInvoker);\n  public java.lang.Class<? extends org.apache.hadoop.io.Writable> getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto);\n  public static org.apache.hadoop.ipc.RPC$RpcInvoker getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind);\n  static java.lang.Class<?> getProtocolClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.ipc.Server get();\n  public static java.lang.ThreadLocal<org.apache.hadoop.ipc.Server$Call> getCurCall();\n  public static int getCallId();\n  public static int getCallRetryCount();\n  public static java.net.InetAddress getRemoteIp();\n  public static byte[] getClientId();\n  public static java.lang.String getRemoteAddress();\n  public static org.apache.hadoop.security.UserGroupInformation getRemoteUser();\n  public static boolean isRpcInvocation();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.ipc.metrics.RpcMetrics getRpcMetrics();\n  public org.apache.hadoop.ipc.metrics.RpcDetailedMetrics getRpcDetailedMetrics();\n  java.lang.Iterable<? extends java.lang.Thread> getHandlers();\n  org.apache.hadoop.ipc.Server$Connection[] getConnections();\n  public void refreshServiceAcl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public void refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.authorize.ServiceAuthorizationManager getServiceAuthorizationManager();\n  static java.lang.Class<? extends java.util.concurrent.BlockingQueue<org.apache.hadoop.ipc.Server$Call>> getQueueClass(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public synchronized void refreshCallQueue(org.apache.hadoop.conf.Configuration);\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  protected org.apache.hadoop.ipc.Server(java.lang.String, int, java.lang.Class<? extends org.apache.hadoop.io.Writable>, int, int, int, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  org.apache.hadoop.conf.Configuration getConf();\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public org.apache.hadoop.io.Writable call(org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public abstract org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) throws java.lang.Exception;\n  public int getPort();\n  public int getNumOpenConnections();\n  public int getCallQueueLen();\n  public int getMaxQueueSize();\n  public int getNumReaders();\n  static org.apache.hadoop.conf.Configuration access$400(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$500(org.apache.hadoop.ipc.Server);\n  static int access$600(org.apache.hadoop.ipc.Server);\n  static java.lang.String access$700(org.apache.hadoop.ipc.Server);\n  static int access$602(org.apache.hadoop.ipc.Server, int);\n  static int access$800(org.apache.hadoop.ipc.Server);\n  static int access$900(org.apache.hadoop.ipc.Server);\n  static boolean access$1000(org.apache.hadoop.ipc.Server);\n  static java.lang.ThreadLocal access$1200();\n  static org.apache.hadoop.ipc.Server$ConnectionManager access$1300(org.apache.hadoop.ipc.Server);\n  static void access$1400(org.apache.hadoop.ipc.Server, org.apache.hadoop.ipc.Server$Connection);\n  static boolean access$1500(org.apache.hadoop.ipc.Server);\n  static int access$1900(org.apache.hadoop.ipc.Server, java.nio.channels.WritableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static int access$2100(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.security.token.SecretManager access$2200(org.apache.hadoop.ipc.Server);\n  static org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto access$2300(org.apache.hadoop.ipc.Server);\n  static java.util.List access$2400(org.apache.hadoop.ipc.Server);\n  static void access$2500(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto, org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Server$Responder access$2600(org.apache.hadoop.ipc.Server);\n  static int access$2700(org.apache.hadoop.ipc.Server);\n  static int access$2800(org.apache.hadoop.ipc.Server, java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer) throws java.io.IOException;\n  static java.nio.ByteBuffer access$2900();\n  static org.apache.hadoop.security.SaslPropertiesResolver access$3000(org.apache.hadoop.ipc.Server);\n  static void access$3100(org.apache.hadoop.ipc.Server, java.io.ByteArrayOutputStream, org.apache.hadoop.ipc.Server$Call, org.apache.hadoop.io.Writable, java.lang.String, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.CallQueueManager access$3200(org.apache.hadoop.ipc.Server);\n  static void access$3300(org.apache.hadoop.ipc.Server, org.apache.hadoop.security.UserGroupInformation, java.lang.String, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  static java.lang.ThreadLocal access$3500();\n  static org.apache.hadoop.ipc.Server$ExceptionsHandler access$3800(org.apache.hadoop.ipc.Server);\n  static int access$3900(org.apache.hadoop.ipc.Server);\n  static int access$4100(org.apache.hadoop.ipc.Server);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/ZeroCopyUnavailableException.class": "Compiled from \"ZeroCopyUnavailableException.java\"\npublic class org.apache.hadoop.fs.ZeroCopyUnavailableException extends java.io.IOException {\n  public org.apache.hadoop.fs.ZeroCopyUnavailableException(java.lang.String);\n  public org.apache.hadoop.fs.ZeroCopyUnavailableException(java.lang.String, java.lang.Exception);\n  public org.apache.hadoop.fs.ZeroCopyUnavailableException(java.lang.Exception);\n}\n", 
  "org/apache/hadoop/fs/FileContext$17.class": "", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto$Builder.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/GlobExpander.class": "Compiled from \"GlobExpander.java\"\nclass org.apache.hadoop.fs.GlobExpander {\n  org.apache.hadoop.fs.GlobExpander();\n  public static java.util.List<java.lang.String> expand(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/io/ArrayPrimitiveWritable.class": "Compiled from \"ArrayPrimitiveWritable.java\"\npublic class org.apache.hadoop.io.ArrayPrimitiveWritable implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.io.ArrayPrimitiveWritable();\n  public org.apache.hadoop.io.ArrayPrimitiveWritable(java.lang.Class<?>);\n  public org.apache.hadoop.io.ArrayPrimitiveWritable(java.lang.Object);\n  public java.lang.Object get();\n  public java.lang.Class<?> getComponentType();\n  public java.lang.Class<?> getDeclaredComponentType();\n  public boolean isDeclaredComponentType(java.lang.Class<?>);\n  public void set(java.lang.Object);\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/ProtobufRpcEngine$RpcMessageWithHeader.class": "Compiled from \"ProtobufRpcEngine.java\"\npublic class org.apache.hadoop.ipc.ProtobufRpcEngine implements org.apache.hadoop.ipc.RpcEngine {\n  public static final org.apache.commons.logging.Log LOG;\n  public org.apache.hadoop.ipc.ProtobufRpcEngine();\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy) throws java.io.IOException;\n  public <T extends java/lang/Object> org.apache.hadoop.ipc.ProtocolProxy<T> getProxy(java.lang.Class<T>, long, java.net.InetSocketAddress, org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory, int, org.apache.hadoop.io.retry.RetryPolicy, java.util.concurrent.atomic.AtomicBoolean) throws java.io.IOException;\n  public org.apache.hadoop.ipc.ProtocolProxy<org.apache.hadoop.ipc.ProtocolMetaInfoPB> getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId, org.apache.hadoop.conf.Configuration, javax.net.SocketFactory) throws java.io.IOException;\n  static org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.ipc.RPC$Server getServer(java.lang.Class<?>, java.lang.Object, java.lang.String, int, int, int, int, boolean, org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>, java.lang.String) throws java.io.IOException;\n  static org.apache.hadoop.ipc.ClientCache access$200();\n  static {};\n}\n", 
  "org/apache/hadoop/fs/GlobExpander$StringWithOffset.class": "Compiled from \"GlobExpander.java\"\nclass org.apache.hadoop.fs.GlobExpander {\n  org.apache.hadoop.fs.GlobExpander();\n  public static java.util.List<java.lang.String> expand(java.lang.String) throws java.io.IOException;\n}\n", 
  "org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos$RefreshCallQueueProtocolService.class": "Compiled from \"RefreshCallQueueProtocolProtos.java\"\npublic final class org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProtoOrBuilder.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/conf/Configuration$DeprecationContext.class": "Compiled from \"Configuration.java\"\npublic class org.apache.hadoop.conf.Configuration implements java.lang.Iterable<java.util.Map$Entry<java.lang.String, java.lang.String>>, org.apache.hadoop.io.Writable {\n  static final java.lang.String UNKNOWN_RESOURCE;\n  static final boolean $assertionsDisabled;\n  public static void addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]);\n  public static void addDeprecation(java.lang.String, java.lang.String[], java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String, java.lang.String);\n  public static void addDeprecation(java.lang.String, java.lang.String[]);\n  public static void addDeprecation(java.lang.String, java.lang.String);\n  public static boolean isDeprecated(java.lang.String);\n  public void setDeprecatedProperties();\n  public org.apache.hadoop.conf.Configuration();\n  public org.apache.hadoop.conf.Configuration(boolean);\n  public org.apache.hadoop.conf.Configuration(org.apache.hadoop.conf.Configuration);\n  public static synchronized void addDefaultResource(java.lang.String);\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public boolean onlyKeyExists(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java/lang/Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java/lang/Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromCredentialProviders(java.lang.String) throws java.io.IOException;\n  protected char[] getPasswordFromConfig(java.lang.String);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U extends java/lang/Object> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U extends java/lang/Object> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  protected synchronized java.util.Properties getProps();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public static void dumpConfiguration(org.apache.hadoop.conf.Configuration, java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  synchronized boolean getQuietMode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n  public static void dumpDeprecatedKeys();\n  public static boolean hasWarnedDeprecation(java.lang.String);\n  static {};\n}\n", 
  "org/apache/hadoop/io/nativeio/package-info.class": "Compiled from \"package-info.java\"\ninterface org.apache.hadoop.io.nativeio.package-info {\n}\n", 
  "org/apache/hadoop/fs/Path.class": "Compiled from \"Path.java\"\npublic class org.apache.hadoop.fs.Path implements java.lang.Comparable {\n  public static final java.lang.String SEPARATOR;\n  public static final char SEPARATOR_CHAR;\n  public static final java.lang.String CUR_DIR;\n  public static final boolean WINDOWS;\n  void checkNotSchemeWithRelative();\n  void checkNotRelative();\n  public static org.apache.hadoop.fs.Path getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path(java.lang.String, java.lang.String);\n  public org.apache.hadoop.fs.Path(org.apache.hadoop.fs.Path, java.lang.String);\n  public org.apache.hadoop.fs.Path(java.lang.String, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.Path(java.lang.String) throws java.lang.IllegalArgumentException;\n  public org.apache.hadoop.fs.Path(java.net.URI);\n  public org.apache.hadoop.fs.Path(java.lang.String, java.lang.String, java.lang.String);\n  public static org.apache.hadoop.fs.Path mergePaths(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public static boolean isWindowsAbsolutePath(java.lang.String, boolean);\n  public java.net.URI toUri();\n  public org.apache.hadoop.fs.FileSystem getFileSystem(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public boolean isAbsoluteAndSchemeAuthorityNull();\n  public boolean isUriPathAbsolute();\n  public boolean isAbsolute();\n  public boolean isRoot();\n  public java.lang.String getName();\n  public org.apache.hadoop.fs.Path getParent();\n  public org.apache.hadoop.fs.Path suffix(java.lang.String);\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n  public int depth();\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.fs.Path makeQualified(java.net.URI, org.apache.hadoop.fs.Path);\n  static {};\n}\n", 
  "org/apache/hadoop/io/MD5Hash.class": "Compiled from \"MD5Hash.java\"\npublic class org.apache.hadoop.io.MD5Hash implements org.apache.hadoop.io.WritableComparable<org.apache.hadoop.io.MD5Hash> {\n  public static final int MD5_LEN;\n  public org.apache.hadoop.io.MD5Hash();\n  public org.apache.hadoop.io.MD5Hash(java.lang.String);\n  public org.apache.hadoop.io.MD5Hash(byte[]);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public static org.apache.hadoop.io.MD5Hash read(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void set(org.apache.hadoop.io.MD5Hash);\n  public byte[] getDigest();\n  public static org.apache.hadoop.io.MD5Hash digest(byte[]);\n  public static java.security.MessageDigest getDigester();\n  public static org.apache.hadoop.io.MD5Hash digest(java.io.InputStream) throws java.io.IOException;\n  public static org.apache.hadoop.io.MD5Hash digest(byte[], int, int);\n  public static org.apache.hadoop.io.MD5Hash digest(java.lang.String);\n  public static org.apache.hadoop.io.MD5Hash digest(org.apache.hadoop.io.UTF8);\n  public long halfDigest();\n  public int quarterDigest();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.io.MD5Hash);\n  public java.lang.String toString();\n  public void setDigest(java.lang.String);\n  public int compareTo(java.lang.Object);\n  static {};\n}\n", 
  "org/apache/hadoop/ipc/protobuf/RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto$1.class": "Compiled from \"RpcHeaderProtos.java\"\npublic final class org.apache.hadoop.ipc.protobuf.RpcHeaderProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$1000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1100();\n  static com.google.protobuf.Descriptors$Descriptor access$2400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2500();\n  static com.google.protobuf.Descriptors$Descriptor access$4000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4100();\n  static com.google.protobuf.Descriptors$Descriptor access$4300();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4400();\n  static com.google.protobuf.Descriptors$FileDescriptor access$6602(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$4302(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$4402(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/security/proto/RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto$1.class": "Compiled from \"RefreshUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$700();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$800();\n  static com.google.protobuf.Descriptors$Descriptor access$1400();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1500();\n  static com.google.protobuf.Descriptors$Descriptor access$2100();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2200();\n  static com.google.protobuf.Descriptors$FileDescriptor access$3002(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$702(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$802(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$1402(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1502(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$2102(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$2202(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileSystem$Statistics$4.class": "Compiled from \"FileSystem.java\"\npublic abstract class org.apache.hadoop.fs.FileSystem extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final java.lang.String FS_DEFAULT_NAME_KEY;\n  public static final java.lang.String DEFAULT_FS;\n  public static final org.apache.commons.logging.Log LOG;\n  public static final int SHUTDOWN_HOOK_PRIORITY;\n  static final org.apache.hadoop.fs.FileSystem$Cache CACHE;\n  protected org.apache.hadoop.fs.FileSystem$Statistics statistics;\n  boolean resolveSymlinks;\n  static void addFileSystemForTesting(java.net.URI, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.net.URI);\n  public static void setDefaultUri(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public void initialize(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getScheme();\n  public abstract java.net.URI getUri();\n  protected java.net.URI getCanonicalUri();\n  protected java.net.URI canonicalizeUri(java.net.URI);\n  protected int getDefaultPort();\n  protected static org.apache.hadoop.fs.FileSystem getFSofPath(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public java.lang.String getCanonicalServiceName();\n  public java.lang.String getName();\n  public static org.apache.hadoop.fs.FileSystem getNamed(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem newInstance(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.LocalFileSystem newInstanceLocal(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void closeAll() throws java.io.IOException;\n  public static void closeAllForUGI(org.apache.hadoop.security.UserGroupInformation) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path makeQualified(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.security.token.Token<?> getDelegationToken(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.security.token.Token<?>[] addDelegationTokens(java.lang.String, org.apache.hadoop.security.Credentials) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem[] getChildFileSystems();\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public static boolean mkdirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FileSystem();\n  protected void checkPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.BlockLocation[] getFileBlockLocations(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsServerDefaults getServerDefaults(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path resolvePath(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, short, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected org.apache.hadoop.fs.FSDataOutputStream primitiveCreate(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable, org.apache.hadoop.fs.Options$ChecksumOpt) throws java.io.IOException;\n  protected boolean primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  protected void primitiveMkdir(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public boolean createNewFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path, int, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n  public void concat(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path[]) throws java.io.IOException;\n  public short getReplication(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean setReplication(org.apache.hadoop.fs.Path, short) throws java.io.IOException;\n  public abstract boolean rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected void rename(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;\n  public boolean truncate(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public boolean delete(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean delete(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public boolean deleteOnExit(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean cancelDeleteOnExit(org.apache.hadoop.fs.Path);\n  protected void processDeleteOnExit();\n  public boolean exists(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isDirectory(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean isFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getLength(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.ContentSummary getContentSummary(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.Path> listCorruptFileBlocks(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[]) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] globStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  protected org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listLocatedStatus(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.FileStatus> listStatusIterator(org.apache.hadoop.fs.Path) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.RemoteIterator<org.apache.hadoop.fs.LocatedFileStatus> listFiles(org.apache.hadoop.fs.Path, boolean) throws java.io.FileNotFoundException, java.io.IOException;\n  public org.apache.hadoop.fs.Path getHomeDirectory();\n  public abstract void setWorkingDirectory(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.Path getWorkingDirectory();\n  protected org.apache.hadoop.fs.Path getInitialWorkingDirectory();\n  public boolean mkdirs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public abstract boolean mkdirs(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void copyFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveFromLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyFromLocalFile(boolean, boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void moveToLocalFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void copyToLocalFile(boolean, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path startLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void completeLocalOutput(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getUsed() throws java.io.IOException;\n  public long getBlockSize(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long getDefaultBlockSize();\n  public long getDefaultBlockSize(org.apache.hadoop.fs.Path);\n  public short getDefaultReplication();\n  public short getDefaultReplication(org.apache.hadoop.fs.Path);\n  public abstract org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void access(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsAction) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, java.io.IOException;\n  static void checkAccessPermissions(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path fixRelativePart(org.apache.hadoop.fs.Path);\n  public void createSymlink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.security.AccessControlException, org.apache.hadoop.fs.FileAlreadyExistsException, java.io.FileNotFoundException, org.apache.hadoop.fs.ParentNotDirectoryException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileLinkStatus(org.apache.hadoop.fs.Path) throws org.apache.hadoop.security.AccessControlException, java.io.FileNotFoundException, org.apache.hadoop.fs.UnsupportedFileSystemException, java.io.IOException;\n  public boolean supportsSymlinks();\n  public org.apache.hadoop.fs.Path getLinkTarget(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  protected org.apache.hadoop.fs.Path resolveLink(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileChecksum getFileChecksum(org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public void setVerifyChecksum(boolean);\n  public void setWriteChecksum(boolean);\n  public org.apache.hadoop.fs.FsStatus getStatus() throws java.io.IOException;\n  public org.apache.hadoop.fs.FsStatus getStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setPermission(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;\n  public void setOwner(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void setTimes(org.apache.hadoop.fs.Path, long, long) throws java.io.IOException;\n  public final org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void renameSnapshot(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public void modifyAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeAclEntries(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public void removeDefaultAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeAcl(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setAcl(org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n  public org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[]) throws java.io.IOException;\n  public void setXAttr(org.apache.hadoop.fs.Path, java.lang.String, byte[], java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;\n  public byte[] getXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.Map<java.lang.String, byte[]> getXAttrs(org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.util.List<java.lang.String> listXAttrs(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeXAttr(org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.fs.FileSystem> getFileSystemClass(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static synchronized java.util.Map<java.lang.String, org.apache.hadoop.fs.FileSystem$Statistics> getStatistics();\n  public static synchronized java.util.List<org.apache.hadoop.fs.FileSystem$Statistics> getAllStatistics();\n  public static synchronized org.apache.hadoop.fs.FileSystem$Statistics getStatistics(java.lang.String, java.lang.Class<? extends org.apache.hadoop.fs.FileSystem>);\n  public static synchronized void clearStatistics();\n  public static synchronized void printStatistics() throws java.io.IOException;\n  public static boolean areSymlinksEnabled();\n  public static void enableSymlinks();\n  static org.apache.hadoop.fs.FileSystem access$200(java.net.URI, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  static org.apache.hadoop.fs.FileSystem$Cache$Key access$302(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem$Cache$Key);\n  static {};\n}\n", 
  "org/apache/hadoop/fs/FileContext$22.class": "", 
  "org/apache/hadoop/io/file/tfile/Compression$Algorithm$1.class": "Compiled from \"Compression.java\"\nfinal class org.apache.hadoop.io.file.tfile.Compression {\n  static final org.apache.commons.logging.Log LOG;\n  static org.apache.hadoop.io.file.tfile.Compression$Algorithm getCompressionAlgorithmByName(java.lang.String);\n  static java.lang.String[] getSupportedAlgorithms();\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos$GetUserMappingsProtocolService$2.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/tools/proto/GetUserMappingsProtocolProtos.class": "Compiled from \"GetUserMappingsProtocolProtos.java\"\npublic final class org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos {\n  public static void registerAllExtensions(com.google.protobuf.ExtensionRegistry);\n  public static com.google.protobuf.Descriptors$FileDescriptor getDescriptor();\n  static com.google.protobuf.Descriptors$Descriptor access$000();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$100();\n  static com.google.protobuf.Descriptors$Descriptor access$900();\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1000();\n  static com.google.protobuf.Descriptors$FileDescriptor access$1902(com.google.protobuf.Descriptors$FileDescriptor);\n  static com.google.protobuf.Descriptors$Descriptor access$002(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$102(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static com.google.protobuf.Descriptors$Descriptor access$902(com.google.protobuf.Descriptors$Descriptor);\n  static com.google.protobuf.GeneratedMessage$FieldAccessorTable access$1002(com.google.protobuf.GeneratedMessage$FieldAccessorTable);\n  static {};\n}\n", 
  "org/apache/hadoop/metrics2/lib/MetricsRegistry.class": "Compiled from \"MetricsRegistry.java\"\npublic class org.apache.hadoop.metrics2.lib.MetricsRegistry {\n  public org.apache.hadoop.metrics2.lib.MetricsRegistry(java.lang.String);\n  public org.apache.hadoop.metrics2.lib.MetricsRegistry(org.apache.hadoop.metrics2.MetricsInfo);\n  public org.apache.hadoop.metrics2.MetricsInfo info();\n  public synchronized org.apache.hadoop.metrics2.lib.MutableMetric get(java.lang.String);\n  public synchronized org.apache.hadoop.metrics2.MetricsTag getTag(java.lang.String);\n  public org.apache.hadoop.metrics2.lib.MutableCounterInt newCounter(java.lang.String, java.lang.String, int);\n  public synchronized org.apache.hadoop.metrics2.lib.MutableCounterInt newCounter(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public org.apache.hadoop.metrics2.lib.MutableCounterLong newCounter(java.lang.String, java.lang.String, long);\n  public synchronized org.apache.hadoop.metrics2.lib.MutableCounterLong newCounter(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public org.apache.hadoop.metrics2.lib.MutableGaugeInt newGauge(java.lang.String, java.lang.String, int);\n  public synchronized org.apache.hadoop.metrics2.lib.MutableGaugeInt newGauge(org.apache.hadoop.metrics2.MetricsInfo, int);\n  public org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(java.lang.String, java.lang.String, long);\n  public synchronized org.apache.hadoop.metrics2.lib.MutableGaugeLong newGauge(org.apache.hadoop.metrics2.MetricsInfo, long);\n  public synchronized org.apache.hadoop.metrics2.lib.MutableQuantiles newQuantiles(java.lang.String, java.lang.String, java.lang.String, java.lang.String, int);\n  public synchronized org.apache.hadoop.metrics2.lib.MutableStat newStat(java.lang.String, java.lang.String, java.lang.String, java.lang.String, boolean);\n  public org.apache.hadoop.metrics2.lib.MutableStat newStat(java.lang.String, java.lang.String, java.lang.String, java.lang.String);\n  public org.apache.hadoop.metrics2.lib.MutableRate newRate(java.lang.String);\n  public org.apache.hadoop.metrics2.lib.MutableRate newRate(java.lang.String, java.lang.String);\n  public org.apache.hadoop.metrics2.lib.MutableRate newRate(java.lang.String, java.lang.String, boolean);\n  public synchronized org.apache.hadoop.metrics2.lib.MutableRate newRate(java.lang.String, java.lang.String, boolean, boolean);\n  synchronized void add(java.lang.String, org.apache.hadoop.metrics2.lib.MutableMetric);\n  public synchronized void add(java.lang.String, long);\n  public org.apache.hadoop.metrics2.lib.MetricsRegistry setContext(java.lang.String);\n  public org.apache.hadoop.metrics2.lib.MetricsRegistry tag(java.lang.String, java.lang.String, java.lang.String);\n  public org.apache.hadoop.metrics2.lib.MetricsRegistry tag(java.lang.String, java.lang.String, java.lang.String, boolean);\n  public synchronized org.apache.hadoop.metrics2.lib.MetricsRegistry tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String, boolean);\n  public org.apache.hadoop.metrics2.lib.MetricsRegistry tag(org.apache.hadoop.metrics2.MetricsInfo, java.lang.String);\n  java.util.Collection<org.apache.hadoop.metrics2.MetricsTag> tags();\n  java.util.Collection<org.apache.hadoop.metrics2.lib.MutableMetric> metrics();\n  public synchronized void snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder, boolean);\n  public java.lang.String toString();\n}\n"
}